{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments on Graph-Generative-Models\n",
    "In this notebook, we aim to evluate the performance of \"GDSS\" proposed in \"Score-based Generative Modeling of Graphs via the System of Stochastic Differential Equations\" (https://arxiv.org/pdf/2202.02514.pdf). The baseline model is tested on 3 datasets (Grid, Protein, 3D Point Cloud) and measured under 4 metrics (degree, clustering, orbit, spectral).\n",
    "\n",
    "It should be noted that we adopt the same datasets presets as in \"Efficient Graph Generation with Graph Recurrent Attention Networks\" (https://arxiv.org/pdf/1910.00760.pdf), where:\n",
    "- Grid: 100 graphs are generated with $100\\leq |V| \\leq 400$;\n",
    "- Protein: 918 graphs are generated with $100\\leq |V| \\leq 500$;\n",
    "- 3D Point-Cloud (FirstMM-DB): 41 graphs are generated with $\\bar{|V|} > 1000$\n",
    "\n",
    "Following the experimental setting as in \"GraphRNN: Generating Realistic Graphs with Deep Auto-regressive Models\" (https://arxiv.org/abs/1802.08773), we conduct a 80\\%-20\\% split of the graph samples in each dataset. Then we generate the same size of graph samples as the test dataset and harness the maximum mean discrepancy (MMD) to evaluate the generative graph distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment on GDSS\n",
    "Here we immigrate the original terminal-executable GDSS codes into the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Change current directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "path = \"./GDSS_o/\"\n",
    "os.chdir(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt\n",
    "%conda install -c conda-forge rdkit=2020.09.1.0\n",
    "!yes | pip install git+https://github.com/fabriziocosta/EDeN.git --user!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Assign dataset and seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'grid'\n",
    "dataset = 'DD'\n",
    "dataset = 'community_small'\n",
    "dataset = 'caveman'\n",
    "# dataset = 'FIRSTMM_DB'\n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Clear cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: GPUtil in /nethome/hsun409/anaconda3/envs/GDSS_env/lib/python3.7/site-packages (1.4.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nethome/hsun409/anaconda3/envs/GDSS_env/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial GPU Usage\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 26% | 65% |\n",
      "|  1 |  1% | 82% |\n",
      "|  2 | 13% | 43% |\n",
      "|  3 |  7% | 60% |\n",
      "|  4 |  6% | 62% |\n",
      "|  5 |  6% | 89% |\n",
      "|  6 |  0% | 90% |\n",
      "|  7 |  0% | 95% |\n",
      "GPU Usage after emptying the cache\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  0% | 66% |\n",
      "|  1 |  0% | 82% |\n",
      "|  2 |  0% | 43% |\n",
      "|  3 |  6% | 60% |\n",
      "|  4 |  5% | 62% |\n",
      "|  5 |  1% | 89% |\n",
      "|  6 |  0% | 90% |\n",
      "|  7 |  0% | 95% |\n"
     ]
    }
   ],
   "source": [
    "%pip install GPUtil\n",
    "\n",
    "import torch\n",
    "from GPUtil import showUtilization as gpu_usage\n",
    "from numba import cuda\n",
    "\n",
    "def free_gpu_cache():\n",
    "    print(\"Initial GPU Usage\")\n",
    "    gpu_usage()                             \n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    cuda.select_device(0)\n",
    "    cuda.close()\n",
    "    cuda.select_device(0)\n",
    "\n",
    "    print(\"GPU Usage after emptying the cache\")\n",
    "    gpu_usage()\n",
    "\n",
    "free_gpu_cache()                           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Generate dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading graph dataset: FIRSTMM_DB\n",
      "56468\n",
      "126038\n",
      "Graphs loaded, total num: 24\n",
      "FIRSTMM_DB 24\n",
      "995\n"
     ]
    }
   ],
   "source": [
    "!python data/data_generators.py --dataset $dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train the GDSS model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import argparse\n",
    "import time\n",
    "from parsers.config import get_config\n",
    "from trainer import Trainer\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"6,7\"\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "ts = time.strftime('%b%d-%H:%M:%S', time.gmtime())\n",
    "config = get_config(dataset, seed)\n",
    "trainer = Trainer(config) \n",
    "ckpt = trainer.train(ts)\n",
    "if 'sample' in config.keys():\n",
    "    config.ckpt = ckpt\n",
    "    sampler = Sampler(config) \n",
    "    sampler.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Generate new graphs by the trained GDSS model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 caveman caveman_5000\n",
      "Test size: 40\n",
      "./checkpoints/caveman/caveman_5000.pth loaded\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Make Directory caveman/test in Logs\n",
      "caveman_5000-sample20:05:40\n",
      "caveman_5000-sample20:05:40\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[caveman]   init=deg (10)   seed=42   batch_size=128\n",
      "----------------------------------------------------------------------------------------------------\n",
      "lr=0.01 schedule=True ema=0.999 epochs=5000 reduce=False eps=1e-05\n",
      "(ScoreNetworkX)+(ScoreNetworkA=GCN,4)   : depth=3 adim=32 nhid=32 layers=5 linears=2 c=(2 8 4)\n",
      "(x:VP)=(0.10, 1.00) N=1000 (adj:VP)=(0.10, 1.00) N=1000\n",
      "----------------------------------------------------------------------------------------------------\n",
      "(Euler)+(Langevin): eps=0.0001 denoise=True ema=False || snr=0.05 seps=0.7 n_steps=1 \n",
      "----------------------------------------------------------------------------------------------------\n",
      "GEN SEED: 13\n",
      "8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Round 0 : 583.04s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Round 1 : 591.05s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Round 2 : 580.44s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Round 3 : 576.11s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Round 4 : 584.50s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Round 5 : 588.54s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Round 6 : 591.70s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Round 7 : 574.78s\n",
      "\u001b[91mdegree   \u001b[0m : \u001b[94m0.008862\u001b[0m\n",
      "\u001b[91mcluster  \u001b[0m : \u001b[94m0.032990\u001b[0m\n",
      "\u001b[91morbit    \u001b[0m : \u001b[94m0.002116\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "path = \"./GDSS_o/\"\n",
    "os.chdir(path)\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,2,3,4,5\"\n",
    "\n",
    "import torch\n",
    "import argparse\n",
    "import time\n",
    "from parsers.config import get_config\n",
    "from trainer import Trainer\n",
    "from sampler import Sampler, Sampler_mol\n",
    "from evaluation.stats import eval_graph_list\n",
    "from evaluation.mmd import gaussian, gaussian_emd\n",
    "from data.data_generators import load_dataset\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# dataset = 'caveman'\n",
    "# dataset = 'cora'\n",
    "# dataset = 'community_small'\n",
    "# dataset = 'breast'\n",
    "# dataset = 'grid'\n",
    "\n",
    "datasets = ['caveman', 'cora', 'breast']\n",
    "\n",
    "\n",
    "# ckpt = 'caveman_5000'\n",
    "# ckpt = 'cora_5000'\n",
    "# ckpt = 'gdss_community_small'\n",
    "# ckpt = 'gdss_grid'\n",
    "ckpts = ['caveman_5000', 'cora_5000', 'breast_5000']\n",
    "ckpts = ['caveman_5000']\n",
    "seed = 42\n",
    "test_split = 0.2\n",
    "\n",
    "GEN_seeds = [11,12,13,14,15]\n",
    "GEN_seeds = [13]\n",
    "for gen_seed in GEN_seeds:\n",
    "    for dataset, ckpt in zip(datasets, ckpts):\n",
    "        # if dataset == 'caveman' and gen_seed == 13:\n",
    "        #     continue\n",
    "        # if dataset == 'cora' and gen_seed == 12:\n",
    "        #     continue\n",
    "        # if dataset == 'breast' and gen_seed == 13:\n",
    "        #     continue\n",
    "        print(gen_seed, dataset, ckpt)\n",
    "        graph_list = load_dataset(data_dir='data', file_name=dataset)\n",
    "        if dataset == 'cora':\n",
    "            print('for cora, only take first 200 samples')\n",
    "            graph_list = graph_list[:200].copy()\n",
    "        test_size = int(test_split * len(graph_list))\n",
    "        print('Test size:', test_size)\n",
    "\n",
    "        config = get_config(dataset, seed)\n",
    "        config.sample.seed = gen_seed\n",
    "        config.ckpt = ckpt\n",
    "        sampler = Sampler(config) \n",
    "        sampler.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "[7.3999999999999995]\n",
      "7.3999999999999995\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAigAAAGdCAYAAAA44ojeAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8rg+JYAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAjnklEQVR4nO3dfXST9f3/8VdbaKBA0hVo045SEZRSaMUBK/EGUSqlVIbH6gSrVGQwWcFBJ2I9iNyIReZR1GGZzgFOCs4pOFBBbkaZh3JXRe4UhaHFQVon0kAdAdr8/tiP67tMUANt82l8Ps65ziHX9WnyTo6ePs+VK2mYz+fzCQAAwCDhwR4AAADgfxEoAADAOAQKAAAwDoECAACMQ6AAAADjECgAAMA4BAoAADAOgQIAAIzTLNgDXIi6ujodPnxYbdq0UVhYWLDHAQAA34PP59Px48eVkJCg8PBvP0fSJAPl8OHDSkxMDPYYAADgAhw6dEgdOnT41jVNMlDatGkj6T9P0G63B3kaAADwfXg8HiUmJlq/x79NkwyUs2/r2O12AgUAgCbm+1yewUWyAADAOAQKAAAwDoECAACMQ6AAAADjECgAAMA4BAoAADAOgQIAAIxDoAAAAOMQKAAAwDgECgAAMA6BAgAAjEOgAAAA4xAoAADAOAQKAAAwTrNgDwCgYV3y4JvBHiFgn87ODvYIAIKMMygAAMA4BAoAADAOgQIAAIxDoAAAAOMQKAAAwDgECgAAMA6BAgAAjEOgAAAA4xAoAADAOAQKAAAwDoECAACMQ6AAAADjECgAAMA4BAoAADAOgQIAAIxDoAAAAOMQKAAAwDgECgAAMA6BAgAAjEOgAAAA4xAoAADAOAQKAAAwDoECAACME1CgFBcXKy0tTXa7XXa7XS6XS2+//bZ1vH///goLC/Pb7r33Xr/7qKioUHZ2tqKiohQbG6tJkybpzJkz9fNsAABASGgWyOIOHTpo9uzZuuyyy+Tz+bRo0SINHTpU77//vrp37y5JGj16tGbMmGH9TFRUlPXv2tpaZWdny+l0atOmTTpy5IhGjBih5s2b67HHHqunpwQAAJq6gAJlyJAhfrdnzZql4uJibd682QqUqKgoOZ3Oc/78O++8o71792rt2rWKi4tTz549NXPmTE2ePFnTpk1TZGTkBT4NAAAQSi74GpTa2lotXbpUNTU1crlc1v7FixerXbt26tGjhwoLC/X1119bx8rKypSamqq4uDhrX2Zmpjwej/bs2XPex/J6vfJ4PH4bAAAIXQGdQZGkXbt2yeVy6eTJk2rdurWWLVumlJQUSdIdd9yhpKQkJSQkaOfOnZo8ebL27dun119/XZLkdrv94kSSddvtdp/3MYuKijR9+vRARwUAAE1UwIHStWtX7dixQ9XV1frLX/6ivLw8lZaWKiUlRWPGjLHWpaamKj4+XgMGDNCBAwfUuXPnCx6ysLBQBQUF1m2Px6PExMQLvj8AAGC2gN/iiYyMVJcuXdSrVy8VFRXpiiuu0NNPP33Otenp6ZKk/fv3S5KcTqcqKyv91py9fb7rViTJZrNZnxw6uwEAgNB10d+DUldXJ6/Xe85jO3bskCTFx8dLklwul3bt2qWqqiprzZo1a2S32623iQAAAAJ6i6ewsFBZWVnq2LGjjh8/rpKSEm3YsEGrV6/WgQMHVFJSosGDB6tt27bauXOnJk6cqH79+iktLU2SNHDgQKWkpOiuu+7SnDlz5Ha7NWXKFOXn58tmszXIEwQAAE1PQIFSVVWlESNG6MiRI3I4HEpLS9Pq1at144036tChQ1q7dq3mzp2rmpoaJSYmKicnR1OmTLF+PiIiQitXrtTYsWPlcrnUqlUr5eXl+X1vCgAAQJjP5/MFe4hAeTweORwOVVdXcz0K8B0uefDNYI8QsE9nZwd7BAANIJDf3/wtHgAAYBwCBQAAGIdAAQAAxiFQAACAcQgUAABgHAIFAAAYh0ABAADGIVAAAIBxCBQAAGAcAgUAABiHQAEAAMYhUAAAgHEIFAAAYBwCBQAAGIdAAQAAxiFQAACAcQgUAABgHAIFAAAYh0ABAADGIVAAAIBxCBQAAGAcAgUAABiHQAEAAMYhUAAAgHEIFAAAYBwCBQAAGIdAAQAAxiFQAACAcQgUAABgHAIFAAAYh0ABAADGIVAAAIBxCBQAAGAcAgUAABiHQAEAAMYJKFCKi4uVlpYmu90uu90ul8ult99+2zp+8uRJ5efnq23btmrdurVycnJUWVnpdx8VFRXKzs5WVFSUYmNjNWnSJJ05c6Z+ng0AAAgJAQVKhw4dNHv2bJWXl2v79u264YYbNHToUO3Zs0eSNHHiRK1YsUKvvvqqSktLdfjwYd1yyy3Wz9fW1io7O1unTp3Spk2btGjRIi1cuFBTp06t32cFAACatDCfz+e7mDuIiYnRb3/7W916661q3769SkpKdOutt0qSPvroI3Xr1k1lZWXq27ev3n77bd100006fPiw4uLiJEnz58/X5MmT9cUXXygyMvJ7PabH45HD4VB1dbXsdvvFjA+EvEsefDPYIwTs09nZwR4BQAMI5Pf3BV+DUltbq6VLl6qmpkYul0vl5eU6ffq0MjIyrDXJycnq2LGjysrKJEllZWVKTU214kSSMjMz5fF4rLMw5+L1euXxePw2AAAQugIOlF27dql169ay2Wy69957tWzZMqWkpMjtdisyMlLR0dF+6+Pi4uR2uyVJbrfbL07OHj977HyKiorkcDisLTExMdCxAQBAExJwoHTt2lU7duzQli1bNHbsWOXl5Wnv3r0NMZulsLBQ1dXV1nbo0KEGfTwAABBczQL9gcjISHXp0kWS1KtXL23btk1PP/20br/9dp06dUrHjh3zO4tSWVkpp9MpSXI6ndq6davf/Z39lM/ZNedis9lks9kCHRUAADRRF/09KHV1dfJ6verVq5eaN2+udevWWcf27duniooKuVwuSZLL5dKuXbtUVVVlrVmzZo3sdrtSUlIudhQAABAiAjqDUlhYqKysLHXs2FHHjx9XSUmJNmzYoNWrV8vhcGjUqFEqKChQTEyM7Ha7xo8fL5fLpb59+0qSBg4cqJSUFN11112aM2eO3G63pkyZovz8fM6QAAAAS0CBUlVVpREjRujIkSNyOBxKS0vT6tWrdeONN0qSnnrqKYWHhysnJ0der1eZmZl67rnnrJ+PiIjQypUrNXbsWLlcLrVq1Up5eXmaMWNG/T4rAADQpF3096AEA9+DAnx/fA8KAFM0yvegAAAANBQCBQAAGIdAAQAAxiFQAACAcQgUAABgHAIFAAAYh0ABAADGIVAAAIBxCBQAAGAcAgUAABiHQAEAAMYhUAAAgHEIFAAAYBwCBQAAGIdAAQAAxiFQAACAcQgUAABgHAIFAAAYh0ABAADGIVAAAIBxCBQAAGAcAgUAABiHQAEAAMZpFuwBACAUXPLgm8EeIWCfzs4O9gjAeXEGBQAAGIdAAQAAxiFQAACAcbgGBUHDe/YAgPPhDAoAADAOgQIAAIxDoAAAAOMQKAAAwDgECgAAMA6BAgAAjEOgAAAA4xAoAADAOAEFSlFRkfr06aM2bdooNjZWN998s/bt2+e3pn///goLC/Pb7r33Xr81FRUVys7OVlRUlGJjYzVp0iSdOXPm4p8NAAAICQF9k2xpaany8/PVp08fnTlzRg899JAGDhyovXv3qlWrVta60aNHa8aMGdbtqKgo69+1tbXKzs6W0+nUpk2bdOTIEY0YMULNmzfXY489Vg9PCQAANHUBBcqqVav8bi9cuFCxsbEqLy9Xv379rP1RUVFyOp3nvI933nlHe/fu1dq1axUXF6eePXtq5syZmjx5sqZNm6bIyMgLeBoAACCUXNQ1KNXV1ZKkmJgYv/2LFy9Wu3bt1KNHDxUWFurrr7+2jpWVlSk1NVVxcXHWvszMTHk8Hu3Zs+ecj+P1euXxePw2AAAQui74jwXW1dVpwoQJuvrqq9WjRw9r/x133KGkpCQlJCRo586dmjx5svbt26fXX39dkuR2u/3iRJJ12+12n/OxioqKNH369AsdFQAANDEXHCj5+fnavXu33n33Xb/9Y8aMsf6dmpqq+Ph4DRgwQAcOHFDnzp0v6LEKCwtVUFBg3fZ4PEpMTLywwQEAgPEu6C2ecePGaeXKlfrb3/6mDh06fOva9PR0SdL+/fslSU6nU5WVlX5rzt4+33UrNptNdrvdbwMAAKEroEDx+XwaN26cli1bpvXr16tTp07f+TM7duyQJMXHx0uSXC6Xdu3apaqqKmvNmjVrZLfblZKSEsg4AAAgRAX0Fk9+fr5KSkr0xhtvqE2bNtY1Iw6HQy1bttSBAwdUUlKiwYMHq23bttq5c6cmTpyofv36KS0tTZI0cOBApaSk6K677tKcOXPkdrs1ZcoU5efny2az1f8zBAAATU5AZ1CKi4tVXV2t/v37Kz4+3tpeeeUVSVJkZKTWrl2rgQMHKjk5Wb/5zW+Uk5OjFStWWPcRERGhlStXKiIiQi6XS3feeadGjBjh970pAADghy2gMyg+n+9bjycmJqq0tPQ77ycpKUlvvfVWIA8NAAB+QPhbPAAAwDgECgAAMA6BAgAAjEOgAAAA4xAoAADAOAQKAAAwDoECAACMQ6AAAADjECgAAMA4BAoAADAOgQIAAIxDoAAAAOMQKAAAwDgECgAAMA6BAgAAjEOgAAAA4xAoAADAOAQKAAAwDoECAACMQ6AAAADjECgAAMA4BAoAADAOgQIAAIxDoAAAAOMQKAAAwDgECgAAMA6BAgAAjEOgAAAA4xAoAADAOAQKAAAwDoECAACMQ6AAAADjECgAAMA4BAoAADAOgQIAAIwTUKAUFRWpT58+atOmjWJjY3XzzTdr3759fmtOnjyp/Px8tW3bVq1bt1ZOTo4qKyv91lRUVCg7O1tRUVGKjY3VpEmTdObMmYt/NgAAICQEFCilpaXKz8/X5s2btWbNGp0+fVoDBw5UTU2NtWbixIlasWKFXn31VZWWlurw4cO65ZZbrOO1tbXKzs7WqVOntGnTJi1atEgLFy7U1KlT6+9ZAQCAJq1ZIItXrVrld3vhwoWKjY1VeXm5+vXrp+rqar344osqKSnRDTfcIElasGCBunXrps2bN6tv37565513tHfvXq1du1ZxcXHq2bOnZs6cqcmTJ2vatGmKjIysv2cHAACapIu6BqW6ulqSFBMTI0kqLy/X6dOnlZGRYa1JTk5Wx44dVVZWJkkqKytTamqq4uLirDWZmZnyeDzas2fPOR/H6/XK4/H4bQAAIHRdcKDU1dVpwoQJuvrqq9WjRw9JktvtVmRkpKKjo/3WxsXFye12W2v+O07OHj977FyKiorkcDisLTEx8ULHBgAATcAFB0p+fr52796tpUuX1uc851RYWKjq6mprO3ToUIM/JgAACJ6ArkE5a9y4cVq5cqU2btyoDh06WPudTqdOnTqlY8eO+Z1FqayslNPptNZs3brV7/7Ofsrn7Jr/ZbPZZLPZLmRUAADQBAV0BsXn82ncuHFatmyZ1q9fr06dOvkd79Wrl5o3b65169ZZ+/bt26eKigq5XC5Jksvl0q5du1RVVWWtWbNmjex2u1JSUi7muQAAgBAR0BmU/Px8lZSU6I033lCbNm2sa0YcDodatmwph8OhUaNGqaCgQDExMbLb7Ro/frxcLpf69u0rSRo4cKBSUlJ01113ac6cOXK73ZoyZYry8/M5SwIAACQFGCjFxcWSpP79+/vtX7Bgge6++25J0lNPPaXw8HDl5OTI6/UqMzNTzz33nLU2IiJCK1eu1NixY+VyudSqVSvl5eVpxowZF/dMAABAyAgoUHw+33euadGihebNm6d58+add01SUpLeeuutQB4aAAD8gPC3eAAAgHEIFAAAYBwCBQAAGIdAAQAAxiFQAACAcQgUAABgHAIFAAAYh0ABAADGIVAAAIBxCBQAAGAcAgUAABiHQAEAAMYhUAAAgHEIFAAAYBwCBQAAGIdAAQAAxiFQAACAcQgUAABgHAIFAAAYh0ABAADGIVAAAIBxCBQAAGAcAgUAABiHQAEAAMYhUAAAgHEIFAAAYBwCBQAAGIdAAQAAxiFQAACAcQgUAABgHAIFAAAYh0ABAADGIVAAAIBxCBQAAGAcAgUAABgn4EDZuHGjhgwZooSEBIWFhWn58uV+x++++26FhYX5bYMGDfJbc/ToUeXm5sputys6OlqjRo3SiRMnLuqJAACA0BFwoNTU1OiKK67QvHnzzrtm0KBBOnLkiLUtWbLE73hubq727NmjNWvWaOXKldq4caPGjBkT+PQAACAkNQv0B7KyspSVlfWta2w2m5xO5zmPffjhh1q1apW2bdum3r17S5KeffZZDR48WE888YQSEhICHQkAAISYBrkGZcOGDYqNjVXXrl01duxYffnll9axsrIyRUdHW3EiSRkZGQoPD9eWLVvOeX9er1cej8dvAwAAoaveA2XQoEF66aWXtG7dOj3++OMqLS1VVlaWamtrJUlut1uxsbF+P9OsWTPFxMTI7Xaf8z6LiorkcDisLTExsb7HBgAABgn4LZ7vMmzYMOvfqampSktLU+fOnbVhwwYNGDDggu6zsLBQBQUF1m2Px0OkAAAQwhr8Y8aXXnqp2rVrp/3790uSnE6nqqqq/NacOXNGR48ePe91KzabTXa73W8DAAChq8ED5fPPP9eXX36p+Ph4SZLL5dKxY8dUXl5urVm/fr3q6uqUnp7e0OMAAIAmIOC3eE6cOGGdDZGkgwcPaseOHYqJiVFMTIymT5+unJwcOZ1OHThwQA888IC6dOmizMxMSVK3bt00aNAgjR49WvPnz9fp06c1btw4DRs2jE/wAAAASRdwBmX79u268sordeWVV0qSCgoKdOWVV2rq1KmKiIjQzp079bOf/UyXX365Ro0apV69eunvf/+7bDabdR+LFy9WcnKyBgwYoMGDB+uaa67R888/X3/PCgAANGkBn0Hp37+/fD7feY+vXr36O+8jJiZGJSUlgT40AAD4geBv8QAAAOMQKAAAwDgECgAAMA6BAgAAjEOgAAAA4xAoAADAOAQKAAAwDoECAACMQ6AAAADjECgAAMA4BAoAADAOgQIAAIxDoAAAAOMQKAAAwDgECgAAMA6BAgAAjEOgAAAA4xAoAADAOAQKAAAwDoECAACMQ6AAAADjECgAAMA4BAoAADAOgQIAAIxDoAAAAOMQKAAAwDgECgAAMA6BAgAAjEOgAAAA4xAoAADAOAQKAAAwDoECAACMQ6AAAADjECgAAMA4BAoAADBOwIGyceNGDRkyRAkJCQoLC9Py5cv9jvt8Pk2dOlXx8fFq2bKlMjIy9Mknn/itOXr0qHJzc2W32xUdHa1Ro0bpxIkTF/VEAABA6Ag4UGpqanTFFVdo3rx55zw+Z84cPfPMM5o/f762bNmiVq1aKTMzUydPnrTW5Obmas+ePVqzZo1WrlypjRs3asyYMRf+LAAAQEhpFugPZGVlKSsr65zHfD6f5s6dqylTpmjo0KGSpJdeeklxcXFavny5hg0bpg8//FCrVq3Stm3b1Lt3b0nSs88+q8GDB+uJJ55QQkLCRTwdAAAQCur1GpSDBw/K7XYrIyPD2udwOJSenq6ysjJJUllZmaKjo604kaSMjAyFh4dry5Yt9TkOAABoogI+g/Jt3G63JCkuLs5vf1xcnHXM7XYrNjbWf4hmzRQTE2Ot+V9er1der9e67fF46nNsAABgmCbxKZ6ioiI5HA5rS0xMDPZIAACgAdVroDidTklSZWWl3/7KykrrmNPpVFVVld/xM2fO6OjRo9aa/1VYWKjq6mprO3ToUH2ODQAADFOvgdKpUyc5nU6tW7fO2ufxeLRlyxa5XC5Jksvl0rFjx1ReXm6tWb9+verq6pSenn7O+7XZbLLb7X4bAAAIXQFfg3LixAnt37/fun3w4EHt2LFDMTEx6tixoyZMmKBHH31Ul112mTp16qSHH35YCQkJuvnmmyVJ3bp106BBgzR69GjNnz9fp0+f1rhx4zRs2DA+wQMAACRdQKBs375d119/vXW7oKBAkpSXl6eFCxfqgQceUE1NjcaMGaNjx47pmmuu0apVq9SiRQvrZxYvXqxx48ZpwIABCg8PV05Ojp555pl6eDoAACAUBBwo/fv3l8/nO+/xsLAwzZgxQzNmzDjvmpiYGJWUlAT60AAA4AeiSXyKBwAA/LAQKAAAwDgECgAAMA6BAgAAjEOgAAAA4xAoAADAOAQKAAAwDoECAACMQ6AAAADjECgAAMA4BAoAADAOgQIAAIxDoAAAAOMQKAAAwDgECgAAMA6BAgAAjEOgAAAA4xAoAADAOAQKAAAwDoECAACMQ6AAAADjECgAAMA4BAoAADAOgQIAAIzTLNgDAADwfV3y4JvBHiFgn87ODvYITRJnUAAAgHEIFAAAYBwCBQAAGIdAAQAAxiFQAACAcQgUAABgHAIFAAAYh0ABAADGIVAAAIBxCBQAAGAcAgUAABin3gNl2rRpCgsL89uSk5Ot4ydPnlR+fr7atm2r1q1bKycnR5WVlfU9BgAAaMIa5AxK9+7ddeTIEWt79913rWMTJ07UihUr9Oqrr6q0tFSHDx/WLbfc0hBjAACAJqpB/ppxs2bN5HQ6v7G/urpaL774okpKSnTDDTdIkhYsWKBu3bpp8+bN6tu3b0OMAwAAmpgGOYPyySefKCEhQZdeeqlyc3NVUVEhSSovL9fp06eVkZFhrU1OTlbHjh1VVlZ23vvzer3yeDx+GwAACF31Hijp6elauHChVq1apeLiYh08eFDXXnutjh8/LrfbrcjISEVHR/v9TFxcnNxu93nvs6ioSA6Hw9oSExPre2wAAGCQen+LJysry/p3Wlqa0tPTlZSUpD//+c9q2bLlBd1nYWGhCgoKrNsej4dIAQAghDX4x4yjo6N1+eWXa//+/XI6nTp16pSOHTvmt6aysvKc16ycZbPZZLfb/TYAABC6GjxQTpw4oQMHDig+Pl69evVS8+bNtW7dOuv4vn37VFFRIZfL1dCjAACAJqLe3+K5//77NWTIECUlJenw4cN65JFHFBERoeHDh8vhcGjUqFEqKChQTEyM7Ha7xo8fL5fLxSd4AACApd4D5fPPP9fw4cP15Zdfqn379rrmmmu0efNmtW/fXpL01FNPKTw8XDk5OfJ6vcrMzNRzzz1X32MAAIAmrN4DZenSpd96vEWLFpo3b57mzZtX3w8NAABCBH+LBwAAGIdAAQAAxiFQAACAcQgUAABgHAIFAAAYh0ABAADGIVAAAIBxCBQAAGAcAgUAABiHQAEAAMYhUAAAgHEIFAAAYBwCBQAAGIdAAQAAxiFQAACAcQgUAABgHAIFAAAYh0ABAADGIVAAAIBxCBQAAGAcAgUAABiHQAEAAMYhUAAAgHEIFAAAYBwCBQAAGIdAAQAAxmkW7AFMdMmDbwZ7hIB9Ojs72CMAAFBvOIMCAACMQ6AAAADjECgAAMA4BAoAADAOgQIAAIxDoAAAAOPwMWMAABpQU/zqCin4X1/BGRQAAGCcoAbKvHnzdMkll6hFixZKT0/X1q1bgzkOAAAwRNAC5ZVXXlFBQYEeeeQRvffee7riiiuUmZmpqqqqYI0EAAAMEbRAefLJJzV69GiNHDlSKSkpmj9/vqKiovTHP/4xWCMBAABDBOUi2VOnTqm8vFyFhYXWvvDwcGVkZKisrOwb671er7xer3W7urpakuTxeBpkvjrv1w1yvw2poV6LhsTr3Dh4nRsHr3PjaIqvc1PVEP99nL1Pn8/3nWuDEij/+te/VFtbq7i4OL/9cXFx+uijj76xvqioSNOnT//G/sTExAabsalxzA32BD8MvM6Ng9e5cfA649s05H8fx48fl8Ph+NY1TeJjxoWFhSooKLBu19XV6ejRo2rbtq3CwsLq9bE8Ho8SExN16NAh2e32er1v/B9e58bB69w4eJ0bB69z42mo19rn8+n48eNKSEj4zrVBCZR27dopIiJClZWVfvsrKyvldDq/sd5ms8lms/nti46ObsgRZbfb+R+gEfA6Nw5e58bB69w4eJ0bT0O81t915uSsoFwkGxkZqV69emndunXWvrq6Oq1bt04ulysYIwEAAIME7S2egoIC5eXlqXfv3vrpT3+quXPnqqamRiNHjgzWSAAAwBBBC5Tbb79dX3zxhaZOnSq3262ePXtq1apV37hwtrHZbDY98sgj33hLCfWL17lx8Do3Dl7nxsHr3HhMeK3DfN/nsz4AAACNiL/FAwAAjEOgAAAA4xAoAADAOAQKAAAwDoFyDrNnz1ZYWJgmTJgQ7FFCzrRp0xQWFua3JScnB3uskPTPf/5Td955p9q2bauWLVsqNTVV27dvD/ZYIeWSSy75xn/PYWFhys/PD/ZoIaW2tlYPP/ywOnXqpJYtW6pz586aOXPm9/p7LgjM8ePHNWHCBCUlJally5a66qqrtG3btqDM0iS+6r4xbdu2Tb///e+VlpYW7FFCVvfu3bV27VrrdrNm/GdY37766itdffXVuv766/X222+rffv2+uSTT/SjH/0o2KOFlG3btqm2tta6vXv3bt1444267bbbgjhV6Hn88cdVXFysRYsWqXv37tq+fbtGjhwph8Oh++67L9jjhZRf/OIX2r17t/70pz8pISFBL7/8sjIyMrR37179+Mc/btRZ+M3wX06cOKHc3Fy98MILevTRR4M9Tshq1qzZOf+kAerP448/rsTERC1YsMDa16lTpyBOFJrat2/vd3v27Nnq3LmzrrvuuiBNFJo2bdqkoUOHKjs7W9J/zlwtWbJEW7duDfJkoeXf//63XnvtNb3xxhvq16+fpP+c9V6xYoWKi4sb/fcib/H8l/z8fGVnZysjIyPYo4S0Tz75RAkJCbr00kuVm5urioqKYI8Ucv7617+qd+/euu222xQbG6srr7xSL7zwQrDHCmmnTp3Syy+/rHvuuafe/4jpD91VV12ldevW6eOPP5YkffDBB3r33XeVlZUV5MlCy5kzZ1RbW6sWLVr47W/ZsqXefffdRp+HMyj/39KlS/Xee+8F7b22H4r09HQtXLhQXbt21ZEjRzR9+nRde+212r17t9q0aRPs8ULGP/7xDxUXF6ugoEAPPfSQtm3bpvvuu0+RkZHKy8sL9nghafny5Tp27JjuvvvuYI8Sch588EF5PB4lJycrIiJCtbW1mjVrlnJzc4M9Wkhp06aNXC6XZs6cqW7duikuLk5LlixRWVmZunTp0vgD+eCrqKjwxcbG+j744ANr33XXXef79a9/HbyhfiC++uorn91u9/3hD38I9ighpXnz5j6Xy+W3b/z48b6+ffsGaaLQN3DgQN9NN90U7DFC0pIlS3wdOnTwLVmyxLdz507fSy+95IuJifEtXLgw2KOFnP379/v69evnk+SLiIjw9enTx5ebm+tLTk5u9Fk4gyKpvLxcVVVV+slPfmLtq62t1caNG/W73/1OXq9XERERQZwwdEVHR+vyyy/X/v37gz1KSImPj1dKSorfvm7duum1114L0kSh7bPPPtPatWv1+uuvB3uUkDRp0iQ9+OCDGjZsmCQpNTVVn332mYqKijgjWM86d+6s0tJS1dTUyOPxKD4+XrfffrsuvfTSRp+Fa1AkDRgwQLt27dKOHTusrXfv3srNzdWOHTuIkwZ04sQJHThwQPHx8cEeJaRcffXV2rdvn9++jz/+WElJSUGaKLQtWLBAsbGx1kWcqF9ff/21wsP9f11FRESorq4uSBOFvlatWik+Pl5fffWVVq9eraFDhzb6DJxB0X/ed+vRo4ffvlatWqlt27bf2I+Lc//992vIkCFKSkrS4cOH9cgjjygiIkLDhw8P9mghZeLEibrqqqv02GOP6ec//7m2bt2q559/Xs8//3ywRws5dXV1WrBggfLy8vjIfAMZMmSIZs2apY4dO6p79+56//339eSTT+qee+4J9mghZ/Xq1fL5fOratav279+vSZMmKTk5WSNHjmz0Wfi/CY3q888/1/Dhw/Xll1+qffv2uuaaa7R58+ZvfFwTF6dPnz5atmyZCgsLNWPGDHXq1Elz587losIGsHbtWlVUVPDLsgE9++yzevjhh/WrX/1KVVVVSkhI0C9/+UtNnTo12KOFnOrqahUWFurzzz9XTEyMcnJyNGvWLDVv3rzRZwnz+fgqPgAAYBauQQEAAMYhUAAAgHEIFAAAYBwCBQAAGIdAAQAAxiFQAACAcQgUAABgHAIFAAAYh0ABAADGIVAAAIBxCBQAAGAcAgUAABjn/wFDinnySV8Q8QAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pickle\n",
    "import networkx as nx\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "path = \"./GDSS_o/\"\n",
    "os.chdir(path)\n",
    "\n",
    "# caveman_filenames = ['caveman_5000-sample18:20:01','caveman_5000-sample18:31:34', \n",
    "#                     'caveman_5000-sample18:42:56', 'caveman_5000-sample18:54:07', 'caveman_5000-sample19:05:19']\n",
    "caveman_filenames = ['caveman_5000-sample20:05:40']\n",
    "max_degree = 7\n",
    "percentage_per_seed = []\n",
    "for caveman_file in caveman_filenames:\n",
    "    save_dir = './samples/pkl/caveman/test/' + caveman_file + '.pkl'\n",
    "    max_degree_list = []\n",
    "    with open(save_dir, 'rb') as f:\n",
    "        graph_list = pickle.load(f)\n",
    "    print(len(graph_list))\n",
    "    for g in graph_list:\n",
    "        max_degree_list.append(max([deg[1] for deg in g.degree()]))\n",
    "    count = sum(deg > max_degree for deg in max_degree_list)\n",
    "    percentage_per_seed.append(count / len(max_degree_list)*100)\n",
    "print(percentage_per_seed)\n",
    "print(np.mean(percentage_per_seed))\n",
    "plt.hist(max_degree_list)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.018696  0.048331  0.006261  0.052933  546.804000]\n",
      "[ 0.160796  0.376346  0.187337  0.056069  410.404000]\n",
      "[ 0.112948  0.015744  0.001897  0.120352  820.944000]\n",
      "[ 0.016804  0.025670  0.003396  0.000942  19.820961]\n",
      "[ 0.011558  0.053710  0.004920  0.001155  13.397731]\n",
      "[ 0.008225  0.006769  0.000725  0.000290  22.578371]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "caveman_data = np.array(   [[0.052101, 0.035703, 0.008728, 0.052592, 543.10],\n",
    "                            [0.009252, 0.052657, 0.011149, 0.054503, 567.02],\n",
    "                            [0.013782, 0.096422, 0.003810, 0.053426, 514.05],\n",
    "                            [0.008394, 0.025967, 0.001613, 0.052327, 541.94],\n",
    "                            [0.009951, 0.030907, 0.006007, 0.051819, 567.91]])\n",
    "cora_data = np.array(      [[0.142844, 0.364501, 0.179599, 0.055982, 384.09],\n",
    "                            [0.171458, 0.359900, 0.192201, 0.056620, 421.69],\n",
    "                            [0.155826, 0.411962, 0.185434, 0.057684, 414.47],\n",
    "                            [0.158801, 0.292940, 0.186371, 0.054138, 415.15],\n",
    "                            [0.175053, 0.452426, 0.193078, 0.055921, 416.62]])\n",
    "breast_data = np.array(    [[0.111363, 0.026105, 0.000992, 0.120782, 788.48],\n",
    "                            [0.100157, 0.012885, 0.002211, 0.120369, 813.11],\n",
    "                            [0.111881, 0.019623, 0.003031, 0.120421, 858.41],\n",
    "                            [0.115581, 0.005944, 0.001996, 0.120318, 818.76],\n",
    "                            [0.125758, 0.014162, 0.001254, 0.119872, 825.96]])\n",
    "with np.printoptions(precision=6, suppress=True, formatter={'float': '{: .6f}'.format}):\n",
    "    print(np.mean(caveman_data, axis=0))\n",
    "    print(np.mean(cora_data, axis=0))\n",
    "    print(np.mean(breast_data, axis=0))\n",
    "    print(np.std(caveman_data, axis=0))\n",
    "    print(np.std(cora_data, axis=0))\n",
    "    print(np.std(breast_data, axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Decide which metric to be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nethome/hsun409/anaconda3/envs/GDSS_env/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "path = \"./GDSS/\"\n",
    "os.chdir(path)\n",
    "\n",
    "seed = 42\n",
    "dataset = 'grid'\n",
    "# dataset = 'community_small'\n",
    "# dataset = 'caveman'\n",
    "# dataset = 'cora'\n",
    "# dataset = 'breast'\n",
    "\n",
    "metric_selection = 'EMD'\n",
    "\n",
    "if metric_selection == 'EMD':\n",
    "    from sampler import Sampler, Sampler_mol\n",
    "    from evaluation.stats import eval_graph_list\n",
    "    from evaluation.mmd import gaussian, gaussian_emd\n",
    "else:\n",
    "    from sampler_new import Sampler, Sampler_mol\n",
    "    from evaluation.stats_new import eval_graph_list\n",
    "    import evaluation.mmd_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load and calculate the metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target dataset:grid\n",
      "Length of testing dataset:20\n",
      "Length of gen dataset:20\n",
      "\u001b[91mdegree   \u001b[0m : \u001b[94m0.056618\u001b[0m\n",
      "\u001b[91mcluster  \u001b[0m : \u001b[94m0.011350\u001b[0m\n",
      "\u001b[91morbit    \u001b[0m : \u001b[94m0.254204\u001b[0m\n",
      "\u001b[91mspectral \u001b[0m : \u001b[94m1.046698\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import math\n",
    "\n",
    "from utils.logger import Logger, set_log, start_log, train_log, sample_log, check_log\n",
    "from data.data_generators import load_dataset\n",
    "\n",
    "# save_dir = '../GDSS_o/samples/pkl/caveman/test/caveman_5000-sample.pkl'\n",
    "# save_dir = '../GDSS_o/samples/pkl/cora/test/cora_5000-sample.pkl'\n",
    "# save_dir = '../GDSS_o/samples/pkl/community_small/test/gdss_community_small-sample.pkl'\n",
    "# save_dir = '../GDSS_o/samples/pkl/breast/test/breast_5000-sample.pkl'\n",
    "\n",
    "with open(save_dir, 'rb') as f:\n",
    "    gen_graph_list = pickle.load(f)\n",
    "\n",
    "test_split = 0.2\n",
    "\n",
    "graph_list = load_dataset(data_dir='../GDSS_o/data', file_name=dataset)\n",
    "print('Target dataset:' + dataset)\n",
    "if dataset == 'cora':\n",
    "    print('for cora, only take first 200 samples')\n",
    "    graph_list = graph_list[:200].copy()\n",
    "test_size = int(test_split * len(graph_list))\n",
    "train_size = len(graph_list) - test_size\n",
    "train_graph_list, test_graph_list = graph_list[:train_size], graph_list[train_size:]\n",
    "# train_graph_list, test_graph_list = graph_list[test_size:], graph_list[:test_size]\n",
    "print('Length of testing dataset:' + str(len(test_graph_list)))\n",
    "print('Length of gen dataset:' + str(len(gen_graph_list)))\n",
    "methods = ['degree', 'cluster', 'orbit', 'spectral'] \n",
    "kernels = {}\n",
    "if metric_selection == 'EMD':\n",
    "    kernels = {'degree':gaussian_emd, \n",
    "                'cluster':gaussian_emd, \n",
    "                'orbit':gaussian,\n",
    "                'spectral':gaussian_emd}\n",
    "    result_dict = eval_graph_list(test_graph_list, gen_graph_list, methods, kernels)\n",
    "else:\n",
    "    result_dict = eval_graph_list(test_graph_list, gen_graph_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Plot graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.plot import plot_graphs_list\n",
    "\n",
    "for idx, g in enumerate(gen_graph_list):\n",
    "    print(idx, g)\n",
    "\n",
    "# plot_graphs_list(graphs=gen_graph_list, title=dataset, max_num=16, save_dir=dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment on GraphAF/GraphDF\n",
    "Here we investigate the likelihood on GraphAF/GraphDF. Since they are autoreggressive models, their likelihoods can be derived directly by extracting the loss functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch; print(torch.__version__)\n",
    "import torch; print(torch.version.cuda)\n",
    "CUDA = 'cu116'\n",
    "TORCH = '1.12.1'\n",
    "%pip install torch-scatter -f https://data.pyg.org/whl/torch-1.12.1+cu116.html\n",
    "%pip install torch-sparse -f https://data.pyg.org/whl/torch-1.12.1+cu116.html\n",
    "%pip install torch-geometric\n",
    "%pip install torch-cluster -f https://data.pyg.org/whl/torch-1.12.1+cu116.html\n",
    "%pip install torch-spline-conv -f https://data.pyg.org/whl/torch-1.12.1+cu116.html\n",
    "%pip install dive-into-graphs\n",
    "\n",
    "import torch_geometric; print(torch_geometric.__version__)\n",
    "from dig.version import __version__; print(__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load dataset 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "83\n",
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "node tensor size: torch.Size([16, 20, 10])\n",
      "adj tensor size: torch.Size([16, 2, 20, 20])\n",
      "80\n"
     ]
    }
   ],
   "source": [
    "from lib2to3.pytree import Node\n",
    "import pickle\n",
    "import networkx as nx\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from scipy.sparse import csr_matrix\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3,4\"\n",
    "import torch\n",
    "\n",
    "\n",
    "def calc_max_prev_node_helper(idx, graphs_path):\n",
    "    with open(graphs_path + 'graph' + str(idx) + '.dat', 'rb') as f:\n",
    "        G = pickle.load(f)\n",
    "\n",
    "    max_prev_node = []\n",
    "    for _ in range(100):\n",
    "        bfs_seq, _ = get_random_bfs_seq(G)\n",
    "        bfs_order_map = {bfs_seq[i]: i for i in range(len(G.nodes()))}\n",
    "        G = nx.relabel_nodes(G, bfs_order_map)\n",
    "\n",
    "        max_prev_node_iter = 0\n",
    "        for u, v in G.edges():\n",
    "            max_prev_node_iter = max(max_prev_node_iter, max(u, v) - min(u, v))\n",
    "\n",
    "        max_prev_node.append(max_prev_node_iter)\n",
    "\n",
    "    return max_prev_node\n",
    "\n",
    "\n",
    "def calc_max_prev_node(graphs_path):\n",
    "    \"\"\"\n",
    "    Approximate max_prev_node from simulating bfs sequences \n",
    "    \"\"\"\n",
    "    max_prev_node = []\n",
    "    count = len([name for name in os.listdir(\n",
    "        graphs_path) if name.endswith(\".dat\")])\n",
    "\n",
    "    max_prev_node = []\n",
    "    with Pool(processes=8) as pool:\n",
    "        for max_prev_node_g in tqdm(pool.imap_unordered(\n",
    "                partial(calc_max_prev_node_helper, graphs_path=graphs_path), list(range(count)))):\n",
    "            max_prev_node.extend(max_prev_node_g)\n",
    "\n",
    "    max_prev_node = sorted(max_prev_node)[-1 * int(0.001 * len(max_prev_node))]\n",
    "    return max_prev_node\n",
    "\n",
    "def get_bfs_seq(G, start_id):\n",
    "    \"\"\"\n",
    "    Get a bfs node sequence\n",
    "    :param G: graph\n",
    "    :param start_id: starting node\n",
    "    :return: List of bfs node sequence\n",
    "    \"\"\"\n",
    "    successors_dict = dict(nx.bfs_successors(G, start_id))\n",
    "    start = [start_id]\n",
    "    output = [start_id]\n",
    "    while len(start) > 0:\n",
    "        succ = []\n",
    "        for current in start:\n",
    "            if current in successors_dict:\n",
    "                succ = succ + successors_dict[current]\n",
    "\n",
    "        output = output + succ\n",
    "        start = succ\n",
    "    return output\n",
    "\n",
    "def get_random_bfs_seq(graph):\n",
    "    n = len(graph.nodes())\n",
    "    # Create a random permutaion of graph nodes\n",
    "    perm = torch.randperm(n)\n",
    "    adj = nx.to_numpy_matrix(graph, nodelist=perm.numpy().tolist(), dtype=int)\n",
    "    G = nx.from_numpy_matrix(adj)\n",
    "    # Construct bfs ordering starting from a random node\n",
    "    start_id = 0\n",
    "    bfs_seq = get_bfs_seq(G, start_id)\n",
    "    return [perm[bfs_seq[i]] for i in range(n)], perm\n",
    "\n",
    "# load a list of graphs\n",
    "def load_graph_list(fname,is_real=True):\n",
    "    with open(fname, \"rb\") as f:\n",
    "        graph_list = pickle.load(f)\n",
    "    for i in range(len(graph_list)):\n",
    "        edges_with_selfloops = graph_list[i].selfloop_edges()\n",
    "        if len(edges_with_selfloops)>0:\n",
    "            graph_list[i].remove_edges_from(edges_with_selfloops)\n",
    "        if is_real:\n",
    "            graph_list[i] = max(nx.connected_component_subgraphs(graph_list[i]), key=len)\n",
    "            graph_list[i] = nx.convert_node_labels_to_integers(graph_list[i])\n",
    "        else:\n",
    "            graph_list[i] = pick_connected_component_new(graph_list[i])\n",
    "    return graph_list\n",
    "\n",
    "class CommunityDataset(Dataset):\n",
    "     def __init__(self, adj, node):\n",
    "         super(Dataset, self).__init__()\n",
    "         self.x = node\n",
    "         self.adj = adj\n",
    "        \n",
    "     def __len__(self):\n",
    "         return len(self.adj)\n",
    "        \n",
    "     def __getitem__(self, index):\n",
    "        return self.adj[index], self.x[index]\n",
    "\n",
    "dataset_path = 'GraphRNN/graphs/GraphRNN_RNN_caveman_small_4_64_train_0.dat'\n",
    "graph_list = load_graph_list(dataset_path)\n",
    "adj_features_list = []\n",
    "node_features_list = []\n",
    "max_node_num = max([g.number_of_nodes() for g in graph_list])\n",
    "print(max_node_num)\n",
    "max_edge_num = max([g.number_of_edges() for g in graph_list])\n",
    "print(max_edge_num)\n",
    "degrees = [list(g.degree().values()) for g in graph_list]\n",
    "degree_list = list(set([item for sublist in degrees for item in sublist]))\n",
    "print(degree_list)\n",
    "\n",
    "for g in graph_list:\n",
    "    adj_tensor = torch.zeros((2, max_node_num, max_node_num))\n",
    "    adj_tensor[0, :g.number_of_nodes(), :g.number_of_nodes()] = torch.tensor(nx.adjacency_matrix(g).toarray())\n",
    "    adj_tensor[1, :g.number_of_nodes(), :g.number_of_nodes()] = 1 - torch.tensor(nx.adjacency_matrix(g).toarray())\n",
    "    # one-hot encoding\n",
    "    node_degree = np.array(list(g.degree().values()), dtype=int) - 1\n",
    "    #(B, N, node_dim)\n",
    "    node_tensor = torch.zeros(max_node_num, len(degree_list))\n",
    "    for idx, deg in enumerate(node_degree):\n",
    "        node_tensor[idx, degree_list == deg] = 1\n",
    "\n",
    "    #(B, 1, N, N)\n",
    "    # adj_tensor = adj_tensor.unsqueeze(0)\n",
    "    adj_features_list.append(adj_tensor)\n",
    "    node_features_list.append(node_tensor)\n",
    "\n",
    "split = 0.8\n",
    "num_samples = len(adj_features_list)\n",
    "training_num = math.ceil(split*num_samples)\n",
    "training_adj_list = adj_features_list[:training_num].copy()\n",
    "training_node_list = node_features_list[:training_num].copy()\n",
    "data_train = CommunityDataset(training_adj_list, training_node_list)\n",
    "\n",
    "data_train_loader = DataLoader(data_train, batch_size=16, shuffle=True)\n",
    "for batch_idx, data_batch in enumerate(data_train_loader):\n",
    "    if batch_idx == 0:\n",
    "        adj_features, node_features = data_batch \n",
    "        #(B, N, node_dim)\n",
    "        print('node tensor size:', node_features.shape)\n",
    "        #(B, 2, N, N)\n",
    "        print('adj tensor size:', adj_features.shape)\n",
    "print(len(data_train_loader.dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dig.ggraph.method import GraphDF, GraphAF\n",
    "import json\n",
    "from dig.ggraph.dataset import ZINC250k\n",
    "from torch_geometric.loader import DenseDataLoader\n",
    "import numpy as np\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "# torch.cuda.set_device(4)\n",
    "\n",
    "runner = GraphAF()\n",
    "\n",
    "config_dict =  {\n",
    "        \"max_size\": 20,\n",
    "        \"edge_unroll\": 20,\n",
    "        \"node_dim\": 10,\n",
    "        \"bond_dim\": 2,\n",
    "        \"num_flow_layer\": 12,\n",
    "        \"num_rgcn_layer\": 3,\n",
    "        \"nhid\": 128,\n",
    "        \"nout\": 128,\n",
    "        \"deq_coeff\": 0.9,\n",
    "        \"st_type\": \"exp\",\n",
    "        \"use_gpu\": True,\n",
    "        \"use_df\": False\n",
    "    }\n",
    "\n",
    "\n",
    "runner.train_rand_gen(loader=data_train_loader, lr = 0.001, wd = 0, max_epochs = 1000, \n",
    "                        model_conf_dict = config_dict, save_interval = 200, save_dir = 'community_test_AF')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Generating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "latent node length: 200\n",
      "latent edge length: 380\n",
      "max_atoms 20\n",
      "i= 0\n",
      "edge_total 0\n",
      "i= 1\n",
      "edge_total 1\n",
      "j= 0\n",
      "i= 2\n",
      "edge_total 2\n",
      "j= 0\n",
      "j= 1\n",
      "i= 3\n",
      "edge_total 3\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "i= 4\n",
      "edge_total 4\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "i= 5\n",
      "edge_total 5\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "i= 6\n",
      "edge_total 6\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "i= 7\n",
      "edge_total 7\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "i= 8\n",
      "edge_total 8\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "i= 9\n",
      "edge_total 9\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "i= 10\n",
      "edge_total 10\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "i= 11\n",
      "edge_total 11\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "j= 10\n",
      "i= 12\n",
      "edge_total 12\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "j= 10\n",
      "j= 11\n",
      "i= 13\n",
      "edge_total 13\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "j= 10\n",
      "j= 11\n",
      "j= 12\n",
      "i= 14\n",
      "edge_total 14\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "j= 10\n",
      "j= 11\n",
      "j= 12\n",
      "j= 13\n",
      "i= 15\n",
      "edge_total 15\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "j= 10\n",
      "j= 11\n",
      "j= 12\n",
      "j= 13\n",
      "j= 14\n",
      "i= 16\n",
      "edge_total 16\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "j= 10\n",
      "j= 11\n",
      "j= 12\n",
      "j= 13\n",
      "j= 14\n",
      "j= 15\n",
      "i= 17\n",
      "edge_total 17\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "j= 10\n",
      "j= 11\n",
      "j= 12\n",
      "j= 13\n",
      "j= 14\n",
      "j= 15\n",
      "j= 16\n",
      "i= 18\n",
      "edge_total 18\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "j= 10\n",
      "j= 11\n",
      "j= 12\n",
      "j= 13\n",
      "j= 14\n",
      "j= 15\n",
      "j= 16\n",
      "j= 17\n",
      "i= 19\n",
      "edge_total 19\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "j= 10\n",
      "j= 11\n",
      "j= 12\n",
      "j= 13\n",
      "j= 14\n",
      "j= 15\n",
      "j= 16\n",
      "j= 17\n",
      "j= 18\n",
      "max_atoms 20\n",
      "i= 0\n",
      "edge_total 0\n",
      "i= 1\n",
      "edge_total 1\n",
      "j= 0\n",
      "i= 2\n",
      "edge_total 2\n",
      "j= 0\n",
      "j= 1\n",
      "i= 3\n",
      "edge_total 3\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "i= 4\n",
      "edge_total 4\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "i= 5\n",
      "edge_total 5\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "i= 6\n",
      "edge_total 6\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "i= 7\n",
      "edge_total 7\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "i= 8\n",
      "edge_total 8\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "i= 9\n",
      "edge_total 9\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "i= 10\n",
      "edge_total 10\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "i= 11\n",
      "edge_total 11\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "j= 10\n",
      "i= 12\n",
      "edge_total 12\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "j= 10\n",
      "j= 11\n",
      "i= 13\n",
      "edge_total 13\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "j= 10\n",
      "j= 11\n",
      "j= 12\n",
      "i= 14\n",
      "edge_total 14\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "j= 10\n",
      "j= 11\n",
      "j= 12\n",
      "j= 13\n",
      "i= 15\n",
      "edge_total 15\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "j= 10\n",
      "j= 11\n",
      "j= 12\n",
      "j= 13\n",
      "j= 14\n",
      "i= 16\n",
      "edge_total 16\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "j= 10\n",
      "j= 11\n",
      "j= 12\n",
      "j= 13\n",
      "j= 14\n",
      "j= 15\n",
      "i= 17\n",
      "edge_total 17\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "j= 10\n",
      "j= 11\n",
      "j= 12\n",
      "j= 13\n",
      "j= 14\n",
      "j= 15\n",
      "j= 16\n",
      "i= 18\n",
      "edge_total 18\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "j= 10\n",
      "j= 11\n",
      "j= 12\n",
      "j= 13\n",
      "j= 14\n",
      "j= 15\n",
      "j= 16\n",
      "j= 17\n",
      "i= 19\n",
      "edge_total 19\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "j= 10\n",
      "j= 11\n",
      "j= 12\n",
      "j= 13\n",
      "j= 14\n",
      "j= 15\n",
      "j= 16\n",
      "j= 17\n",
      "j= 18\n",
      "max_atoms 20\n",
      "i= 0\n",
      "edge_total 0\n",
      "i= 1\n",
      "edge_total 1\n",
      "j= 0\n",
      "i= 2\n",
      "edge_total 2\n",
      "j= 0\n",
      "j= 1\n",
      "i= 3\n",
      "edge_total 3\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "i= 4\n",
      "edge_total 4\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "i= 5\n",
      "edge_total 5\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "i= 6\n",
      "edge_total 6\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "i= 7\n",
      "edge_total 7\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "i= 8\n",
      "edge_total 8\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "i= 9\n",
      "edge_total 9\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "i= 10\n",
      "edge_total 10\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "i= 11\n",
      "edge_total 11\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "j= 10\n",
      "i= 12\n",
      "edge_total 12\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "j= 10\n",
      "j= 11\n",
      "i= 13\n",
      "edge_total 13\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "j= 10\n",
      "j= 11\n",
      "j= 12\n",
      "i= 14\n",
      "edge_total 14\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "j= 10\n",
      "j= 11\n",
      "j= 12\n",
      "j= 13\n",
      "i= 15\n",
      "edge_total 15\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "j= 10\n",
      "j= 11\n",
      "j= 12\n",
      "j= 13\n",
      "j= 14\n",
      "i= 16\n",
      "edge_total 16\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "j= 10\n",
      "j= 11\n",
      "j= 12\n",
      "j= 13\n",
      "j= 14\n",
      "j= 15\n",
      "i= 17\n",
      "edge_total 17\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "j= 10\n",
      "j= 11\n",
      "j= 12\n",
      "j= 13\n",
      "j= 14\n",
      "j= 15\n",
      "j= 16\n",
      "i= 18\n",
      "edge_total 18\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "j= 10\n",
      "j= 11\n",
      "j= 12\n",
      "j= 13\n",
      "j= 14\n",
      "j= 15\n",
      "j= 16\n",
      "j= 17\n",
      "i= 19\n",
      "edge_total 19\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "j= 10\n",
      "j= 11\n",
      "j= 12\n",
      "j= 13\n",
      "j= 14\n",
      "j= 15\n",
      "j= 16\n",
      "j= 17\n",
      "j= 18\n",
      "max_atoms 20\n",
      "i= 0\n",
      "edge_total 0\n",
      "i= 1\n",
      "edge_total 1\n",
      "j= 0\n",
      "i= 2\n",
      "edge_total 2\n",
      "j= 0\n",
      "j= 1\n",
      "i= 3\n",
      "edge_total 3\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "i= 4\n",
      "edge_total 4\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "i= 5\n",
      "edge_total 5\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "i= 6\n",
      "edge_total 6\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "i= 7\n",
      "edge_total 7\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "i= 8\n",
      "edge_total 8\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "i= 9\n",
      "edge_total 9\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "i= 10\n",
      "edge_total 10\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "i= 11\n",
      "edge_total 11\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "j= 10\n",
      "i= 12\n",
      "edge_total 12\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "j= 10\n",
      "j= 11\n",
      "i= 13\n",
      "edge_total 13\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "j= 10\n",
      "j= 11\n",
      "j= 12\n",
      "i= 14\n",
      "edge_total 14\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "j= 10\n",
      "j= 11\n",
      "j= 12\n",
      "j= 13\n",
      "i= 15\n",
      "edge_total 15\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "j= 10\n",
      "j= 11\n",
      "j= 12\n",
      "j= 13\n",
      "j= 14\n",
      "i= 16\n",
      "edge_total 16\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "j= 10\n",
      "j= 11\n",
      "j= 12\n",
      "j= 13\n",
      "j= 14\n",
      "j= 15\n",
      "i= 17\n",
      "edge_total 17\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "j= 10\n",
      "j= 11\n",
      "j= 12\n",
      "j= 13\n",
      "j= 14\n",
      "j= 15\n",
      "j= 16\n",
      "i= 18\n",
      "edge_total 18\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "j= 10\n",
      "j= 11\n",
      "j= 12\n",
      "j= 13\n",
      "j= 14\n",
      "j= 15\n",
      "j= 16\n",
      "j= 17\n",
      "i= 19\n",
      "edge_total 19\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "j= 10\n",
      "j= 11\n",
      "j= 12\n",
      "j= 13\n",
      "j= 14\n",
      "j= 15\n",
      "j= 16\n",
      "j= 17\n",
      "j= 18\n",
      "max_atoms 20\n",
      "i= 0\n",
      "edge_total 0\n",
      "i= 1\n",
      "edge_total 1\n",
      "j= 0\n",
      "i= 2\n",
      "edge_total 2\n",
      "j= 0\n",
      "j= 1\n",
      "i= 3\n",
      "edge_total 3\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "i= 4\n",
      "edge_total 4\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "i= 5\n",
      "edge_total 5\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "i= 6\n",
      "edge_total 6\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "i= 7\n",
      "edge_total 7\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "i= 8\n",
      "edge_total 8\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "i= 9\n",
      "edge_total 9\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "i= 10\n",
      "edge_total 10\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "i= 11\n",
      "edge_total 11\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "j= 10\n",
      "i= 12\n",
      "edge_total 12\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "j= 10\n",
      "j= 11\n",
      "i= 13\n",
      "edge_total 13\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "j= 10\n",
      "j= 11\n",
      "j= 12\n",
      "i= 14\n",
      "edge_total 14\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "j= 10\n",
      "j= 11\n",
      "j= 12\n",
      "j= 13\n",
      "i= 15\n",
      "edge_total 15\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "j= 10\n",
      "j= 11\n",
      "j= 12\n",
      "j= 13\n",
      "j= 14\n",
      "i= 16\n",
      "edge_total 16\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "j= 10\n",
      "j= 11\n",
      "j= 12\n",
      "j= 13\n",
      "j= 14\n",
      "j= 15\n",
      "i= 17\n",
      "edge_total 17\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "j= 10\n",
      "j= 11\n",
      "j= 12\n",
      "j= 13\n",
      "j= 14\n",
      "j= 15\n",
      "j= 16\n",
      "i= 18\n",
      "edge_total 18\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "j= 10\n",
      "j= 11\n",
      "j= 12\n",
      "j= 13\n",
      "j= 14\n",
      "j= 15\n",
      "j= 16\n",
      "j= 17\n",
      "i= 19\n",
      "edge_total 19\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "j= 10\n",
      "j= 11\n",
      "j= 12\n",
      "j= 13\n",
      "j= 14\n",
      "j= 15\n",
      "j= 16\n",
      "j= 17\n",
      "j= 18\n",
      "max_atoms 20\n",
      "i= 0\n",
      "edge_total 0\n",
      "i= 1\n",
      "edge_total 1\n",
      "j= 0\n",
      "i= 2\n",
      "edge_total 2\n",
      "j= 0\n",
      "j= 1\n",
      "i= 3\n",
      "edge_total 3\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "i= 4\n",
      "edge_total 4\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "i= 5\n",
      "edge_total 5\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "i= 6\n",
      "edge_total 6\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "i= 7\n",
      "edge_total 7\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "i= 8\n",
      "edge_total 8\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "i= 9\n",
      "edge_total 9\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "i= 10\n",
      "edge_total 10\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "i= 11\n",
      "edge_total 11\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "j= 10\n",
      "i= 12\n",
      "edge_total 12\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "j= 10\n",
      "j= 11\n",
      "i= 13\n",
      "edge_total 13\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "j= 10\n",
      "j= 11\n",
      "j= 12\n",
      "i= 14\n",
      "edge_total 14\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "j= 10\n",
      "j= 11\n",
      "j= 12\n",
      "j= 13\n",
      "i= 15\n",
      "edge_total 15\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "j= 10\n",
      "j= 11\n",
      "j= 12\n",
      "j= 13\n",
      "j= 14\n",
      "i= 16\n",
      "edge_total 16\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "j= 10\n",
      "j= 11\n",
      "j= 12\n",
      "j= 13\n",
      "j= 14\n",
      "j= 15\n",
      "i= 17\n",
      "edge_total 17\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "j= 10\n",
      "j= 11\n",
      "j= 12\n",
      "j= 13\n",
      "j= 14\n",
      "j= 15\n",
      "j= 16\n",
      "i= 18\n",
      "edge_total 18\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "j= 10\n",
      "j= 11\n",
      "j= 12\n",
      "j= 13\n",
      "j= 14\n",
      "j= 15\n",
      "j= 16\n",
      "j= 17\n",
      "i= 19\n",
      "edge_total 19\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "j= 10\n",
      "j= 11\n",
      "j= 12\n",
      "j= 13\n",
      "j= 14\n",
      "j= 15\n",
      "j= 16\n",
      "j= 17\n",
      "j= 18\n",
      "max_atoms 20\n",
      "i= 0\n",
      "edge_total 0\n",
      "i= 1\n",
      "edge_total 1\n",
      "j= 0\n",
      "i= 2\n",
      "edge_total 2\n",
      "j= 0\n",
      "j= 1\n",
      "i= 3\n",
      "edge_total 3\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "i= 4\n",
      "edge_total 4\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "i= 5\n",
      "edge_total 5\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "i= 6\n",
      "edge_total 6\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "i= 7\n",
      "edge_total 7\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "i= 8\n",
      "edge_total 8\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "i= 9\n",
      "edge_total 9\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "i= 10\n",
      "edge_total 10\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "i= 11\n",
      "edge_total 11\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "j= 10\n",
      "i= 12\n",
      "edge_total 12\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "j= 10\n",
      "j= 11\n",
      "i= 13\n",
      "edge_total 13\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "j= 10\n",
      "j= 11\n",
      "j= 12\n",
      "i= 14\n",
      "edge_total 14\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "j= 10\n",
      "j= 11\n",
      "j= 12\n",
      "j= 13\n",
      "i= 15\n",
      "edge_total 15\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "j= 10\n",
      "j= 11\n",
      "j= 12\n",
      "j= 13\n",
      "j= 14\n",
      "i= 16\n",
      "edge_total 16\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "j= 10\n",
      "j= 11\n",
      "j= 12\n",
      "j= 13\n",
      "j= 14\n",
      "j= 15\n",
      "i= 17\n",
      "edge_total 17\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "j= 10\n",
      "j= 11\n",
      "j= 12\n",
      "j= 13\n",
      "j= 14\n",
      "j= 15\n",
      "j= 16\n",
      "i= 18\n",
      "edge_total 18\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "j= 10\n",
      "j= 11\n",
      "j= 12\n",
      "j= 13\n",
      "j= 14\n",
      "j= 15\n",
      "j= 16\n",
      "j= 17\n",
      "i= 19\n",
      "edge_total 19\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "j= 10\n",
      "j= 11\n",
      "j= 12\n",
      "j= 13\n",
      "j= 14\n",
      "j= 15\n",
      "j= 16\n",
      "j= 17\n",
      "j= 18\n",
      "max_atoms 20\n",
      "i= 0\n",
      "edge_total 0\n",
      "i= 1\n",
      "edge_total 1\n",
      "j= 0\n",
      "i= 2\n",
      "edge_total 2\n",
      "j= 0\n",
      "j= 1\n",
      "i= 3\n",
      "edge_total 3\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "i= 4\n",
      "edge_total 4\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "i= 5\n",
      "edge_total 5\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "i= 6\n",
      "edge_total 6\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "i= 7\n",
      "edge_total 7\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "i= 8\n",
      "edge_total 8\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "i= 9\n",
      "edge_total 9\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "i= 10\n",
      "edge_total 10\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "i= 11\n",
      "edge_total 11\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "j= 10\n",
      "i= 12\n",
      "edge_total 12\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "j= 10\n",
      "j= 11\n",
      "i= 13\n",
      "edge_total 13\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "j= 10\n",
      "j= 11\n",
      "j= 12\n",
      "i= 14\n",
      "edge_total 14\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "j= 10\n",
      "j= 11\n",
      "j= 12\n",
      "j= 13\n",
      "i= 15\n",
      "edge_total 15\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "j= 10\n",
      "j= 11\n",
      "j= 12\n",
      "j= 13\n",
      "j= 14\n",
      "i= 16\n",
      "edge_total 16\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "j= 10\n",
      "j= 11\n",
      "j= 12\n",
      "j= 13\n",
      "j= 14\n",
      "j= 15\n",
      "i= 17\n",
      "edge_total 17\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "j= 10\n",
      "j= 11\n",
      "j= 12\n",
      "j= 13\n",
      "j= 14\n",
      "j= 15\n",
      "j= 16\n",
      "i= 18\n",
      "edge_total 18\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "j= 10\n",
      "j= 11\n",
      "j= 12\n",
      "j= 13\n",
      "j= 14\n",
      "j= 15\n",
      "j= 16\n",
      "j= 17\n",
      "i= 19\n",
      "edge_total 19\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "j= 10\n",
      "j= 11\n",
      "j= 12\n",
      "j= 13\n",
      "j= 14\n",
      "j= 15\n",
      "j= 16\n",
      "j= 17\n",
      "j= 18\n",
      "max_atoms 20\n",
      "i= 0\n",
      "edge_total 0\n",
      "i= 1\n",
      "edge_total 1\n",
      "j= 0\n",
      "i= 2\n",
      "edge_total 2\n",
      "j= 0\n",
      "j= 1\n",
      "i= 3\n",
      "edge_total 3\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "i= 4\n",
      "edge_total 4\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "i= 5\n",
      "edge_total 5\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "i= 6\n",
      "edge_total 6\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "i= 7\n",
      "edge_total 7\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "i= 8\n",
      "edge_total 8\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "i= 9\n",
      "edge_total 9\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "i= 10\n",
      "edge_total 10\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "i= 11\n",
      "edge_total 11\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "j= 10\n",
      "i= 12\n",
      "edge_total 12\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "j= 10\n",
      "j= 11\n",
      "i= 13\n",
      "edge_total 13\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "j= 10\n",
      "j= 11\n",
      "j= 12\n",
      "i= 14\n",
      "edge_total 14\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "j= 10\n",
      "j= 11\n",
      "j= 12\n",
      "j= 13\n",
      "i= 15\n",
      "edge_total 15\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "j= 10\n",
      "j= 11\n",
      "j= 12\n",
      "j= 13\n",
      "j= 14\n",
      "i= 16\n",
      "edge_total 16\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "j= 10\n",
      "j= 11\n",
      "j= 12\n",
      "j= 13\n",
      "j= 14\n",
      "j= 15\n",
      "i= 17\n",
      "edge_total 17\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "j= 10\n",
      "j= 11\n",
      "j= 12\n",
      "j= 13\n",
      "j= 14\n",
      "j= 15\n",
      "j= 16\n",
      "i= 18\n",
      "edge_total 18\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "j= 10\n",
      "j= 11\n",
      "j= 12\n",
      "j= 13\n",
      "j= 14\n",
      "j= 15\n",
      "j= 16\n",
      "j= 17\n",
      "i= 19\n",
      "edge_total 19\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "j= 10\n",
      "j= 11\n",
      "j= 12\n",
      "j= 13\n",
      "j= 14\n",
      "j= 15\n",
      "j= 16\n",
      "j= 17\n",
      "j= 18\n",
      "max_atoms 20\n",
      "i= 0\n",
      "edge_total 0\n",
      "i= 1\n",
      "edge_total 1\n",
      "j= 0\n",
      "i= 2\n",
      "edge_total 2\n",
      "j= 0\n",
      "j= 1\n",
      "i= 3\n",
      "edge_total 3\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "i= 4\n",
      "edge_total 4\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "i= 5\n",
      "edge_total 5\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "i= 6\n",
      "edge_total 6\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "i= 7\n",
      "edge_total 7\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "i= 8\n",
      "edge_total 8\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "i= 9\n",
      "edge_total 9\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "i= 10\n",
      "edge_total 10\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "i= 11\n",
      "edge_total 11\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "j= 10\n",
      "i= 12\n",
      "edge_total 12\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "j= 10\n",
      "j= 11\n",
      "i= 13\n",
      "edge_total 13\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "j= 10\n",
      "j= 11\n",
      "j= 12\n",
      "i= 14\n",
      "edge_total 14\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "j= 10\n",
      "j= 11\n",
      "j= 12\n",
      "j= 13\n",
      "i= 15\n",
      "edge_total 15\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "j= 10\n",
      "j= 11\n",
      "j= 12\n",
      "j= 13\n",
      "j= 14\n",
      "i= 16\n",
      "edge_total 16\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "j= 10\n",
      "j= 11\n",
      "j= 12\n",
      "j= 13\n",
      "j= 14\n",
      "j= 15\n",
      "i= 17\n",
      "edge_total 17\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "j= 10\n",
      "j= 11\n",
      "j= 12\n",
      "j= 13\n",
      "j= 14\n",
      "j= 15\n",
      "j= 16\n",
      "i= 18\n",
      "edge_total 18\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "j= 10\n",
      "j= 11\n",
      "j= 12\n",
      "j= 13\n",
      "j= 14\n",
      "j= 15\n",
      "j= 16\n",
      "j= 17\n",
      "i= 19\n",
      "edge_total 19\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "j= 10\n",
      "j= 11\n",
      "j= 12\n",
      "j= 13\n",
      "j= 14\n",
      "j= 15\n",
      "j= 16\n",
      "j= 17\n",
      "j= 18\n",
      "Generated 10 molecules\n",
      "max_atoms 20\n",
      "i= 0\n",
      "edge_total 0\n",
      "i= 1\n",
      "edge_total 1\n",
      "j= 0\n",
      "i= 2\n",
      "edge_total 2\n",
      "j= 0\n",
      "j= 1\n",
      "i= 3\n",
      "edge_total 3\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "i= 4\n",
      "edge_total 4\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "i= 5\n",
      "edge_total 5\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "i= 6\n",
      "edge_total 6\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "i= 7\n",
      "edge_total 7\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "i= 8\n",
      "edge_total 8\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "i= 9\n",
      "edge_total 9\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "i= 10\n",
      "edge_total 10\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "i= 11\n",
      "edge_total 11\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "j= 10\n",
      "i= 12\n",
      "edge_total 12\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "j= 10\n",
      "j= 11\n",
      "i= 13\n",
      "edge_total 13\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "j= 10\n",
      "j= 11\n",
      "j= 12\n",
      "i= 14\n",
      "edge_total 14\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "j= 10\n",
      "j= 11\n",
      "j= 12\n",
      "j= 13\n",
      "i= 15\n",
      "edge_total 15\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "j= 10\n",
      "j= 11\n",
      "j= 12\n",
      "j= 13\n",
      "j= 14\n",
      "i= 16\n",
      "edge_total 16\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "j= 10\n",
      "j= 11\n",
      "j= 12\n",
      "j= 13\n",
      "j= 14\n",
      "j= 15\n",
      "i= 17\n",
      "edge_total 17\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "j= 10\n",
      "j= 11\n",
      "j= 12\n",
      "j= 13\n",
      "j= 14\n",
      "j= 15\n",
      "j= 16\n",
      "i= 18\n",
      "edge_total 18\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "j= 10\n",
      "j= 11\n",
      "j= 12\n",
      "j= 13\n",
      "j= 14\n",
      "j= 15\n",
      "j= 16\n",
      "j= 17\n",
      "i= 19\n",
      "edge_total 19\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "j= 10\n",
      "j= 11\n",
      "j= 12\n",
      "j= 13\n",
      "j= 14\n",
      "j= 15\n",
      "j= 16\n",
      "j= 17\n",
      "j= 18\n",
      "max_atoms 20\n",
      "i= 0\n",
      "edge_total 0\n",
      "i= 1\n",
      "edge_total 1\n",
      "j= 0\n",
      "i= 2\n",
      "edge_total 2\n",
      "j= 0\n",
      "j= 1\n",
      "i= 3\n",
      "edge_total 3\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "i= 4\n",
      "edge_total 4\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "i= 5\n",
      "edge_total 5\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "i= 6\n",
      "edge_total 6\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "i= 7\n",
      "edge_total 7\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "i= 8\n",
      "edge_total 8\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "i= 9\n",
      "edge_total 9\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "i= 10\n",
      "edge_total 10\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "i= 11\n",
      "edge_total 11\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "j= 10\n",
      "i= 12\n",
      "edge_total 12\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "j= 10\n",
      "j= 11\n",
      "i= 13\n",
      "edge_total 13\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "j= 10\n",
      "j= 11\n",
      "j= 12\n",
      "i= 14\n",
      "edge_total 14\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "j= 10\n",
      "j= 11\n",
      "j= 12\n",
      "j= 13\n",
      "i= 15\n",
      "edge_total 15\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "j= 10\n",
      "j= 11\n",
      "j= 12\n",
      "j= 13\n",
      "j= 14\n",
      "i= 16\n",
      "edge_total 16\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "j= 10\n",
      "j= 11\n",
      "j= 12\n",
      "j= 13\n",
      "j= 14\n",
      "j= 15\n",
      "i= 17\n",
      "edge_total 17\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "j= 10\n",
      "j= 11\n",
      "j= 12\n",
      "j= 13\n",
      "j= 14\n",
      "j= 15\n",
      "j= 16\n",
      "i= 18\n",
      "edge_total 18\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "j= 10\n",
      "j= 11\n",
      "j= 12\n",
      "j= 13\n",
      "j= 14\n",
      "j= 15\n",
      "j= 16\n",
      "j= 17\n",
      "i= 19\n",
      "edge_total 19\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "j= 10\n",
      "j= 11\n",
      "j= 12\n",
      "j= 13\n",
      "j= 14\n",
      "j= 15\n",
      "j= 16\n",
      "j= 17\n",
      "j= 18\n",
      "max_atoms 20\n",
      "i= 0\n",
      "edge_total 0\n",
      "i= 1\n",
      "edge_total 1\n",
      "j= 0\n",
      "i= 2\n",
      "edge_total 2\n",
      "j= 0\n",
      "j= 1\n",
      "i= 3\n",
      "edge_total 3\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "i= 4\n",
      "edge_total 4\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "i= 5\n",
      "edge_total 5\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "i= 6\n",
      "edge_total 6\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "i= 7\n",
      "edge_total 7\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "i= 8\n",
      "edge_total 8\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "i= 9\n",
      "edge_total 9\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "i= 10\n",
      "edge_total 10\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "i= 11\n",
      "edge_total 11\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "j= 10\n",
      "i= 12\n",
      "edge_total 12\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "j= 10\n",
      "j= 11\n",
      "i= 13\n",
      "edge_total 13\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "j= 10\n",
      "j= 11\n",
      "j= 12\n",
      "i= 14\n",
      "edge_total 14\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "j= 10\n",
      "j= 11\n",
      "j= 12\n",
      "j= 13\n",
      "i= 15\n",
      "edge_total 15\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "j= 10\n",
      "j= 11\n",
      "j= 12\n",
      "j= 13\n",
      "j= 14\n",
      "i= 16\n",
      "edge_total 16\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "j= 10\n",
      "j= 11\n",
      "j= 12\n",
      "j= 13\n",
      "j= 14\n",
      "j= 15\n",
      "i= 17\n",
      "edge_total 17\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "j= 10\n",
      "j= 11\n",
      "j= 12\n",
      "j= 13\n",
      "j= 14\n",
      "j= 15\n",
      "j= 16\n",
      "i= 18\n",
      "edge_total 18\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "j= 10\n",
      "j= 11\n",
      "j= 12\n",
      "j= 13\n",
      "j= 14\n",
      "j= 15\n",
      "j= 16\n",
      "j= 17\n",
      "i= 19\n",
      "edge_total 19\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "j= 10\n",
      "j= 11\n",
      "j= 12\n",
      "j= 13\n",
      "j= 14\n",
      "j= 15\n",
      "j= 16\n",
      "j= 17\n",
      "j= 18\n",
      "max_atoms 20\n",
      "i= 0\n",
      "edge_total 0\n",
      "i= 1\n",
      "edge_total 1\n",
      "j= 0\n",
      "i= 2\n",
      "edge_total 2\n",
      "j= 0\n",
      "j= 1\n",
      "i= 3\n",
      "edge_total 3\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "i= 4\n",
      "edge_total 4\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "i= 5\n",
      "edge_total 5\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "i= 6\n",
      "edge_total 6\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "i= 7\n",
      "edge_total 7\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "i= 8\n",
      "edge_total 8\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "i= 9\n",
      "edge_total 9\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "i= 10\n",
      "edge_total 10\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "i= 11\n",
      "edge_total 11\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "j= 10\n",
      "i= 12\n",
      "edge_total 12\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "j= 10\n",
      "j= 11\n",
      "i= 13\n",
      "edge_total 13\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "j= 10\n",
      "j= 11\n",
      "j= 12\n",
      "i= 14\n",
      "edge_total 14\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "j= 10\n",
      "j= 11\n",
      "j= 12\n",
      "j= 13\n",
      "i= 15\n",
      "edge_total 15\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "j= 10\n",
      "j= 11\n",
      "j= 12\n",
      "j= 13\n",
      "j= 14\n",
      "i= 16\n",
      "edge_total 16\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "j= 10\n",
      "j= 11\n",
      "j= 12\n",
      "j= 13\n",
      "j= 14\n",
      "j= 15\n",
      "i= 17\n",
      "edge_total 17\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "j= 10\n",
      "j= 11\n",
      "j= 12\n",
      "j= 13\n",
      "j= 14\n",
      "j= 15\n",
      "j= 16\n",
      "i= 18\n",
      "edge_total 18\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "j= 10\n",
      "j= 11\n",
      "j= 12\n",
      "j= 13\n",
      "j= 14\n",
      "j= 15\n",
      "j= 16\n",
      "j= 17\n",
      "i= 19\n",
      "edge_total 19\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "j= 10\n",
      "j= 11\n",
      "j= 12\n",
      "j= 13\n",
      "j= 14\n",
      "j= 15\n",
      "j= 16\n",
      "j= 17\n",
      "j= 18\n",
      "max_atoms 20\n",
      "i= 0\n",
      "edge_total 0\n",
      "i= 1\n",
      "edge_total 1\n",
      "j= 0\n",
      "i= 2\n",
      "edge_total 2\n",
      "j= 0\n",
      "j= 1\n",
      "i= 3\n",
      "edge_total 3\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "i= 4\n",
      "edge_total 4\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "i= 5\n",
      "edge_total 5\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "i= 6\n",
      "edge_total 6\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "i= 7\n",
      "edge_total 7\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "i= 8\n",
      "edge_total 8\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "i= 9\n",
      "edge_total 9\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "i= 10\n",
      "edge_total 10\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "i= 11\n",
      "edge_total 11\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "j= 10\n",
      "i= 12\n",
      "edge_total 12\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "j= 10\n",
      "j= 11\n",
      "i= 13\n",
      "edge_total 13\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "j= 10\n",
      "j= 11\n",
      "j= 12\n",
      "i= 14\n",
      "edge_total 14\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "j= 10\n",
      "j= 11\n",
      "j= 12\n",
      "j= 13\n",
      "i= 15\n",
      "edge_total 15\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "j= 10\n",
      "j= 11\n",
      "j= 12\n",
      "j= 13\n",
      "j= 14\n",
      "i= 16\n",
      "edge_total 16\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "j= 10\n",
      "j= 11\n",
      "j= 12\n",
      "j= 13\n",
      "j= 14\n",
      "j= 15\n",
      "i= 17\n",
      "edge_total 17\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "j= 10\n",
      "j= 11\n",
      "j= 12\n",
      "j= 13\n",
      "j= 14\n",
      "j= 15\n",
      "j= 16\n",
      "i= 18\n",
      "edge_total 18\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "j= 10\n",
      "j= 11\n",
      "j= 12\n",
      "j= 13\n",
      "j= 14\n",
      "j= 15\n",
      "j= 16\n",
      "j= 17\n",
      "i= 19\n",
      "edge_total 19\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "j= 10\n",
      "j= 11\n",
      "j= 12\n",
      "j= 13\n",
      "j= 14\n",
      "j= 15\n",
      "j= 16\n",
      "j= 17\n",
      "j= 18\n",
      "max_atoms 20\n",
      "i= 0\n",
      "edge_total 0\n",
      "i= 1\n",
      "edge_total 1\n",
      "j= 0\n",
      "i= 2\n",
      "edge_total 2\n",
      "j= 0\n",
      "j= 1\n",
      "i= 3\n",
      "edge_total 3\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "i= 4\n",
      "edge_total 4\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "i= 5\n",
      "edge_total 5\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "i= 6\n",
      "edge_total 6\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "i= 7\n",
      "edge_total 7\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "i= 8\n",
      "edge_total 8\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "i= 9\n",
      "edge_total 9\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "i= 10\n",
      "edge_total 10\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "i= 11\n",
      "edge_total 11\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "j= 10\n",
      "i= 12\n",
      "edge_total 12\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "j= 10\n",
      "j= 11\n",
      "i= 13\n",
      "edge_total 13\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "j= 10\n",
      "j= 11\n",
      "j= 12\n",
      "i= 14\n",
      "edge_total 14\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "j= 10\n",
      "j= 11\n",
      "j= 12\n",
      "j= 13\n",
      "i= 15\n",
      "edge_total 15\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "j= 10\n",
      "j= 11\n",
      "j= 12\n",
      "j= 13\n",
      "j= 14\n",
      "i= 16\n",
      "edge_total 16\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "j= 10\n",
      "j= 11\n",
      "j= 12\n",
      "j= 13\n",
      "j= 14\n",
      "j= 15\n",
      "i= 17\n",
      "edge_total 17\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "j= 10\n",
      "j= 11\n",
      "j= 12\n",
      "j= 13\n",
      "j= 14\n",
      "j= 15\n",
      "j= 16\n",
      "i= 18\n",
      "edge_total 18\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "j= 10\n",
      "j= 11\n",
      "j= 12\n",
      "j= 13\n",
      "j= 14\n",
      "j= 15\n",
      "j= 16\n",
      "j= 17\n",
      "i= 19\n",
      "edge_total 19\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "j= 10\n",
      "j= 11\n",
      "j= 12\n",
      "j= 13\n",
      "j= 14\n",
      "j= 15\n",
      "j= 16\n",
      "j= 17\n",
      "j= 18\n",
      "max_atoms 20\n",
      "i= 0\n",
      "edge_total 0\n",
      "i= 1\n",
      "edge_total 1\n",
      "j= 0\n",
      "i= 2\n",
      "edge_total 2\n",
      "j= 0\n",
      "j= 1\n",
      "i= 3\n",
      "edge_total 3\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "i= 4\n",
      "edge_total 4\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "i= 5\n",
      "edge_total 5\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "i= 6\n",
      "edge_total 6\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "i= 7\n",
      "edge_total 7\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "i= 8\n",
      "edge_total 8\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "i= 9\n",
      "edge_total 9\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "i= 10\n",
      "edge_total 10\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "i= 11\n",
      "edge_total 11\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "j= 10\n",
      "i= 12\n",
      "edge_total 12\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "j= 10\n",
      "j= 11\n",
      "i= 13\n",
      "edge_total 13\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "j= 10\n",
      "j= 11\n",
      "j= 12\n",
      "i= 14\n",
      "edge_total 14\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "j= 10\n",
      "j= 11\n",
      "j= 12\n",
      "j= 13\n",
      "i= 15\n",
      "edge_total 15\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "j= 10\n",
      "j= 11\n",
      "j= 12\n",
      "j= 13\n",
      "j= 14\n",
      "i= 16\n",
      "edge_total 16\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "j= 10\n",
      "j= 11\n",
      "j= 12\n",
      "j= 13\n",
      "j= 14\n",
      "j= 15\n",
      "i= 17\n",
      "edge_total 17\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "j= 10\n",
      "j= 11\n",
      "j= 12\n",
      "j= 13\n",
      "j= 14\n",
      "j= 15\n",
      "j= 16\n",
      "i= 18\n",
      "edge_total 18\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "j= 10\n",
      "j= 11\n",
      "j= 12\n",
      "j= 13\n",
      "j= 14\n",
      "j= 15\n",
      "j= 16\n",
      "j= 17\n",
      "i= 19\n",
      "edge_total 19\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "j= 10\n",
      "j= 11\n",
      "j= 12\n",
      "j= 13\n",
      "j= 14\n",
      "j= 15\n",
      "j= 16\n",
      "j= 17\n",
      "j= 18\n",
      "max_atoms 20\n",
      "i= 0\n",
      "edge_total 0\n",
      "i= 1\n",
      "edge_total 1\n",
      "j= 0\n",
      "i= 2\n",
      "edge_total 2\n",
      "j= 0\n",
      "j= 1\n",
      "i= 3\n",
      "edge_total 3\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "i= 4\n",
      "edge_total 4\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "i= 5\n",
      "edge_total 5\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "i= 6\n",
      "edge_total 6\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "i= 7\n",
      "edge_total 7\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "i= 8\n",
      "edge_total 8\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "i= 9\n",
      "edge_total 9\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "i= 10\n",
      "edge_total 10\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "i= 11\n",
      "edge_total 11\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "j= 10\n",
      "i= 12\n",
      "edge_total 12\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "j= 10\n",
      "j= 11\n",
      "i= 13\n",
      "edge_total 13\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "j= 10\n",
      "j= 11\n",
      "j= 12\n",
      "i= 14\n",
      "edge_total 14\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "j= 10\n",
      "j= 11\n",
      "j= 12\n",
      "j= 13\n",
      "i= 15\n",
      "edge_total 15\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "j= 10\n",
      "j= 11\n",
      "j= 12\n",
      "j= 13\n",
      "j= 14\n",
      "i= 16\n",
      "edge_total 16\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "j= 10\n",
      "j= 11\n",
      "j= 12\n",
      "j= 13\n",
      "j= 14\n",
      "j= 15\n",
      "i= 17\n",
      "edge_total 17\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "j= 10\n",
      "j= 11\n",
      "j= 12\n",
      "j= 13\n",
      "j= 14\n",
      "j= 15\n",
      "j= 16\n",
      "i= 18\n",
      "edge_total 18\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "j= 10\n",
      "j= 11\n",
      "j= 12\n",
      "j= 13\n",
      "j= 14\n",
      "j= 15\n",
      "j= 16\n",
      "j= 17\n",
      "i= 19\n",
      "edge_total 19\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "j= 10\n",
      "j= 11\n",
      "j= 12\n",
      "j= 13\n",
      "j= 14\n",
      "j= 15\n",
      "j= 16\n",
      "j= 17\n",
      "j= 18\n",
      "max_atoms 20\n",
      "i= 0\n",
      "edge_total 0\n",
      "i= 1\n",
      "edge_total 1\n",
      "j= 0\n",
      "i= 2\n",
      "edge_total 2\n",
      "j= 0\n",
      "j= 1\n",
      "i= 3\n",
      "edge_total 3\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "i= 4\n",
      "edge_total 4\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "i= 5\n",
      "edge_total 5\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "i= 6\n",
      "edge_total 6\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "i= 7\n",
      "edge_total 7\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "i= 8\n",
      "edge_total 8\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "i= 9\n",
      "edge_total 9\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "i= 10\n",
      "edge_total 10\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "i= 11\n",
      "edge_total 11\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "j= 10\n",
      "i= 12\n",
      "edge_total 12\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "j= 10\n",
      "j= 11\n",
      "i= 13\n",
      "edge_total 13\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "j= 10\n",
      "j= 11\n",
      "j= 12\n",
      "i= 14\n",
      "edge_total 14\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "j= 10\n",
      "j= 11\n",
      "j= 12\n",
      "j= 13\n",
      "i= 15\n",
      "edge_total 15\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "j= 10\n",
      "j= 11\n",
      "j= 12\n",
      "j= 13\n",
      "j= 14\n",
      "i= 16\n",
      "edge_total 16\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "j= 10\n",
      "j= 11\n",
      "j= 12\n",
      "j= 13\n",
      "j= 14\n",
      "j= 15\n",
      "i= 17\n",
      "edge_total 17\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "j= 10\n",
      "j= 11\n",
      "j= 12\n",
      "j= 13\n",
      "j= 14\n",
      "j= 15\n",
      "j= 16\n",
      "i= 18\n",
      "edge_total 18\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "j= 10\n",
      "j= 11\n",
      "j= 12\n",
      "j= 13\n",
      "j= 14\n",
      "j= 15\n",
      "j= 16\n",
      "j= 17\n",
      "i= 19\n",
      "edge_total 19\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "j= 10\n",
      "j= 11\n",
      "j= 12\n",
      "j= 13\n",
      "j= 14\n",
      "j= 15\n",
      "j= 16\n",
      "j= 17\n",
      "j= 18\n",
      "max_atoms 20\n",
      "i= 0\n",
      "edge_total 0\n",
      "i= 1\n",
      "edge_total 1\n",
      "j= 0\n",
      "i= 2\n",
      "edge_total 2\n",
      "j= 0\n",
      "j= 1\n",
      "i= 3\n",
      "edge_total 3\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "i= 4\n",
      "edge_total 4\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "i= 5\n",
      "edge_total 5\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "i= 6\n",
      "edge_total 6\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "i= 7\n",
      "edge_total 7\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "i= 8\n",
      "edge_total 8\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "i= 9\n",
      "edge_total 9\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "i= 10\n",
      "edge_total 10\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "i= 11\n",
      "edge_total 11\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "j= 10\n",
      "i= 12\n",
      "edge_total 12\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "j= 10\n",
      "j= 11\n",
      "i= 13\n",
      "edge_total 13\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "j= 10\n",
      "j= 11\n",
      "j= 12\n",
      "i= 14\n",
      "edge_total 14\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "j= 10\n",
      "j= 11\n",
      "j= 12\n",
      "j= 13\n",
      "i= 15\n",
      "edge_total 15\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "j= 10\n",
      "j= 11\n",
      "j= 12\n",
      "j= 13\n",
      "j= 14\n",
      "i= 16\n",
      "edge_total 16\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "j= 10\n",
      "j= 11\n",
      "j= 12\n",
      "j= 13\n",
      "j= 14\n",
      "j= 15\n",
      "i= 17\n",
      "edge_total 17\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "j= 10\n",
      "j= 11\n",
      "j= 12\n",
      "j= 13\n",
      "j= 14\n",
      "j= 15\n",
      "j= 16\n",
      "i= 18\n",
      "edge_total 18\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "j= 10\n",
      "j= 11\n",
      "j= 12\n",
      "j= 13\n",
      "j= 14\n",
      "j= 15\n",
      "j= 16\n",
      "j= 17\n",
      "i= 19\n",
      "edge_total 19\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "j= 5\n",
      "j= 6\n",
      "j= 7\n",
      "j= 8\n",
      "j= 9\n",
      "j= 10\n",
      "j= 11\n",
      "j= 12\n",
      "j= 13\n",
      "j= 14\n",
      "j= 15\n",
      "j= 16\n",
      "j= 17\n",
      "j= 18\n",
      "Generated 20 molecules\n"
     ]
    }
   ],
   "source": [
    "from rdkit import RDLogger\n",
    "from dig.ggraph.method import GraphDF, GraphAF\n",
    "import json\n",
    "from dig.ggraph.dataset import ZINC250k\n",
    "from torch_geometric.loader import DenseDataLoader\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "runner = GraphAF()\n",
    "\n",
    "RDLogger.DisableLog('rdApp.*')\n",
    "ckpt_path = 'community_test_AF/rand_gen_ckpt_1000.pth'\n",
    "config_dict =  {\n",
    "        \"max_size\": 20,\n",
    "        \"edge_unroll\": 20,\n",
    "        \"node_dim\": 10,\n",
    "        \"bond_dim\": 2,\n",
    "        \"num_flow_layer\": 12,\n",
    "        \"num_rgcn_layer\": 3,\n",
    "        \"nhid\": 128,\n",
    "        \"nout\": 128,\n",
    "        \"deq_coeff\": 0.9,\n",
    "        \"st_type\": \"exp\",\n",
    "        \"use_gpu\": True,\n",
    "        \"use_df\": False\n",
    "    }\n",
    "atomic_list = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "# atomic_list = [0,1]\n",
    "n_mols = 20\n",
    "graph_list = runner.run_rand_gen(model_conf_dict = config_dict, checkpoint_path=ckpt_path, num_max_node=config_dict['max_size'],\n",
    "                            n_mols=n_mols, atomic_num_list = atomic_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.],\n",
      "        [0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.],\n",
      "        [0., 0., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.],\n",
      "        [0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 0.],\n",
      "        [0., 1., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 1., 1., 0., 0.,\n",
      "         0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 1., 0., 0.,\n",
      "         1., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 1.,\n",
      "         0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 1.,\n",
      "         0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "         1., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0.,\n",
      "         0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 1.,\n",
      "         1., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0.,\n",
      "         0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1.,\n",
      "         0., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         1., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1.,\n",
      "         1., 1.]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(graph_list[0,0,:,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration 0 | loss 0.2593346834182739\n",
      "Training iteration 1 | loss 0.3031226694583893\n",
      "Training iteration 2 | loss 0.27785301208496094\n",
      "Training iteration 3 | loss 0.2607676386833191\n",
      "Training iteration 4 | loss 0.2518436908721924\n",
      "Training | Average loss 0.27058433890342715\n",
      "Training iteration 0 | loss 0.24983400106430054\n",
      "Training iteration 1 | loss 0.2762548625469208\n",
      "Training iteration 2 | loss 0.2853888273239136\n",
      "Training iteration 3 | loss 0.2118375301361084\n",
      "Training iteration 4 | loss 0.3070693016052246\n",
      "Training | Average loss 0.2660769045352936\n",
      "Training iteration 0 | loss 0.24277186393737793\n",
      "Training iteration 1 | loss 0.24662896990776062\n",
      "Training iteration 2 | loss 0.28040099143981934\n",
      "Training iteration 3 | loss 0.2880449891090393\n",
      "Training iteration 4 | loss 0.25558724999427795\n",
      "Training | Average loss 0.26268681287765505\n",
      "Training iteration 0 | loss 0.27311867475509644\n",
      "Training iteration 1 | loss 0.2824874222278595\n",
      "Training iteration 2 | loss 0.26202553510665894\n",
      "Training iteration 3 | loss 0.2442696988582611\n",
      "Training iteration 4 | loss 0.2554246485233307\n",
      "Training | Average loss 0.26346519589424133\n",
      "Training iteration 0 | loss 0.2755754590034485\n",
      "Training iteration 1 | loss 0.23932713270187378\n",
      "Training iteration 2 | loss 0.2582116425037384\n",
      "Training iteration 3 | loss 0.26540887355804443\n",
      "Training iteration 4 | loss 0.2647811770439148\n",
      "Training | Average loss 0.260660856962204\n",
      "Training iteration 0 | loss 0.25039228796958923\n",
      "Training iteration 1 | loss 0.2751343846321106\n",
      "Training iteration 2 | loss 0.25076162815093994\n",
      "Training iteration 3 | loss 0.2838086783885956\n",
      "Training iteration 4 | loss 0.24045462906360626\n",
      "Training | Average loss 0.2601103216409683\n",
      "Training iteration 0 | loss 0.2808118462562561\n",
      "Training iteration 1 | loss 0.24192868173122406\n",
      "Training iteration 2 | loss 0.23713284730911255\n",
      "Training iteration 3 | loss 0.29552391171455383\n",
      "Training iteration 4 | loss 0.24253155291080475\n",
      "Training | Average loss 0.25958576798439026\n",
      "Training iteration 0 | loss 0.27943888306617737\n",
      "Training iteration 1 | loss 0.2289314717054367\n",
      "Training iteration 2 | loss 0.2706630527973175\n",
      "Training iteration 3 | loss 0.24077914655208588\n",
      "Training iteration 4 | loss 0.2681945860385895\n",
      "Training | Average loss 0.2576014280319214\n",
      "Training iteration 0 | loss 0.24098390340805054\n",
      "Training iteration 1 | loss 0.2810678780078888\n",
      "Training iteration 2 | loss 0.26806649565696716\n",
      "Training iteration 3 | loss 0.249665305018425\n",
      "Training iteration 4 | loss 0.24718832969665527\n",
      "Training | Average loss 0.2573943823575974\n",
      "Training iteration 0 | loss 0.25461050868034363\n",
      "Training iteration 1 | loss 0.28636831045150757\n",
      "Training iteration 2 | loss 0.2480745017528534\n",
      "Training iteration 3 | loss 0.2545922100543976\n",
      "Training iteration 4 | loss 0.25921866297721863\n",
      "Training | Average loss 0.26057283878326415\n",
      "Training iteration 0 | loss 0.2580273747444153\n",
      "Training iteration 1 | loss 0.262008935213089\n",
      "Training iteration 2 | loss 0.264384925365448\n",
      "Training iteration 3 | loss 0.2793092131614685\n",
      "Training iteration 4 | loss 0.23177611827850342\n",
      "Training | Average loss 0.25910131335258485\n",
      "Training iteration 0 | loss 0.26262396574020386\n",
      "Training iteration 1 | loss 0.2587984800338745\n",
      "Training iteration 2 | loss 0.2583332359790802\n",
      "Training iteration 3 | loss 0.25927984714508057\n",
      "Training iteration 4 | loss 0.24600841104984283\n",
      "Training | Average loss 0.2570087879896164\n",
      "Training iteration 0 | loss 0.27891239523887634\n",
      "Training iteration 1 | loss 0.23945868015289307\n",
      "Training iteration 2 | loss 0.2611013352870941\n",
      "Training iteration 3 | loss 0.2556703984737396\n",
      "Training iteration 4 | loss 0.25961834192276\n",
      "Training | Average loss 0.25895223021507263\n",
      "Training iteration 0 | loss 0.24901467561721802\n",
      "Training iteration 1 | loss 0.22389234602451324\n",
      "Training iteration 2 | loss 0.24999821186065674\n",
      "Training iteration 3 | loss 0.2543867826461792\n",
      "Training iteration 4 | loss 0.29010868072509766\n",
      "Training | Average loss 0.25348013937473296\n",
      "Training iteration 0 | loss 0.29149484634399414\n",
      "Training iteration 1 | loss 0.2629055082798004\n",
      "Training iteration 2 | loss 0.22890125215053558\n",
      "Training iteration 3 | loss 0.23966430127620697\n",
      "Training iteration 4 | loss 0.269366979598999\n",
      "Training | Average loss 0.2584665775299072\n",
      "Training iteration 0 | loss 0.2639215886592865\n",
      "Training iteration 1 | loss 0.2282031923532486\n",
      "Training iteration 2 | loss 0.25632473826408386\n",
      "Training iteration 3 | loss 0.2689429521560669\n",
      "Training iteration 4 | loss 0.2485222965478897\n",
      "Training | Average loss 0.2531829535961151\n",
      "Training iteration 0 | loss 0.25746074318885803\n",
      "Training iteration 1 | loss 0.258686363697052\n",
      "Training iteration 2 | loss 0.22159802913665771\n",
      "Training iteration 3 | loss 0.25592169165611267\n",
      "Training iteration 4 | loss 0.2702283263206482\n",
      "Training | Average loss 0.25277903079986574\n",
      "Training iteration 0 | loss 0.22635187208652496\n",
      "Training iteration 1 | loss 0.2615841329097748\n",
      "Training iteration 2 | loss 0.24533459544181824\n",
      "Training iteration 3 | loss 0.24272403120994568\n",
      "Training iteration 4 | loss 0.28203195333480835\n",
      "Training | Average loss 0.2516053169965744\n",
      "Training iteration 0 | loss 0.2535437345504761\n",
      "Training iteration 1 | loss 0.2753183841705322\n",
      "Training iteration 2 | loss 0.23746074736118317\n",
      "Training iteration 3 | loss 0.24786266684532166\n",
      "Training iteration 4 | loss 0.24098986387252808\n",
      "Training | Average loss 0.25103507936000824\n",
      "Training iteration 0 | loss 0.22935901582241058\n",
      "Training iteration 1 | loss 0.24747027456760406\n",
      "Training iteration 2 | loss 0.22713696956634521\n",
      "Training iteration 3 | loss 0.2746204733848572\n",
      "Training iteration 4 | loss 0.26686474680900574\n",
      "Training | Average loss 0.24909029603004457\n",
      "Training iteration 0 | loss 0.2568843364715576\n",
      "Training iteration 1 | loss 0.277555376291275\n",
      "Training iteration 2 | loss 0.2229965478181839\n",
      "Training iteration 3 | loss 0.23739147186279297\n",
      "Training iteration 4 | loss 0.25319498777389526\n",
      "Training | Average loss 0.24960454404354096\n",
      "Training iteration 0 | loss 0.23211024701595306\n",
      "Training iteration 1 | loss 0.25054824352264404\n",
      "Training iteration 2 | loss 0.25593405961990356\n",
      "Training iteration 3 | loss 0.26188597083091736\n",
      "Training iteration 4 | loss 0.23432138562202454\n",
      "Training | Average loss 0.24695998132228852\n",
      "Training iteration 0 | loss 0.26286378502845764\n",
      "Training iteration 1 | loss 0.26752322912216187\n",
      "Training iteration 2 | loss 0.226370707154274\n",
      "Training iteration 3 | loss 0.24719087779521942\n",
      "Training iteration 4 | loss 0.2377709150314331\n",
      "Training | Average loss 0.2483439028263092\n",
      "Training iteration 0 | loss 0.22722752392292023\n",
      "Training iteration 1 | loss 0.26524147391319275\n",
      "Training iteration 2 | loss 0.25164690613746643\n",
      "Training iteration 3 | loss 0.23796796798706055\n",
      "Training iteration 4 | loss 0.25294721126556396\n",
      "Training | Average loss 0.24700621664524078\n",
      "Training iteration 0 | loss 0.23279045522212982\n",
      "Training iteration 1 | loss 0.271235853433609\n",
      "Training iteration 2 | loss 0.22057297825813293\n",
      "Training iteration 3 | loss 0.22514773905277252\n",
      "Training iteration 4 | loss 0.2758886516094208\n",
      "Training | Average loss 0.245127135515213\n",
      "Training iteration 0 | loss 0.23771719634532928\n",
      "Training iteration 1 | loss 0.23910801112651825\n",
      "Training iteration 2 | loss 0.253635972738266\n",
      "Training iteration 3 | loss 0.2459983229637146\n",
      "Training iteration 4 | loss 0.24962182343006134\n",
      "Training | Average loss 0.2452162653207779\n",
      "Training iteration 0 | loss 0.26111024618148804\n",
      "Training iteration 1 | loss 0.2537139058113098\n",
      "Training iteration 2 | loss 0.2429324984550476\n",
      "Training iteration 3 | loss 0.23492036759853363\n",
      "Training iteration 4 | loss 0.23358753323554993\n",
      "Training | Average loss 0.2452529102563858\n",
      "Training iteration 0 | loss 0.2269781231880188\n",
      "Training iteration 1 | loss 0.23666805028915405\n",
      "Training iteration 2 | loss 0.2635473906993866\n",
      "Training iteration 3 | loss 0.22268718481063843\n",
      "Training iteration 4 | loss 0.27329644560813904\n",
      "Training | Average loss 0.2446354389190674\n",
      "Training iteration 0 | loss 0.23302575945854187\n",
      "Training iteration 1 | loss 0.25121545791625977\n",
      "Training iteration 2 | loss 0.2414080649614334\n",
      "Training iteration 3 | loss 0.2540493309497833\n",
      "Training iteration 4 | loss 0.23061779141426086\n",
      "Training | Average loss 0.24206328094005586\n",
      "Training iteration 0 | loss 0.23862405121326447\n",
      "Training iteration 1 | loss 0.2387869954109192\n",
      "Training iteration 2 | loss 0.21917958557605743\n",
      "Training iteration 3 | loss 0.267601877450943\n",
      "Training iteration 4 | loss 0.24589280784130096\n",
      "Training | Average loss 0.242017063498497\n",
      "Training iteration 0 | loss 0.2543138563632965\n",
      "Training iteration 1 | loss 0.2434936761856079\n",
      "Training iteration 2 | loss 0.2239813506603241\n",
      "Training iteration 3 | loss 0.2432781308889389\n",
      "Training iteration 4 | loss 0.2317071557044983\n",
      "Training | Average loss 0.23935483396053314\n",
      "Training iteration 0 | loss 0.25607189536094666\n",
      "Training iteration 1 | loss 0.2216537743806839\n",
      "Training iteration 2 | loss 0.2455955296754837\n",
      "Training iteration 3 | loss 0.21997644007205963\n",
      "Training iteration 4 | loss 0.2573356032371521\n",
      "Training | Average loss 0.2401266485452652\n",
      "Training iteration 0 | loss 0.23864127695560455\n",
      "Training iteration 1 | loss 0.23027543723583221\n",
      "Training iteration 2 | loss 0.2504133880138397\n",
      "Training iteration 3 | loss 0.21950627863407135\n",
      "Training iteration 4 | loss 0.25658270716667175\n",
      "Training | Average loss 0.23908381760120392\n",
      "Training iteration 0 | loss 0.23442789912223816\n",
      "Training iteration 1 | loss 0.24821898341178894\n",
      "Training iteration 2 | loss 0.26195472478866577\n",
      "Training iteration 3 | loss 0.23983028531074524\n",
      "Training iteration 4 | loss 0.20392097532749176\n",
      "Training | Average loss 0.23767057359218596\n",
      "Training iteration 0 | loss 0.2315962165594101\n",
      "Training iteration 1 | loss 0.2246575653553009\n",
      "Training iteration 2 | loss 0.21240979433059692\n",
      "Training iteration 3 | loss 0.2599053680896759\n",
      "Training iteration 4 | loss 0.2636139690876007\n",
      "Training | Average loss 0.2384365826845169\n",
      "Training iteration 0 | loss 0.26914873719215393\n",
      "Training iteration 1 | loss 0.22969979047775269\n",
      "Training iteration 2 | loss 0.21234901249408722\n",
      "Training iteration 3 | loss 0.20953895151615143\n",
      "Training iteration 4 | loss 0.2573665976524353\n",
      "Training | Average loss 0.2356206178665161\n",
      "Training iteration 0 | loss 0.23433664441108704\n",
      "Training iteration 1 | loss 0.22511543333530426\n",
      "Training iteration 2 | loss 0.25782662630081177\n",
      "Training iteration 3 | loss 0.22259020805358887\n",
      "Training iteration 4 | loss 0.24607817828655243\n",
      "Training | Average loss 0.23718941807746888\n",
      "Training iteration 0 | loss 0.2349604070186615\n",
      "Training iteration 1 | loss 0.24396072328090668\n",
      "Training iteration 2 | loss 0.21586598455905914\n",
      "Training iteration 3 | loss 0.24887126684188843\n",
      "Training iteration 4 | loss 0.23381797969341278\n",
      "Training | Average loss 0.2354952722787857\n",
      "Training iteration 0 | loss 0.22761020064353943\n",
      "Training iteration 1 | loss 0.22798572480678558\n",
      "Training iteration 2 | loss 0.2521618902683258\n",
      "Training iteration 3 | loss 0.23760128021240234\n",
      "Training iteration 4 | loss 0.22944462299346924\n",
      "Training | Average loss 0.23496074378490447\n",
      "Training iteration 0 | loss 0.243295818567276\n",
      "Training iteration 1 | loss 0.2189663052558899\n",
      "Training iteration 2 | loss 0.2608593702316284\n",
      "Training iteration 3 | loss 0.21453428268432617\n",
      "Training iteration 4 | loss 0.23333176970481873\n",
      "Training | Average loss 0.23419750928878785\n",
      "Training iteration 0 | loss 0.22005786001682281\n",
      "Training iteration 1 | loss 0.2820601761341095\n",
      "Training iteration 2 | loss 0.24580314755439758\n",
      "Training iteration 3 | loss 0.21556130051612854\n",
      "Training iteration 4 | loss 0.20085817575454712\n",
      "Training | Average loss 0.23286813199520112\n",
      "Training iteration 0 | loss 0.22652170062065125\n",
      "Training iteration 1 | loss 0.21076470613479614\n",
      "Training iteration 2 | loss 0.27065780758857727\n",
      "Training iteration 3 | loss 0.2189704328775406\n",
      "Training iteration 4 | loss 0.2405863255262375\n",
      "Training | Average loss 0.23350019454956056\n",
      "Training iteration 0 | loss 0.24175231158733368\n",
      "Training iteration 1 | loss 0.23574107885360718\n",
      "Training iteration 2 | loss 0.22510628402233124\n",
      "Training iteration 3 | loss 0.21243831515312195\n",
      "Training iteration 4 | loss 0.2478458732366562\n",
      "Training | Average loss 0.23257677257061005\n",
      "Training iteration 0 | loss 0.24722300469875336\n",
      "Training iteration 1 | loss 0.24621792137622833\n",
      "Training iteration 2 | loss 0.22214271128177643\n",
      "Training iteration 3 | loss 0.2157922387123108\n",
      "Training iteration 4 | loss 0.22766759991645813\n",
      "Training | Average loss 0.2318086951971054\n",
      "Training iteration 0 | loss 0.2563968896865845\n",
      "Training iteration 1 | loss 0.1963534951210022\n",
      "Training iteration 2 | loss 0.23991751670837402\n",
      "Training iteration 3 | loss 0.23632001876831055\n",
      "Training iteration 4 | loss 0.2263157218694687\n",
      "Training | Average loss 0.23106072843074799\n",
      "Training iteration 0 | loss 0.2089436948299408\n",
      "Training iteration 1 | loss 0.22859510779380798\n",
      "Training iteration 2 | loss 0.2464582771062851\n",
      "Training iteration 3 | loss 0.2378445863723755\n",
      "Training iteration 4 | loss 0.2343294620513916\n",
      "Training | Average loss 0.23123422563076018\n",
      "Training iteration 0 | loss 0.20770882070064545\n",
      "Training iteration 1 | loss 0.2435782551765442\n",
      "Training iteration 2 | loss 0.2201075255870819\n",
      "Training iteration 3 | loss 0.2363341748714447\n",
      "Training iteration 4 | loss 0.2425914853811264\n",
      "Training | Average loss 0.23006405234336852\n",
      "Training iteration 0 | loss 0.20987913012504578\n",
      "Training iteration 1 | loss 0.2345709204673767\n",
      "Training iteration 2 | loss 0.22665034234523773\n",
      "Training iteration 3 | loss 0.2278989553451538\n",
      "Training iteration 4 | loss 0.2457851767539978\n",
      "Training | Average loss 0.22895690500736238\n",
      "Training iteration 0 | loss 0.2135903686285019\n",
      "Training iteration 1 | loss 0.27370715141296387\n",
      "Training iteration 2 | loss 0.23282891511917114\n",
      "Training iteration 3 | loss 0.23458340764045715\n",
      "Training iteration 4 | loss 0.19189952313899994\n",
      "Training | Average loss 0.2293218731880188\n",
      "Training iteration 0 | loss 0.24161234498023987\n",
      "Training iteration 1 | loss 0.21628069877624512\n",
      "Training iteration 2 | loss 0.23310935497283936\n",
      "Training iteration 3 | loss 0.23559807240962982\n",
      "Training iteration 4 | loss 0.21536310017108917\n",
      "Training | Average loss 0.22839271426200866\n",
      "Training iteration 0 | loss 0.19689536094665527\n",
      "Training iteration 1 | loss 0.21753045916557312\n",
      "Training iteration 2 | loss 0.2281535416841507\n",
      "Training iteration 3 | loss 0.2347356528043747\n",
      "Training iteration 4 | loss 0.2555243670940399\n",
      "Training | Average loss 0.22656787633895875\n",
      "Training iteration 0 | loss 0.21129973232746124\n",
      "Training iteration 1 | loss 0.21939073503017426\n",
      "Training iteration 2 | loss 0.2460068315267563\n",
      "Training iteration 3 | loss 0.21247802674770355\n",
      "Training iteration 4 | loss 0.24346749484539032\n",
      "Training | Average loss 0.22652856409549713\n",
      "Training iteration 0 | loss 0.22032316029071808\n",
      "Training iteration 1 | loss 0.22669024765491486\n",
      "Training iteration 2 | loss 0.24109986424446106\n",
      "Training iteration 3 | loss 0.2100166529417038\n",
      "Training iteration 4 | loss 0.23349228501319885\n",
      "Training | Average loss 0.22632444202899932\n",
      "Training iteration 0 | loss 0.21532680094242096\n",
      "Training iteration 1 | loss 0.20042693614959717\n",
      "Training iteration 2 | loss 0.2621852457523346\n",
      "Training iteration 3 | loss 0.21433307230472565\n",
      "Training iteration 4 | loss 0.23435568809509277\n",
      "Training | Average loss 0.22532554864883422\n",
      "Training iteration 0 | loss 0.2436312437057495\n",
      "Training iteration 1 | loss 0.2075408399105072\n",
      "Training iteration 2 | loss 0.2283526211977005\n",
      "Training iteration 3 | loss 0.23418457806110382\n",
      "Training iteration 4 | loss 0.20766496658325195\n",
      "Training | Average loss 0.2242748498916626\n",
      "Training iteration 0 | loss 0.21436168253421783\n",
      "Training iteration 1 | loss 0.2306705266237259\n",
      "Training iteration 2 | loss 0.22879235446453094\n",
      "Training iteration 3 | loss 0.20908719301223755\n",
      "Training iteration 4 | loss 0.23471271991729736\n",
      "Training | Average loss 0.22352489531040193\n",
      "Training iteration 0 | loss 0.24094443023204803\n",
      "Training iteration 1 | loss 0.2137727588415146\n",
      "Training iteration 2 | loss 0.2480078786611557\n",
      "Training iteration 3 | loss 0.2182811051607132\n",
      "Training iteration 4 | loss 0.19792704284191132\n",
      "Training | Average loss 0.22378664314746857\n",
      "Training iteration 0 | loss 0.22814267873764038\n",
      "Training iteration 1 | loss 0.23682734370231628\n",
      "Training iteration 2 | loss 0.2101462334394455\n",
      "Training iteration 3 | loss 0.23436026275157928\n",
      "Training iteration 4 | loss 0.2014554738998413\n",
      "Training | Average loss 0.22218639850616456\n",
      "Training iteration 0 | loss 0.23519958555698395\n",
      "Training iteration 1 | loss 0.19144292175769806\n",
      "Training iteration 2 | loss 0.24937714636325836\n",
      "Training iteration 3 | loss 0.2325499951839447\n",
      "Training iteration 4 | loss 0.19972951710224152\n",
      "Training | Average loss 0.2216598331928253\n",
      "Training iteration 0 | loss 0.21838267147541046\n",
      "Training iteration 1 | loss 0.23013144731521606\n",
      "Training iteration 2 | loss 0.2217949628829956\n",
      "Training iteration 3 | loss 0.2233070433139801\n",
      "Training iteration 4 | loss 0.20925065875053406\n",
      "Training | Average loss 0.22057335674762726\n",
      "Training iteration 0 | loss 0.21673835813999176\n",
      "Training iteration 1 | loss 0.21056684851646423\n",
      "Training iteration 2 | loss 0.22384320199489594\n",
      "Training iteration 3 | loss 0.24236960709095\n",
      "Training iteration 4 | loss 0.20351986587047577\n",
      "Training | Average loss 0.21940757632255553\n",
      "Training iteration 0 | loss 0.2044372260570526\n",
      "Training iteration 1 | loss 0.24115732312202454\n",
      "Training iteration 2 | loss 0.22985424101352692\n",
      "Training iteration 3 | loss 0.20050011575222015\n",
      "Training iteration 4 | loss 0.21603712439537048\n",
      "Training | Average loss 0.21839720606803895\n",
      "Training iteration 0 | loss 0.22043254971504211\n",
      "Training iteration 1 | loss 0.2105960249900818\n",
      "Training iteration 2 | loss 0.2313973754644394\n",
      "Training iteration 3 | loss 0.20345419645309448\n",
      "Training iteration 4 | loss 0.22821660339832306\n",
      "Training | Average loss 0.21881935000419617\n",
      "Training iteration 0 | loss 0.2400825470685959\n",
      "Training iteration 1 | loss 0.2259652465581894\n",
      "Training iteration 2 | loss 0.20352767407894135\n",
      "Training iteration 3 | loss 0.20744550228118896\n",
      "Training iteration 4 | loss 0.2105337530374527\n",
      "Training | Average loss 0.21751094460487366\n",
      "Training iteration 0 | loss 0.21499089896678925\n",
      "Training iteration 1 | loss 0.20686845481395721\n",
      "Training iteration 2 | loss 0.18348415195941925\n",
      "Training iteration 3 | loss 0.23750978708267212\n",
      "Training iteration 4 | loss 0.24637115001678467\n",
      "Training | Average loss 0.2178448885679245\n",
      "Training iteration 0 | loss 0.23593184351921082\n",
      "Training iteration 1 | loss 0.19892768561840057\n",
      "Training iteration 2 | loss 0.21417014300823212\n",
      "Training iteration 3 | loss 0.21638180315494537\n",
      "Training iteration 4 | loss 0.2142190933227539\n",
      "Training | Average loss 0.21592611372470855\n",
      "Training iteration 0 | loss 0.21640416979789734\n",
      "Training iteration 1 | loss 0.216824471950531\n",
      "Training iteration 2 | loss 0.21510984003543854\n",
      "Training iteration 3 | loss 0.20538954436779022\n",
      "Training iteration 4 | loss 0.2305036038160324\n",
      "Training | Average loss 0.2168463259935379\n",
      "Training iteration 0 | loss 0.2118438184261322\n",
      "Training iteration 1 | loss 0.20466159284114838\n",
      "Training iteration 2 | loss 0.22632348537445068\n",
      "Training iteration 3 | loss 0.20629367232322693\n",
      "Training iteration 4 | loss 0.23242764174938202\n",
      "Training | Average loss 0.21631004214286803\n",
      "Training iteration 0 | loss 0.22017934918403625\n",
      "Training iteration 1 | loss 0.22405438125133514\n",
      "Training iteration 2 | loss 0.20365291833877563\n",
      "Training iteration 3 | loss 0.2277432382106781\n",
      "Training iteration 4 | loss 0.1991657316684723\n",
      "Training | Average loss 0.21495912373065948\n",
      "Training iteration 0 | loss 0.22283461689949036\n",
      "Training iteration 1 | loss 0.2182013839483261\n",
      "Training iteration 2 | loss 0.18972794711589813\n",
      "Training iteration 3 | loss 0.22785739600658417\n",
      "Training iteration 4 | loss 0.20873796939849854\n",
      "Training | Average loss 0.21347186267375945\n",
      "Training iteration 0 | loss 0.20179185271263123\n",
      "Training iteration 1 | loss 0.21464893221855164\n",
      "Training iteration 2 | loss 0.21231383085250854\n",
      "Training iteration 3 | loss 0.2413310557603836\n",
      "Training iteration 4 | loss 0.1957644820213318\n",
      "Training | Average loss 0.21317003071308135\n",
      "Training iteration 0 | loss 0.23050802946090698\n",
      "Training iteration 1 | loss 0.20504339039325714\n",
      "Training iteration 2 | loss 0.2012338489294052\n",
      "Training iteration 3 | loss 0.2086023986339569\n",
      "Training iteration 4 | loss 0.22215113043785095\n",
      "Training | Average loss 0.21350775957107543\n",
      "Training iteration 0 | loss 0.19030387699604034\n",
      "Training iteration 1 | loss 0.2142443209886551\n",
      "Training iteration 2 | loss 0.2131265103816986\n",
      "Training iteration 3 | loss 0.21390831470489502\n",
      "Training iteration 4 | loss 0.23175732791423798\n",
      "Training | Average loss 0.2126680701971054\n",
      "Training iteration 0 | loss 0.2345142811536789\n",
      "Training iteration 1 | loss 0.19723352789878845\n",
      "Training iteration 2 | loss 0.22253602743148804\n",
      "Training iteration 3 | loss 0.21567240357398987\n",
      "Training iteration 4 | loss 0.1894095242023468\n",
      "Training | Average loss 0.2118731528520584\n",
      "Training iteration 0 | loss 0.19399748742580414\n",
      "Training iteration 1 | loss 0.21238672733306885\n",
      "Training iteration 2 | loss 0.21686674654483795\n",
      "Training iteration 3 | loss 0.20837171375751495\n",
      "Training iteration 4 | loss 0.232694610953331\n",
      "Training | Average loss 0.2128634572029114\n",
      "Training iteration 0 | loss 0.21688780188560486\n",
      "Training iteration 1 | loss 0.22191904485225677\n",
      "Training iteration 2 | loss 0.2052990049123764\n",
      "Training iteration 3 | loss 0.19615589082241058\n",
      "Training iteration 4 | loss 0.21570809185504913\n",
      "Training | Average loss 0.21119396686553954\n",
      "Training iteration 0 | loss 0.19271932542324066\n",
      "Training iteration 1 | loss 0.18963290750980377\n",
      "Training iteration 2 | loss 0.23066705465316772\n",
      "Training iteration 3 | loss 0.2263084352016449\n",
      "Training iteration 4 | loss 0.21160615980625153\n",
      "Training | Average loss 0.21018677651882173\n",
      "Training iteration 0 | loss 0.1936396211385727\n",
      "Training iteration 1 | loss 0.21924640238285065\n",
      "Training iteration 2 | loss 0.2123434692621231\n",
      "Training iteration 3 | loss 0.20870350301265717\n",
      "Training iteration 4 | loss 0.21339274942874908\n",
      "Training | Average loss 0.20946514904499053\n",
      "Training iteration 0 | loss 0.19944049417972565\n",
      "Training iteration 1 | loss 0.22080403566360474\n",
      "Training iteration 2 | loss 0.21080535650253296\n",
      "Training iteration 3 | loss 0.2104729861021042\n",
      "Training iteration 4 | loss 0.20324841141700745\n",
      "Training | Average loss 0.208954256772995\n",
      "Training iteration 0 | loss 0.1972188949584961\n",
      "Training iteration 1 | loss 0.17775337398052216\n",
      "Training iteration 2 | loss 0.23067563772201538\n",
      "Training iteration 3 | loss 0.209049791097641\n",
      "Training iteration 4 | loss 0.223428875207901\n",
      "Training | Average loss 0.20762531459331512\n",
      "Training iteration 0 | loss 0.21447785198688507\n",
      "Training iteration 1 | loss 0.20044277608394623\n",
      "Training iteration 2 | loss 0.21077759563922882\n",
      "Training iteration 3 | loss 0.21459510922431946\n",
      "Training iteration 4 | loss 0.20228394865989685\n",
      "Training | Average loss 0.2085154563188553\n",
      "Training iteration 0 | loss 0.20732972025871277\n",
      "Training iteration 1 | loss 0.22385750710964203\n",
      "Training iteration 2 | loss 0.19812846183776855\n",
      "Training iteration 3 | loss 0.20667384564876556\n",
      "Training iteration 4 | loss 0.19952000677585602\n",
      "Training | Average loss 0.207101908326149\n",
      "Training iteration 0 | loss 0.18189802765846252\n",
      "Training iteration 1 | loss 0.21098358929157257\n",
      "Training iteration 2 | loss 0.20782096683979034\n",
      "Training iteration 3 | loss 0.20869798958301544\n",
      "Training iteration 4 | loss 0.22762057185173035\n",
      "Training | Average loss 0.20740422904491423\n",
      "Training iteration 0 | loss 0.22080941498279572\n",
      "Training iteration 1 | loss 0.23608927428722382\n",
      "Training iteration 2 | loss 0.182381734251976\n",
      "Training iteration 3 | loss 0.19594527781009674\n",
      "Training iteration 4 | loss 0.195610910654068\n",
      "Training | Average loss 0.20616732239723207\n",
      "Training iteration 0 | loss 0.19025863707065582\n",
      "Training iteration 1 | loss 0.21879315376281738\n",
      "Training iteration 2 | loss 0.21157458424568176\n",
      "Training iteration 3 | loss 0.21746839582920074\n",
      "Training iteration 4 | loss 0.19017697870731354\n",
      "Training | Average loss 0.20565434992313386\n",
      "Training iteration 0 | loss 0.20251953601837158\n",
      "Training iteration 1 | loss 0.1913209855556488\n",
      "Training iteration 2 | loss 0.2066514641046524\n",
      "Training iteration 3 | loss 0.2235715091228485\n",
      "Training iteration 4 | loss 0.2002834677696228\n",
      "Training | Average loss 0.2048693925142288\n",
      "Training iteration 0 | loss 0.21485742926597595\n",
      "Training iteration 1 | loss 0.21093930304050446\n",
      "Training iteration 2 | loss 0.20042529702186584\n",
      "Training iteration 3 | loss 0.19069159030914307\n",
      "Training iteration 4 | loss 0.2070441097021103\n",
      "Training | Average loss 0.20479154586791992\n",
      "Training iteration 0 | loss 0.1851399838924408\n",
      "Training iteration 1 | loss 0.2083481401205063\n",
      "Training iteration 2 | loss 0.19244612753391266\n",
      "Training iteration 3 | loss 0.21064810454845428\n",
      "Training iteration 4 | loss 0.22590036690235138\n",
      "Training | Average loss 0.2044965445995331\n",
      "Training iteration 0 | loss 0.19712114334106445\n",
      "Training iteration 1 | loss 0.21659542620182037\n",
      "Training iteration 2 | loss 0.2029017210006714\n",
      "Training iteration 3 | loss 0.20505546033382416\n",
      "Training iteration 4 | loss 0.19399525225162506\n",
      "Training | Average loss 0.20313380062580108\n",
      "Training iteration 0 | loss 0.19996517896652222\n",
      "Training iteration 1 | loss 0.19401249289512634\n",
      "Training iteration 2 | loss 0.20470263063907623\n",
      "Training iteration 3 | loss 0.20658135414123535\n",
      "Training iteration 4 | loss 0.2156836837530136\n",
      "Training | Average loss 0.20418906807899476\n",
      "Training iteration 0 | loss 0.23443429172039032\n",
      "Training iteration 1 | loss 0.17655467987060547\n",
      "Training iteration 2 | loss 0.16846081614494324\n",
      "Training iteration 3 | loss 0.20043614506721497\n",
      "Training iteration 4 | loss 0.23174437880516052\n",
      "Training | Average loss 0.2023260623216629\n",
      "Training iteration 0 | loss 0.18423624336719513\n",
      "Training iteration 1 | loss 0.20744697749614716\n",
      "Training iteration 2 | loss 0.22919140756130219\n",
      "Training iteration 3 | loss 0.1955706924200058\n",
      "Training iteration 4 | loss 0.19653131067752838\n",
      "Training | Average loss 0.20259532630443572\n",
      "Training iteration 0 | loss 0.19606299698352814\n",
      "Training iteration 1 | loss 0.19545243680477142\n",
      "Training iteration 2 | loss 0.21244007349014282\n",
      "Training iteration 3 | loss 0.19807210564613342\n",
      "Training iteration 4 | loss 0.20899203419685364\n",
      "Training | Average loss 0.2022039294242859\n",
      "Training iteration 0 | loss 0.20397883653640747\n",
      "Training iteration 1 | loss 0.21266663074493408\n",
      "Training iteration 2 | loss 0.20188532769680023\n",
      "Training iteration 3 | loss 0.199274942278862\n",
      "Training iteration 4 | loss 0.19146209955215454\n",
      "Training | Average loss 0.20185356736183166\n",
      "Training iteration 0 | loss 0.18538665771484375\n",
      "Training iteration 1 | loss 0.19536851346492767\n",
      "Training iteration 2 | loss 0.229256734251976\n",
      "Training iteration 3 | loss 0.198622927069664\n",
      "Training iteration 4 | loss 0.19113881886005402\n",
      "Training | Average loss 0.1999547302722931\n",
      "Training iteration 0 | loss 0.18212732672691345\n",
      "Training iteration 1 | loss 0.18761122226715088\n",
      "Training iteration 2 | loss 0.21348494291305542\n",
      "Training iteration 3 | loss 0.21245019137859344\n",
      "Training iteration 4 | loss 0.20596124231815338\n",
      "Training | Average loss 0.20032698512077332\n",
      "Training iteration 0 | loss 0.2103932648897171\n",
      "Training iteration 1 | loss 0.19027172029018402\n",
      "Training iteration 2 | loss 0.23034121096134186\n",
      "Training iteration 3 | loss 0.18062664568424225\n",
      "Training iteration 4 | loss 0.18496374785900116\n",
      "Training | Average loss 0.19931931793689728\n",
      "Training iteration 0 | loss 0.1997068226337433\n",
      "Training iteration 1 | loss 0.19012479484081268\n",
      "Training iteration 2 | loss 0.19535601139068604\n",
      "Training iteration 3 | loss 0.20744626224040985\n",
      "Training iteration 4 | loss 0.20413625240325928\n",
      "Training | Average loss 0.19935402870178223\n",
      "Training iteration 0 | loss 0.19022762775421143\n",
      "Training iteration 1 | loss 0.21076811850070953\n",
      "Training iteration 2 | loss 0.1936882734298706\n",
      "Training iteration 3 | loss 0.21397778391838074\n",
      "Training iteration 4 | loss 0.1844545602798462\n",
      "Training | Average loss 0.1986232727766037\n",
      "Training iteration 0 | loss 0.18401260673999786\n",
      "Training iteration 1 | loss 0.18542267382144928\n",
      "Training iteration 2 | loss 0.19822096824645996\n",
      "Training iteration 3 | loss 0.2135048806667328\n",
      "Training iteration 4 | loss 0.21297286450862885\n",
      "Training | Average loss 0.19882679879665374\n",
      "Training iteration 0 | loss 0.19137060642242432\n",
      "Training iteration 1 | loss 0.19227710366249084\n",
      "Training iteration 2 | loss 0.1999814659357071\n",
      "Training iteration 3 | loss 0.19879160821437836\n",
      "Training iteration 4 | loss 0.205084890127182\n",
      "Training | Average loss 0.19750113487243653\n",
      "Training iteration 0 | loss 0.174176424741745\n",
      "Training iteration 1 | loss 0.1728305220603943\n",
      "Training iteration 2 | loss 0.20572778582572937\n",
      "Training iteration 3 | loss 0.21538488566875458\n",
      "Training iteration 4 | loss 0.21987482905387878\n",
      "Training | Average loss 0.1975988894701004\n",
      "Training iteration 0 | loss 0.20251265168190002\n",
      "Training iteration 1 | loss 0.1785331517457962\n",
      "Training iteration 2 | loss 0.19311292469501495\n",
      "Training iteration 3 | loss 0.2051689624786377\n",
      "Training iteration 4 | loss 0.20202413201332092\n",
      "Training | Average loss 0.19627036452293395\n",
      "Training iteration 0 | loss 0.20130081474781036\n",
      "Training iteration 1 | loss 0.1954108625650406\n",
      "Training iteration 2 | loss 0.17013639211654663\n",
      "Training iteration 3 | loss 0.20545420050621033\n",
      "Training iteration 4 | loss 0.20942626893520355\n",
      "Training | Average loss 0.1963457077741623\n",
      "Training iteration 0 | loss 0.20430351793766022\n",
      "Training iteration 1 | loss 0.18870706856250763\n",
      "Training iteration 2 | loss 0.19362494349479675\n",
      "Training iteration 3 | loss 0.1916678100824356\n",
      "Training iteration 4 | loss 0.1989114135503769\n",
      "Training | Average loss 0.19544295072555543\n",
      "Training iteration 0 | loss 0.19497226178646088\n",
      "Training iteration 1 | loss 0.20169217884540558\n",
      "Training iteration 2 | loss 0.1817702203989029\n",
      "Training iteration 3 | loss 0.1980600655078888\n",
      "Training iteration 4 | loss 0.1975446492433548\n",
      "Training | Average loss 0.19480787515640258\n",
      "Training iteration 0 | loss 0.2077731341123581\n",
      "Training iteration 1 | loss 0.1897130161523819\n",
      "Training iteration 2 | loss 0.19287614524364471\n",
      "Training iteration 3 | loss 0.21159403026103973\n",
      "Training iteration 4 | loss 0.1742144376039505\n",
      "Training | Average loss 0.19523415267467498\n",
      "Training iteration 0 | loss 0.1923321932554245\n",
      "Training iteration 1 | loss 0.2058258056640625\n",
      "Training iteration 2 | loss 0.1807311475276947\n",
      "Training iteration 3 | loss 0.18458567559719086\n",
      "Training iteration 4 | loss 0.21104475855827332\n",
      "Training | Average loss 0.19490391612052918\n",
      "Training iteration 0 | loss 0.20428630709648132\n",
      "Training iteration 1 | loss 0.20503297448158264\n",
      "Training iteration 2 | loss 0.15966033935546875\n",
      "Training iteration 3 | loss 0.19857172667980194\n",
      "Training iteration 4 | loss 0.2023160755634308\n",
      "Training | Average loss 0.19397348463535308\n",
      "Training iteration 0 | loss 0.2041269987821579\n",
      "Training iteration 1 | loss 0.19327448308467865\n",
      "Training iteration 2 | loss 0.18903501331806183\n",
      "Training iteration 3 | loss 0.19876480102539062\n",
      "Training iteration 4 | loss 0.1838613897562027\n",
      "Training | Average loss 0.19381253719329833\n",
      "Training iteration 0 | loss 0.1922750174999237\n",
      "Training iteration 1 | loss 0.21336303651332855\n",
      "Training iteration 2 | loss 0.18789151310920715\n",
      "Training iteration 3 | loss 0.1865893006324768\n",
      "Training iteration 4 | loss 0.18798480927944183\n",
      "Training | Average loss 0.1936207354068756\n",
      "Training iteration 0 | loss 0.16834339499473572\n",
      "Training iteration 1 | loss 0.1858261376619339\n",
      "Training iteration 2 | loss 0.22014452517032623\n",
      "Training iteration 3 | loss 0.1855582594871521\n",
      "Training iteration 4 | loss 0.20599336922168732\n",
      "Training | Average loss 0.19317313730716706\n",
      "Training iteration 0 | loss 0.20022214949131012\n",
      "Training iteration 1 | loss 0.18540002405643463\n",
      "Training iteration 2 | loss 0.18139730393886566\n",
      "Training iteration 3 | loss 0.1890871226787567\n",
      "Training iteration 4 | loss 0.20081044733524323\n",
      "Training | Average loss 0.19138340950012206\n",
      "Training iteration 0 | loss 0.17181570827960968\n",
      "Training iteration 1 | loss 0.18749533593654633\n",
      "Training iteration 2 | loss 0.20654673874378204\n",
      "Training iteration 3 | loss 0.18882542848587036\n",
      "Training iteration 4 | loss 0.20343752205371857\n",
      "Training | Average loss 0.1916241466999054\n",
      "Training iteration 0 | loss 0.19899462163448334\n",
      "Training iteration 1 | loss 0.18512140214443207\n",
      "Training iteration 2 | loss 0.19957785308361053\n",
      "Training iteration 3 | loss 0.19809125363826752\n",
      "Training iteration 4 | loss 0.17652446031570435\n",
      "Training | Average loss 0.19166191816329955\n",
      "Training iteration 0 | loss 0.18488483130931854\n",
      "Training iteration 1 | loss 0.17794211208820343\n",
      "Training iteration 2 | loss 0.19422653317451477\n",
      "Training iteration 3 | loss 0.1872335821390152\n",
      "Training iteration 4 | loss 0.2082212120294571\n",
      "Training | Average loss 0.1905016541481018\n",
      "Training iteration 0 | loss 0.17466293275356293\n",
      "Training iteration 1 | loss 0.18656858801841736\n",
      "Training iteration 2 | loss 0.20629440248012543\n",
      "Training iteration 3 | loss 0.18783822655677795\n",
      "Training iteration 4 | loss 0.19629456102848053\n",
      "Training | Average loss 0.19033174216747284\n",
      "Training iteration 0 | loss 0.1830597221851349\n",
      "Training iteration 1 | loss 0.20155027508735657\n",
      "Training iteration 2 | loss 0.1863154172897339\n",
      "Training iteration 3 | loss 0.19297637045383453\n",
      "Training iteration 4 | loss 0.18769647181034088\n",
      "Training | Average loss 0.19031965136528015\n",
      "Training iteration 0 | loss 0.18432249128818512\n",
      "Training iteration 1 | loss 0.19529742002487183\n",
      "Training iteration 2 | loss 0.18562769889831543\n",
      "Training iteration 3 | loss 0.20627538859844208\n",
      "Training iteration 4 | loss 0.18145333230495453\n",
      "Training | Average loss 0.1905952662229538\n",
      "Training iteration 0 | loss 0.19048172235488892\n",
      "Training iteration 1 | loss 0.18212313950061798\n",
      "Training iteration 2 | loss 0.19594678282737732\n",
      "Training iteration 3 | loss 0.1930065006017685\n",
      "Training iteration 4 | loss 0.18558275699615479\n",
      "Training | Average loss 0.1894281804561615\n",
      "Training iteration 0 | loss 0.19024328887462616\n",
      "Training iteration 1 | loss 0.19937223196029663\n",
      "Training iteration 2 | loss 0.19421975314617157\n",
      "Training iteration 3 | loss 0.17857220768928528\n",
      "Training iteration 4 | loss 0.1811046600341797\n",
      "Training | Average loss 0.18870242834091186\n",
      "Training iteration 0 | loss 0.17304222285747528\n",
      "Training iteration 1 | loss 0.20676730573177338\n",
      "Training iteration 2 | loss 0.19962936639785767\n",
      "Training iteration 3 | loss 0.1929178237915039\n",
      "Training iteration 4 | loss 0.17014986276626587\n",
      "Training | Average loss 0.1885013163089752\n",
      "Training iteration 0 | loss 0.16903893649578094\n",
      "Training iteration 1 | loss 0.18432484567165375\n",
      "Training iteration 2 | loss 0.18831367790699005\n",
      "Training iteration 3 | loss 0.2018471211194992\n",
      "Training iteration 4 | loss 0.20251548290252686\n",
      "Training | Average loss 0.18920801281929017\n",
      "Training iteration 0 | loss 0.18166550993919373\n",
      "Training iteration 1 | loss 0.1705496609210968\n",
      "Training iteration 2 | loss 0.1928817182779312\n",
      "Training iteration 3 | loss 0.21057912707328796\n",
      "Training iteration 4 | loss 0.1886720359325409\n",
      "Training | Average loss 0.18886961042881012\n",
      "Training iteration 0 | loss 0.19433684647083282\n",
      "Training iteration 1 | loss 0.17803481221199036\n",
      "Training iteration 2 | loss 0.1955191045999527\n",
      "Training iteration 3 | loss 0.1803441196680069\n",
      "Training iteration 4 | loss 0.18896903097629547\n",
      "Training | Average loss 0.18744078278541565\n",
      "Training iteration 0 | loss 0.17749060690402985\n",
      "Training iteration 1 | loss 0.18825365602970123\n",
      "Training iteration 2 | loss 0.18142201006412506\n",
      "Training iteration 3 | loss 0.20882286131381989\n",
      "Training iteration 4 | loss 0.17952407896518707\n",
      "Training | Average loss 0.18710264265537263\n",
      "Training iteration 0 | loss 0.19700410962104797\n",
      "Training iteration 1 | loss 0.16140924394130707\n",
      "Training iteration 2 | loss 0.18836826086044312\n",
      "Training iteration 3 | loss 0.19840708374977112\n",
      "Training iteration 4 | loss 0.1856834888458252\n",
      "Training | Average loss 0.1861744374036789\n",
      "Training iteration 0 | loss 0.1859656423330307\n",
      "Training iteration 1 | loss 0.19171170890331268\n",
      "Training iteration 2 | loss 0.17197389900684357\n",
      "Training iteration 3 | loss 0.1828913539648056\n",
      "Training iteration 4 | loss 0.20014531910419464\n",
      "Training | Average loss 0.18653758466243744\n",
      "Training iteration 0 | loss 0.18855445086956024\n",
      "Training iteration 1 | loss 0.17806020379066467\n",
      "Training iteration 2 | loss 0.1836707592010498\n",
      "Training iteration 3 | loss 0.1881004422903061\n",
      "Training iteration 4 | loss 0.18815907835960388\n",
      "Training | Average loss 0.18530898690223693\n",
      "Training iteration 0 | loss 0.19037583470344543\n",
      "Training iteration 1 | loss 0.17218460142612457\n",
      "Training iteration 2 | loss 0.18438313901424408\n",
      "Training iteration 3 | loss 0.2026774138212204\n",
      "Training iteration 4 | loss 0.19556263089179993\n",
      "Training | Average loss 0.1890367239713669\n",
      "Training iteration 0 | loss 0.2028988152742386\n",
      "Training iteration 1 | loss 0.21683217585086823\n",
      "Training iteration 2 | loss 0.1986425518989563\n",
      "Training iteration 3 | loss 0.1954377144575119\n",
      "Training iteration 4 | loss 0.20046788454055786\n",
      "Training | Average loss 0.20285582840442656\n",
      "Training iteration 0 | loss 0.19716279208660126\n",
      "Training iteration 1 | loss 0.21262980997562408\n",
      "Training iteration 2 | loss 0.2045724242925644\n",
      "Training iteration 3 | loss 0.22923362255096436\n",
      "Training iteration 4 | loss 0.17498493194580078\n",
      "Training | Average loss 0.20371671617031098\n",
      "Training iteration 0 | loss 0.20086325705051422\n",
      "Training iteration 1 | loss 0.2005380094051361\n",
      "Training iteration 2 | loss 0.174248605966568\n",
      "Training iteration 3 | loss 0.19051973521709442\n",
      "Training iteration 4 | loss 0.20481033623218536\n",
      "Training | Average loss 0.1941959887742996\n",
      "Training iteration 0 | loss 0.19209080934524536\n",
      "Training iteration 1 | loss 0.19078265130519867\n",
      "Training iteration 2 | loss 0.2055814415216446\n",
      "Training iteration 3 | loss 0.18781232833862305\n",
      "Training iteration 4 | loss 0.2067515105009079\n",
      "Training | Average loss 0.1966037482023239\n",
      "Training iteration 0 | loss 0.20674364268779755\n",
      "Training iteration 1 | loss 0.21710075438022614\n",
      "Training iteration 2 | loss 0.19474221765995026\n",
      "Training iteration 3 | loss 0.17673836648464203\n",
      "Training iteration 4 | loss 0.17206783592700958\n",
      "Training | Average loss 0.1934785634279251\n",
      "Training iteration 0 | loss 0.19231703877449036\n",
      "Training iteration 1 | loss 0.1960456371307373\n",
      "Training iteration 2 | loss 0.20301511883735657\n",
      "Training iteration 3 | loss 0.19345751404762268\n",
      "Training iteration 4 | loss 0.17298604547977448\n",
      "Training | Average loss 0.19156427085399627\n",
      "Training iteration 0 | loss 0.17028352618217468\n",
      "Training iteration 1 | loss 0.19890373945236206\n",
      "Training iteration 2 | loss 0.1955377757549286\n",
      "Training iteration 3 | loss 0.18436837196350098\n",
      "Training iteration 4 | loss 0.19528932869434357\n",
      "Training | Average loss 0.18887654840946197\n",
      "Training iteration 0 | loss 0.1854018121957779\n",
      "Training iteration 1 | loss 0.20460641384124756\n",
      "Training iteration 2 | loss 0.17215444147586823\n",
      "Training iteration 3 | loss 0.18641552329063416\n",
      "Training iteration 4 | loss 0.17415568232536316\n",
      "Training | Average loss 0.1845467746257782\n",
      "Training iteration 0 | loss 0.1821316033601761\n",
      "Training iteration 1 | loss 0.1882898211479187\n",
      "Training iteration 2 | loss 0.18694178760051727\n",
      "Training iteration 3 | loss 0.17424722015857697\n",
      "Training iteration 4 | loss 0.19371213018894196\n",
      "Training | Average loss 0.18506451249122619\n",
      "Training iteration 0 | loss 0.1915525496006012\n",
      "Training iteration 1 | loss 0.15848681330680847\n",
      "Training iteration 2 | loss 0.16592355072498322\n",
      "Training iteration 3 | loss 0.21283893287181854\n",
      "Training iteration 4 | loss 0.19454476237297058\n",
      "Training | Average loss 0.1846693217754364\n",
      "Training iteration 0 | loss 0.17283327877521515\n",
      "Training iteration 1 | loss 0.17860251665115356\n",
      "Training iteration 2 | loss 0.18068571388721466\n",
      "Training iteration 3 | loss 0.19202402234077454\n",
      "Training iteration 4 | loss 0.203880175948143\n",
      "Training | Average loss 0.18560514152050017\n",
      "Training iteration 0 | loss 0.17513956129550934\n",
      "Training iteration 1 | loss 0.1911408007144928\n",
      "Training iteration 2 | loss 0.18540027737617493\n",
      "Training iteration 3 | loss 0.18245507776737213\n",
      "Training iteration 4 | loss 0.18383178114891052\n",
      "Training | Average loss 0.18359349966049193\n",
      "Training iteration 0 | loss 0.17022831737995148\n",
      "Training iteration 1 | loss 0.18840496242046356\n",
      "Training iteration 2 | loss 0.1609533131122589\n",
      "Training iteration 3 | loss 0.16972558200359344\n",
      "Training iteration 4 | loss 0.22243142127990723\n",
      "Training | Average loss 0.18234871923923493\n",
      "Training iteration 0 | loss 0.18103358149528503\n",
      "Training iteration 1 | loss 0.17104430496692657\n",
      "Training iteration 2 | loss 0.1975623369216919\n",
      "Training iteration 3 | loss 0.19550268352031708\n",
      "Training iteration 4 | loss 0.16512808203697205\n",
      "Training | Average loss 0.18205419778823853\n",
      "Training iteration 0 | loss 0.17489676177501678\n",
      "Training iteration 1 | loss 0.20113742351531982\n",
      "Training iteration 2 | loss 0.17914821207523346\n",
      "Training iteration 3 | loss 0.1780180186033249\n",
      "Training iteration 4 | loss 0.1780657023191452\n",
      "Training | Average loss 0.18225322365760804\n",
      "Training iteration 0 | loss 0.1859021931886673\n",
      "Training iteration 1 | loss 0.15295568108558655\n",
      "Training iteration 2 | loss 0.20035329461097717\n",
      "Training iteration 3 | loss 0.17487260699272156\n",
      "Training iteration 4 | loss 0.18995670974254608\n",
      "Training | Average loss 0.18080809712409973\n",
      "Training iteration 0 | loss 0.18037164211273193\n",
      "Training iteration 1 | loss 0.17175713181495667\n",
      "Training iteration 2 | loss 0.17891888320446014\n",
      "Training iteration 3 | loss 0.1943771094083786\n",
      "Training iteration 4 | loss 0.1772381216287613\n",
      "Training | Average loss 0.18053257763385772\n",
      "Training iteration 0 | loss 0.17135612666606903\n",
      "Training iteration 1 | loss 0.1848924607038498\n",
      "Training iteration 2 | loss 0.1900113821029663\n",
      "Training iteration 3 | loss 0.17420336604118347\n",
      "Training iteration 4 | loss 0.1856713443994522\n",
      "Training | Average loss 0.18122693598270417\n",
      "Training iteration 0 | loss 0.18481329083442688\n",
      "Training iteration 1 | loss 0.18175677955150604\n",
      "Training iteration 2 | loss 0.17670799791812897\n",
      "Training iteration 3 | loss 0.1837301254272461\n",
      "Training iteration 4 | loss 0.18348804116249084\n",
      "Training | Average loss 0.18209924697875976\n",
      "Training iteration 0 | loss 0.1797415316104889\n",
      "Training iteration 1 | loss 0.17307773232460022\n",
      "Training iteration 2 | loss 0.18384578824043274\n",
      "Training iteration 3 | loss 0.18542705476284027\n",
      "Training iteration 4 | loss 0.18363675475120544\n",
      "Training | Average loss 0.1811457723379135\n",
      "Training iteration 0 | loss 0.18238565325737\n",
      "Training iteration 1 | loss 0.1723126918077469\n",
      "Training iteration 2 | loss 0.1824696660041809\n",
      "Training iteration 3 | loss 0.18802927434444427\n",
      "Training iteration 4 | loss 0.17754995822906494\n",
      "Training | Average loss 0.1805494487285614\n",
      "Training iteration 0 | loss 0.18846675753593445\n",
      "Training iteration 1 | loss 0.16505631804466248\n",
      "Training iteration 2 | loss 0.16650092601776123\n",
      "Training iteration 3 | loss 0.17741864919662476\n",
      "Training iteration 4 | loss 0.19728320837020874\n",
      "Training | Average loss 0.17894517183303832\n",
      "Training iteration 0 | loss 0.20977374911308289\n",
      "Training iteration 1 | loss 0.1837354451417923\n",
      "Training iteration 2 | loss 0.16388985514640808\n",
      "Training iteration 3 | loss 0.1691935807466507\n",
      "Training iteration 4 | loss 0.17390620708465576\n",
      "Training | Average loss 0.18009976744651796\n",
      "Training iteration 0 | loss 0.17061258852481842\n",
      "Training iteration 1 | loss 0.174921914935112\n",
      "Training iteration 2 | loss 0.16935031116008759\n",
      "Training iteration 3 | loss 0.18429873883724213\n",
      "Training iteration 4 | loss 0.19103845953941345\n",
      "Training | Average loss 0.1780444025993347\n",
      "Training iteration 0 | loss 0.15720470249652863\n",
      "Training iteration 1 | loss 0.19755984842777252\n",
      "Training iteration 2 | loss 0.1947251260280609\n",
      "Training iteration 3 | loss 0.16621588170528412\n",
      "Training iteration 4 | loss 0.1764187514781952\n",
      "Training | Average loss 0.17842486202716829\n",
      "Training iteration 0 | loss 0.18293149769306183\n",
      "Training iteration 1 | loss 0.1540968418121338\n",
      "Training iteration 2 | loss 0.16888898611068726\n",
      "Training iteration 3 | loss 0.19328337907791138\n",
      "Training iteration 4 | loss 0.19881433248519897\n",
      "Training | Average loss 0.17960300743579866\n",
      "Training iteration 0 | loss 0.20430655777454376\n",
      "Training iteration 1 | loss 0.17046351730823517\n",
      "Training iteration 2 | loss 0.16805046796798706\n",
      "Training iteration 3 | loss 0.1819925308227539\n",
      "Training iteration 4 | loss 0.17311158776283264\n",
      "Training | Average loss 0.17958493232727052\n",
      "Training iteration 0 | loss 0.19733282923698425\n",
      "Training iteration 1 | loss 0.18513883650302887\n",
      "Training iteration 2 | loss 0.17006582021713257\n",
      "Training iteration 3 | loss 0.16751380264759064\n",
      "Training iteration 4 | loss 0.17354491353034973\n",
      "Training | Average loss 0.1787192404270172\n",
      "Training iteration 0 | loss 0.18046468496322632\n",
      "Training iteration 1 | loss 0.17935651540756226\n",
      "Training iteration 2 | loss 0.18182341754436493\n",
      "Training iteration 3 | loss 0.16618983447551727\n",
      "Training iteration 4 | loss 0.17666718363761902\n",
      "Training | Average loss 0.17690032720565796\n",
      "Training iteration 0 | loss 0.18340253829956055\n",
      "Training iteration 1 | loss 0.17250236868858337\n",
      "Training iteration 2 | loss 0.16618680953979492\n",
      "Training iteration 3 | loss 0.1858566254377365\n",
      "Training iteration 4 | loss 0.17877373099327087\n",
      "Training | Average loss 0.17734441459178923\n",
      "Training iteration 0 | loss 0.17053502798080444\n",
      "Training iteration 1 | loss 0.18700237572193146\n",
      "Training iteration 2 | loss 0.17320850491523743\n",
      "Training iteration 3 | loss 0.17196787893772125\n",
      "Training iteration 4 | loss 0.17930150032043457\n",
      "Training | Average loss 0.17640305757522584\n",
      "Training iteration 0 | loss 0.1830172836780548\n",
      "Training iteration 1 | loss 0.17122884094715118\n",
      "Training iteration 2 | loss 0.1680540144443512\n",
      "Training iteration 3 | loss 0.1699294000864029\n",
      "Training iteration 4 | loss 0.18999388813972473\n",
      "Training | Average loss 0.17644468545913697\n",
      "Training iteration 0 | loss 0.18133445084095\n",
      "Training iteration 1 | loss 0.14984023571014404\n",
      "Training iteration 2 | loss 0.18590450286865234\n",
      "Training iteration 3 | loss 0.16849593818187714\n",
      "Training iteration 4 | loss 0.19120657444000244\n",
      "Training | Average loss 0.1753563404083252\n",
      "Training iteration 0 | loss 0.19280610978603363\n",
      "Training iteration 1 | loss 0.18404865264892578\n",
      "Training iteration 2 | loss 0.17333708703517914\n",
      "Training iteration 3 | loss 0.1667560189962387\n",
      "Training iteration 4 | loss 0.15933363139629364\n",
      "Training | Average loss 0.17525629997253417\n",
      "Training iteration 0 | loss 0.17258886992931366\n",
      "Training iteration 1 | loss 0.17743976414203644\n",
      "Training iteration 2 | loss 0.17010562121868134\n",
      "Training iteration 3 | loss 0.16961146891117096\n",
      "Training iteration 4 | loss 0.18917551636695862\n",
      "Training | Average loss 0.1757842481136322\n",
      "Training iteration 0 | loss 0.17250432074069977\n",
      "Training iteration 1 | loss 0.17778822779655457\n",
      "Training iteration 2 | loss 0.17002445459365845\n",
      "Training iteration 3 | loss 0.1739475429058075\n",
      "Training iteration 4 | loss 0.1803954541683197\n",
      "Training | Average loss 0.174932000041008\n",
      "Training iteration 0 | loss 0.180013507604599\n",
      "Training iteration 1 | loss 0.16722960770130157\n",
      "Training iteration 2 | loss 0.17535482347011566\n",
      "Training iteration 3 | loss 0.16393610835075378\n",
      "Training iteration 4 | loss 0.18508318066596985\n",
      "Training | Average loss 0.17432344555854798\n",
      "Training iteration 0 | loss 0.19087381660938263\n",
      "Training iteration 1 | loss 0.16276200115680695\n",
      "Training iteration 2 | loss 0.1659458875656128\n",
      "Training iteration 3 | loss 0.1878495216369629\n",
      "Training iteration 4 | loss 0.16581058502197266\n",
      "Training | Average loss 0.17464836239814757\n",
      "Training iteration 0 | loss 0.16841870546340942\n",
      "Training iteration 1 | loss 0.1960746794939041\n",
      "Training iteration 2 | loss 0.18338832259178162\n",
      "Training iteration 3 | loss 0.159078449010849\n",
      "Training iteration 4 | loss 0.16159076988697052\n",
      "Training | Average loss 0.17371018528938292\n",
      "Training iteration 0 | loss 0.18997834622859955\n",
      "Training iteration 1 | loss 0.1720549464225769\n",
      "Training iteration 2 | loss 0.17209647595882416\n",
      "Training iteration 3 | loss 0.17799259722232819\n",
      "Training iteration 4 | loss 0.15720334649085999\n",
      "Training | Average loss 0.17386514246463775\n",
      "Training iteration 0 | loss 0.17466503381729126\n",
      "Training iteration 1 | loss 0.17561419308185577\n",
      "Training iteration 2 | loss 0.17629167437553406\n",
      "Training iteration 3 | loss 0.1747468113899231\n",
      "Training iteration 4 | loss 0.16109898686408997\n",
      "Training | Average loss 0.17248333990573883\n",
      "Training iteration 0 | loss 0.17626744508743286\n",
      "Training iteration 1 | loss 0.16871650516986847\n",
      "Training iteration 2 | loss 0.1539488434791565\n",
      "Training iteration 3 | loss 0.16957783699035645\n",
      "Training iteration 4 | loss 0.19388718903064728\n",
      "Training | Average loss 0.1724795639514923\n",
      "Training iteration 0 | loss 0.17225372791290283\n",
      "Training iteration 1 | loss 0.17802438139915466\n",
      "Training iteration 2 | loss 0.1772122085094452\n",
      "Training iteration 3 | loss 0.16181303560733795\n",
      "Training iteration 4 | loss 0.17862814664840698\n",
      "Training | Average loss 0.17358630001544953\n",
      "Training iteration 0 | loss 0.16847415268421173\n",
      "Training iteration 1 | loss 0.18491141498088837\n",
      "Training iteration 2 | loss 0.1703454703092575\n",
      "Training iteration 3 | loss 0.182725727558136\n",
      "Training iteration 4 | loss 0.1590898036956787\n",
      "Training | Average loss 0.17310931384563447\n",
      "Training iteration 0 | loss 0.16525602340698242\n",
      "Training iteration 1 | loss 0.1657775640487671\n",
      "Training iteration 2 | loss 0.18062683939933777\n",
      "Training iteration 3 | loss 0.18914726376533508\n",
      "Training iteration 4 | loss 0.1644514948129654\n",
      "Training | Average loss 0.17305183708667754\n",
      "Training iteration 0 | loss 0.14501892030239105\n",
      "Training iteration 1 | loss 0.1550149917602539\n",
      "Training iteration 2 | loss 0.19277797639369965\n",
      "Training iteration 3 | loss 0.18268682062625885\n",
      "Training iteration 4 | loss 0.17993861436843872\n",
      "Training | Average loss 0.17108746469020844\n",
      "Training iteration 0 | loss 0.1785115748643875\n",
      "Training iteration 1 | loss 0.16955754160881042\n",
      "Training iteration 2 | loss 0.16998429596424103\n",
      "Training iteration 3 | loss 0.19158856570720673\n",
      "Training iteration 4 | loss 0.15036389231681824\n",
      "Training | Average loss 0.1720011740922928\n",
      "Training iteration 0 | loss 0.18279193341732025\n",
      "Training iteration 1 | loss 0.1749766767024994\n",
      "Training iteration 2 | loss 0.17632749676704407\n",
      "Training iteration 3 | loss 0.15595708787441254\n",
      "Training iteration 4 | loss 0.16774237155914307\n",
      "Training | Average loss 0.17155911326408385\n",
      "Training iteration 0 | loss 0.17902079224586487\n",
      "Training iteration 1 | loss 0.16086144745349884\n",
      "Training iteration 2 | loss 0.17645344138145447\n",
      "Training iteration 3 | loss 0.16599242389202118\n",
      "Training iteration 4 | loss 0.1691765934228897\n",
      "Training | Average loss 0.1703009396791458\n",
      "Training iteration 0 | loss 0.1880812793970108\n",
      "Training iteration 1 | loss 0.17938672006130219\n",
      "Training iteration 2 | loss 0.17858076095581055\n",
      "Training iteration 3 | loss 0.15753184258937836\n",
      "Training iteration 4 | loss 0.14796453714370728\n",
      "Training | Average loss 0.17030902802944184\n",
      "Training iteration 0 | loss 0.1763967126607895\n",
      "Training iteration 1 | loss 0.1626088172197342\n",
      "Training iteration 2 | loss 0.1775333285331726\n",
      "Training iteration 3 | loss 0.16836313903331757\n",
      "Training iteration 4 | loss 0.16670729219913483\n",
      "Training | Average loss 0.17032185792922974\n",
      "Training iteration 0 | loss 0.1678043007850647\n",
      "Training iteration 1 | loss 0.18058902025222778\n",
      "Training iteration 2 | loss 0.15987201035022736\n",
      "Training iteration 3 | loss 0.1702832132577896\n",
      "Training iteration 4 | loss 0.17364460229873657\n",
      "Training | Average loss 0.17043862938880922\n",
      "Training iteration 0 | loss 0.15473195910453796\n",
      "Training iteration 1 | loss 0.17772240936756134\n",
      "Training iteration 2 | loss 0.16489145159721375\n",
      "Training iteration 3 | loss 0.17799685895442963\n",
      "Training iteration 4 | loss 0.17515815794467926\n",
      "Training | Average loss 0.1701001673936844\n",
      "Training iteration 0 | loss 0.1634400486946106\n",
      "Training iteration 1 | loss 0.16106478869915009\n",
      "Training iteration 2 | loss 0.17144569754600525\n",
      "Training iteration 3 | loss 0.16748562455177307\n",
      "Training iteration 4 | loss 0.18488778173923492\n",
      "Training | Average loss 0.16966478824615477\n",
      "Training iteration 0 | loss 0.16170990467071533\n",
      "Training iteration 1 | loss 0.17095623910427094\n",
      "Training iteration 2 | loss 0.19733381271362305\n",
      "Training iteration 3 | loss 0.1527990996837616\n",
      "Training iteration 4 | loss 0.16045834124088287\n",
      "Training | Average loss 0.16865147948265075\n",
      "Training iteration 0 | loss 0.16689777374267578\n",
      "Training iteration 1 | loss 0.16062912344932556\n",
      "Training iteration 2 | loss 0.17349477112293243\n",
      "Training iteration 3 | loss 0.17731241881847382\n",
      "Training iteration 4 | loss 0.16876408457756042\n",
      "Training | Average loss 0.1694196343421936\n",
      "Training iteration 0 | loss 0.16721384227275848\n",
      "Training iteration 1 | loss 0.17522132396697998\n",
      "Training iteration 2 | loss 0.17205563187599182\n",
      "Training iteration 3 | loss 0.16718824207782745\n",
      "Training iteration 4 | loss 0.15603314340114594\n",
      "Training | Average loss 0.16754243671894073\n",
      "Training iteration 0 | loss 0.16140441596508026\n",
      "Training iteration 1 | loss 0.16543227434158325\n",
      "Training iteration 2 | loss 0.16602358222007751\n",
      "Training iteration 3 | loss 0.16536875069141388\n",
      "Training iteration 4 | loss 0.17936016619205475\n",
      "Training | Average loss 0.16751783788204194\n",
      "Training iteration 0 | loss 0.1749136745929718\n",
      "Training iteration 1 | loss 0.1843896508216858\n",
      "Training iteration 2 | loss 0.14919710159301758\n",
      "Training iteration 3 | loss 0.16373604536056519\n",
      "Training iteration 4 | loss 0.16684405505657196\n",
      "Training | Average loss 0.16781610548496245\n",
      "Training iteration 0 | loss 0.16503934562206268\n",
      "Training iteration 1 | loss 0.16071054339408875\n",
      "Training iteration 2 | loss 0.15988431870937347\n",
      "Training iteration 3 | loss 0.18073046207427979\n",
      "Training iteration 4 | loss 0.1703660935163498\n",
      "Training | Average loss 0.16734615266323088\n",
      "Training iteration 0 | loss 0.19311559200286865\n",
      "Training iteration 1 | loss 0.1604418307542801\n",
      "Training iteration 2 | loss 0.15688104927539825\n",
      "Training iteration 3 | loss 0.1609586477279663\n",
      "Training iteration 4 | loss 0.16525250673294067\n",
      "Training | Average loss 0.16732992529869078\n",
      "Training iteration 0 | loss 0.1726282685995102\n",
      "Training iteration 1 | loss 0.17359969019889832\n",
      "Training iteration 2 | loss 0.16498160362243652\n",
      "Training iteration 3 | loss 0.15258070826530457\n",
      "Training iteration 4 | loss 0.17188988626003265\n",
      "Training | Average loss 0.16713603138923644\n",
      "Training iteration 0 | loss 0.1652616262435913\n",
      "Training iteration 1 | loss 0.1780296117067337\n",
      "Training iteration 2 | loss 0.1698375642299652\n",
      "Training iteration 3 | loss 0.15670561790466309\n",
      "Training iteration 4 | loss 0.1624108850955963\n",
      "Training | Average loss 0.16644906103610993\n",
      "Training iteration 0 | loss 0.17536719143390656\n",
      "Training iteration 1 | loss 0.15716992318630219\n",
      "Training iteration 2 | loss 0.19507363438606262\n",
      "Training iteration 3 | loss 0.15536172688007355\n",
      "Training iteration 4 | loss 0.15560975670814514\n",
      "Training | Average loss 0.167716446518898\n",
      "Training iteration 0 | loss 0.15964573621749878\n",
      "Training iteration 1 | loss 0.16907890141010284\n",
      "Training iteration 2 | loss 0.17320318520069122\n",
      "Training iteration 3 | loss 0.16811735928058624\n",
      "Training iteration 4 | loss 0.1605304330587387\n",
      "Training | Average loss 0.16611512303352355\n",
      "Training iteration 0 | loss 0.1575348675251007\n",
      "Training iteration 1 | loss 0.17347723245620728\n",
      "Training iteration 2 | loss 0.17101995646953583\n",
      "Training iteration 3 | loss 0.15956848859786987\n",
      "Training iteration 4 | loss 0.17080993950366974\n",
      "Training | Average loss 0.1664820969104767\n",
      "Training iteration 0 | loss 0.17820587754249573\n",
      "Training iteration 1 | loss 0.1722460240125656\n",
      "Training iteration 2 | loss 0.16171996295452118\n",
      "Training iteration 3 | loss 0.161359041929245\n",
      "Training iteration 4 | loss 0.15695983171463013\n",
      "Training | Average loss 0.16609814763069153\n",
      "Training iteration 0 | loss 0.17468667030334473\n",
      "Training iteration 1 | loss 0.16822423040866852\n",
      "Training iteration 2 | loss 0.16856007277965546\n",
      "Training iteration 3 | loss 0.1648426651954651\n",
      "Training iteration 4 | loss 0.15853585302829742\n",
      "Training | Average loss 0.16696989834308623\n",
      "Training iteration 0 | loss 0.157319113612175\n",
      "Training iteration 1 | loss 0.1708764284849167\n",
      "Training iteration 2 | loss 0.16369973123073578\n",
      "Training iteration 3 | loss 0.16288568079471588\n",
      "Training iteration 4 | loss 0.17463985085487366\n",
      "Training | Average loss 0.1658841609954834\n",
      "Training iteration 0 | loss 0.17078427970409393\n",
      "Training iteration 1 | loss 0.15947581827640533\n",
      "Training iteration 2 | loss 0.1520317792892456\n",
      "Training iteration 3 | loss 0.1683102399110794\n",
      "Training iteration 4 | loss 0.17315532267093658\n",
      "Training | Average loss 0.16475148797035216\n",
      "Training iteration 0 | loss 0.19437755644321442\n",
      "Training iteration 1 | loss 0.1567835956811905\n",
      "Training iteration 2 | loss 0.15598562359809875\n",
      "Training iteration 3 | loss 0.16668370366096497\n",
      "Training iteration 4 | loss 0.15113110840320587\n",
      "Training | Average loss 0.1649923175573349\n",
      "Training iteration 0 | loss 0.16644752025604248\n",
      "Training iteration 1 | loss 0.1486510932445526\n",
      "Training iteration 2 | loss 0.17243613302707672\n",
      "Training iteration 3 | loss 0.16727980971336365\n",
      "Training iteration 4 | loss 0.16809363663196564\n",
      "Training | Average loss 0.16458163857460023\n",
      "Training iteration 0 | loss 0.17663507163524628\n",
      "Training iteration 1 | loss 0.15505483746528625\n",
      "Training iteration 2 | loss 0.1717437505722046\n",
      "Training iteration 3 | loss 0.15696130692958832\n",
      "Training iteration 4 | loss 0.16281718015670776\n",
      "Training | Average loss 0.16464242935180665\n",
      "Training iteration 0 | loss 0.1490279883146286\n",
      "Training iteration 1 | loss 0.1704426258802414\n",
      "Training iteration 2 | loss 0.16984955966472626\n",
      "Training iteration 3 | loss 0.16478393971920013\n",
      "Training iteration 4 | loss 0.1671338528394699\n",
      "Training | Average loss 0.16424759328365326\n",
      "Training iteration 0 | loss 0.16225847601890564\n",
      "Training iteration 1 | loss 0.17758138477802277\n",
      "Training iteration 2 | loss 0.15023495256900787\n",
      "Training iteration 3 | loss 0.16364653408527374\n",
      "Training iteration 4 | loss 0.1652706265449524\n",
      "Training | Average loss 0.16379839479923247\n",
      "Training iteration 0 | loss 0.15825870633125305\n",
      "Training iteration 1 | loss 0.1591508537530899\n",
      "Training iteration 2 | loss 0.1530151665210724\n",
      "Training iteration 3 | loss 0.16987310349941254\n",
      "Training iteration 4 | loss 0.17496703565120697\n",
      "Training | Average loss 0.16305297315120698\n",
      "Training iteration 0 | loss 0.1525847166776657\n",
      "Training iteration 1 | loss 0.15744750201702118\n",
      "Training iteration 2 | loss 0.16074566543102264\n",
      "Training iteration 3 | loss 0.16825617849826813\n",
      "Training iteration 4 | loss 0.1785711646080017\n",
      "Training | Average loss 0.16352104544639587\n",
      "Training iteration 0 | loss 0.1621522605419159\n",
      "Training iteration 1 | loss 0.16162844002246857\n",
      "Training iteration 2 | loss 0.15335312485694885\n",
      "Training iteration 3 | loss 0.1570775806903839\n",
      "Training iteration 4 | loss 0.1818421185016632\n",
      "Training | Average loss 0.1632107049226761\n",
      "Training iteration 0 | loss 0.1526549607515335\n",
      "Training iteration 1 | loss 0.16941975057125092\n",
      "Training iteration 2 | loss 0.1633862406015396\n",
      "Training iteration 3 | loss 0.18507146835327148\n",
      "Training iteration 4 | loss 0.1433563530445099\n",
      "Training | Average loss 0.16277775466442107\n",
      "Training iteration 0 | loss 0.16329728066921234\n",
      "Training iteration 1 | loss 0.16755086183547974\n",
      "Training iteration 2 | loss 0.16443562507629395\n",
      "Training iteration 3 | loss 0.17508667707443237\n",
      "Training iteration 4 | loss 0.14151227474212646\n",
      "Training | Average loss 0.16237654387950898\n",
      "Training iteration 0 | loss 0.13728180527687073\n",
      "Training iteration 1 | loss 0.17851614952087402\n",
      "Training iteration 2 | loss 0.16188757121562958\n",
      "Training iteration 3 | loss 0.16806146502494812\n",
      "Training iteration 4 | loss 0.167543426156044\n",
      "Training | Average loss 0.16265808343887328\n",
      "Training iteration 0 | loss 0.1471434384584427\n",
      "Training iteration 1 | loss 0.1558738499879837\n",
      "Training iteration 2 | loss 0.16352713108062744\n",
      "Training iteration 3 | loss 0.19390274584293365\n",
      "Training iteration 4 | loss 0.15120770037174225\n",
      "Training | Average loss 0.16233097314834594\n",
      "Training iteration 0 | loss 0.15766185522079468\n",
      "Training iteration 1 | loss 0.1595371663570404\n",
      "Training iteration 2 | loss 0.15673138201236725\n",
      "Training iteration 3 | loss 0.16940973699092865\n",
      "Training iteration 4 | loss 0.16876359283924103\n",
      "Training | Average loss 0.1624207466840744\n",
      "Training iteration 0 | loss 0.1570550799369812\n",
      "Training iteration 1 | loss 0.15744918584823608\n",
      "Training iteration 2 | loss 0.17022556066513062\n",
      "Training iteration 3 | loss 0.16891922056674957\n",
      "Training iteration 4 | loss 0.15588699281215668\n",
      "Training | Average loss 0.16190720796585084\n",
      "Training iteration 0 | loss 0.1448538452386856\n",
      "Training iteration 1 | loss 0.1696120649576187\n",
      "Training iteration 2 | loss 0.15365935862064362\n",
      "Training iteration 3 | loss 0.16472968459129333\n",
      "Training iteration 4 | loss 0.17260557413101196\n",
      "Training | Average loss 0.16109210550785064\n",
      "Training iteration 0 | loss 0.17436835169792175\n",
      "Training iteration 1 | loss 0.13598209619522095\n",
      "Training iteration 2 | loss 0.15993569791316986\n",
      "Training iteration 3 | loss 0.16916775703430176\n",
      "Training iteration 4 | loss 0.16651301085948944\n",
      "Training | Average loss 0.16119338274002076\n",
      "Training iteration 0 | loss 0.14662691950798035\n",
      "Training iteration 1 | loss 0.16446533799171448\n",
      "Training iteration 2 | loss 0.15229089558124542\n",
      "Training iteration 3 | loss 0.17652924358844757\n",
      "Training iteration 4 | loss 0.16346202790737152\n",
      "Training | Average loss 0.16067488491535187\n",
      "Training iteration 0 | loss 0.16508367657661438\n",
      "Training iteration 1 | loss 0.1618271917104721\n",
      "Training iteration 2 | loss 0.16317439079284668\n",
      "Training iteration 3 | loss 0.160493865609169\n",
      "Training iteration 4 | loss 0.15173253417015076\n",
      "Training | Average loss 0.1604623317718506\n",
      "Training iteration 0 | loss 0.16408184170722961\n",
      "Training iteration 1 | loss 0.15229511260986328\n",
      "Training iteration 2 | loss 0.16698673367500305\n",
      "Training iteration 3 | loss 0.1763189435005188\n",
      "Training iteration 4 | loss 0.14141874015331268\n",
      "Training | Average loss 0.16022027432918548\n",
      "Training iteration 0 | loss 0.1581406146287918\n",
      "Training iteration 1 | loss 0.15785755217075348\n",
      "Training iteration 2 | loss 0.14769332110881805\n",
      "Training iteration 3 | loss 0.16952385008335114\n",
      "Training iteration 4 | loss 0.16662783920764923\n",
      "Training | Average loss 0.15996863543987275\n",
      "Training iteration 0 | loss 0.145127072930336\n",
      "Training iteration 1 | loss 0.15262417495250702\n",
      "Training iteration 2 | loss 0.1722879558801651\n",
      "Training iteration 3 | loss 0.15162445604801178\n",
      "Training iteration 4 | loss 0.176834374666214\n",
      "Training | Average loss 0.15969960689544677\n",
      "Training iteration 0 | loss 0.16532737016677856\n",
      "Training iteration 1 | loss 0.16065728664398193\n",
      "Training iteration 2 | loss 0.15597528219223022\n",
      "Training iteration 3 | loss 0.16977344453334808\n",
      "Training iteration 4 | loss 0.15193676948547363\n",
      "Training | Average loss 0.16073403060436248\n",
      "Training iteration 0 | loss 0.154788076877594\n",
      "Training iteration 1 | loss 0.1450265347957611\n",
      "Training iteration 2 | loss 0.15741059184074402\n",
      "Training iteration 3 | loss 0.17017985880374908\n",
      "Training iteration 4 | loss 0.17372079193592072\n",
      "Training | Average loss 0.16022517085075377\n",
      "Training iteration 0 | loss 0.14772586524486542\n",
      "Training iteration 1 | loss 0.17806820571422577\n",
      "Training iteration 2 | loss 0.1555459350347519\n",
      "Training iteration 3 | loss 0.15731744468212128\n",
      "Training iteration 4 | loss 0.1582181304693222\n",
      "Training | Average loss 0.1593751162290573\n",
      "Training iteration 0 | loss 0.17263402044773102\n",
      "Training iteration 1 | loss 0.14499157667160034\n",
      "Training iteration 2 | loss 0.17525169253349304\n",
      "Training iteration 3 | loss 0.1512746810913086\n",
      "Training iteration 4 | loss 0.15812596678733826\n",
      "Training | Average loss 0.16045558750629424\n",
      "Training iteration 0 | loss 0.1811864972114563\n",
      "Training iteration 1 | loss 0.14297756552696228\n",
      "Training iteration 2 | loss 0.15639586746692657\n",
      "Training iteration 3 | loss 0.15850843489170074\n",
      "Training iteration 4 | loss 0.15991650521755219\n",
      "Training | Average loss 0.15979697406291962\n",
      "Training iteration 0 | loss 0.1534423530101776\n",
      "Training iteration 1 | loss 0.15202279388904572\n",
      "Training iteration 2 | loss 0.1618146449327469\n",
      "Training iteration 3 | loss 0.1653996855020523\n",
      "Training iteration 4 | loss 0.16044802963733673\n",
      "Training | Average loss 0.15862550139427184\n",
      "Training iteration 0 | loss 0.1515788733959198\n",
      "Training iteration 1 | loss 0.15705451369285583\n",
      "Training iteration 2 | loss 0.1616658717393875\n",
      "Training iteration 3 | loss 0.15057100355625153\n",
      "Training iteration 4 | loss 0.17306546866893768\n",
      "Training | Average loss 0.15878714621067047\n",
      "Training iteration 0 | loss 0.1601531207561493\n",
      "Training iteration 1 | loss 0.14799486100673676\n",
      "Training iteration 2 | loss 0.15116016566753387\n",
      "Training iteration 3 | loss 0.17183919250965118\n",
      "Training iteration 4 | loss 0.16100887954235077\n",
      "Training | Average loss 0.15843124389648439\n",
      "Training iteration 0 | loss 0.15896476805210114\n",
      "Training iteration 1 | loss 0.1515868902206421\n",
      "Training iteration 2 | loss 0.17094148695468903\n",
      "Training iteration 3 | loss 0.15963684022426605\n",
      "Training iteration 4 | loss 0.1487434059381485\n",
      "Training | Average loss 0.15797467827796935\n",
      "Training iteration 0 | loss 0.15657690167427063\n",
      "Training iteration 1 | loss 0.1818699985742569\n",
      "Training iteration 2 | loss 0.16549569368362427\n",
      "Training iteration 3 | loss 0.14658214151859283\n",
      "Training iteration 4 | loss 0.1381404548883438\n",
      "Training | Average loss 0.1577330380678177\n",
      "Training iteration 0 | loss 0.15981890261173248\n",
      "Training iteration 1 | loss 0.16519290208816528\n",
      "Training iteration 2 | loss 0.14783747494220734\n",
      "Training iteration 3 | loss 0.1371900886297226\n",
      "Training iteration 4 | loss 0.17769595980644226\n",
      "Training | Average loss 0.157547065615654\n",
      "Training iteration 0 | loss 0.16734197735786438\n",
      "Training iteration 1 | loss 0.157918781042099\n",
      "Training iteration 2 | loss 0.14697237312793732\n",
      "Training iteration 3 | loss 0.1648324728012085\n",
      "Training iteration 4 | loss 0.15234260261058807\n",
      "Training | Average loss 0.15788164138793945\n",
      "Training iteration 0 | loss 0.14702020585536957\n",
      "Training iteration 1 | loss 0.1736592799425125\n",
      "Training iteration 2 | loss 0.15102121233940125\n",
      "Training iteration 3 | loss 0.1553758829832077\n",
      "Training iteration 4 | loss 0.16063226759433746\n",
      "Training | Average loss 0.1575417697429657\n",
      "Training iteration 0 | loss 0.17480888962745667\n",
      "Training iteration 1 | loss 0.1519092172384262\n",
      "Training iteration 2 | loss 0.15042732656002045\n",
      "Training iteration 3 | loss 0.15318745374679565\n",
      "Training iteration 4 | loss 0.15566730499267578\n",
      "Training | Average loss 0.15720003843307495\n",
      "Training iteration 0 | loss 0.13768252730369568\n",
      "Training iteration 1 | loss 0.15276935696601868\n",
      "Training iteration 2 | loss 0.16513964533805847\n",
      "Training iteration 3 | loss 0.16843222081661224\n",
      "Training iteration 4 | loss 0.16143099963665009\n",
      "Training | Average loss 0.15709095001220702\n",
      "Training iteration 0 | loss 0.17938539385795593\n",
      "Training iteration 1 | loss 0.13523617386817932\n",
      "Training iteration 2 | loss 0.17447349429130554\n",
      "Training iteration 3 | loss 0.1561952531337738\n",
      "Training iteration 4 | loss 0.1415434628725052\n",
      "Training | Average loss 0.15736675560474395\n",
      "Training iteration 0 | loss 0.15636517107486725\n",
      "Training iteration 1 | loss 0.15223582088947296\n",
      "Training iteration 2 | loss 0.14841973781585693\n",
      "Training iteration 3 | loss 0.1748436689376831\n",
      "Training iteration 4 | loss 0.1529787927865982\n",
      "Training | Average loss 0.1569686383008957\n",
      "Training iteration 0 | loss 0.16719657182693481\n",
      "Training iteration 1 | loss 0.16221240162849426\n",
      "Training iteration 2 | loss 0.16698379814624786\n",
      "Training iteration 3 | loss 0.15748025476932526\n",
      "Training iteration 4 | loss 0.1306539624929428\n",
      "Training | Average loss 0.156905397772789\n",
      "Training iteration 0 | loss 0.14976993203163147\n",
      "Training iteration 1 | loss 0.15846571326255798\n",
      "Training iteration 2 | loss 0.15566450357437134\n",
      "Training iteration 3 | loss 0.15349873900413513\n",
      "Training iteration 4 | loss 0.16543462872505188\n",
      "Training | Average loss 0.15656670331954955\n",
      "Training iteration 0 | loss 0.1552312970161438\n",
      "Training iteration 1 | loss 0.15788200497627258\n",
      "Training iteration 2 | loss 0.16574223339557648\n",
      "Training iteration 3 | loss 0.1499594748020172\n",
      "Training iteration 4 | loss 0.1551256775856018\n",
      "Training | Average loss 0.1567881375551224\n",
      "Training iteration 0 | loss 0.13902664184570312\n",
      "Training iteration 1 | loss 0.18152986466884613\n",
      "Training iteration 2 | loss 0.15554149448871613\n",
      "Training iteration 3 | loss 0.1417408585548401\n",
      "Training iteration 4 | loss 0.16412319242954254\n",
      "Training | Average loss 0.1563924103975296\n",
      "Training iteration 0 | loss 0.15086062252521515\n",
      "Training iteration 1 | loss 0.14802084863185883\n",
      "Training iteration 2 | loss 0.1553063690662384\n",
      "Training iteration 3 | loss 0.16124439239501953\n",
      "Training iteration 4 | loss 0.16178849339485168\n",
      "Training | Average loss 0.15544414520263672\n",
      "Training iteration 0 | loss 0.15502874553203583\n",
      "Training iteration 1 | loss 0.14682674407958984\n",
      "Training iteration 2 | loss 0.16742479801177979\n",
      "Training iteration 3 | loss 0.1676148772239685\n",
      "Training iteration 4 | loss 0.14032690227031708\n",
      "Training | Average loss 0.1554444134235382\n",
      "Training iteration 0 | loss 0.15513600409030914\n",
      "Training iteration 1 | loss 0.1390332281589508\n",
      "Training iteration 2 | loss 0.16021038591861725\n",
      "Training iteration 3 | loss 0.1583646535873413\n",
      "Training iteration 4 | loss 0.16599346697330475\n",
      "Training | Average loss 0.15574754774570465\n",
      "Training iteration 0 | loss 0.16967113316059113\n",
      "Training iteration 1 | loss 0.15321645140647888\n",
      "Training iteration 2 | loss 0.158174529671669\n",
      "Training iteration 3 | loss 0.14780206978321075\n",
      "Training iteration 4 | loss 0.14636890590190887\n",
      "Training | Average loss 0.15504661798477173\n",
      "Training iteration 0 | loss 0.15154382586479187\n",
      "Training iteration 1 | loss 0.16096441447734833\n",
      "Training iteration 2 | loss 0.15512555837631226\n",
      "Training iteration 3 | loss 0.1559755504131317\n",
      "Training iteration 4 | loss 0.15166498720645905\n",
      "Training | Average loss 0.15505486726760864\n",
      "Training iteration 0 | loss 0.14128263294696808\n",
      "Training iteration 1 | loss 0.158852681517601\n",
      "Training iteration 2 | loss 0.1625143438577652\n",
      "Training iteration 3 | loss 0.16624556481838226\n",
      "Training iteration 4 | loss 0.14741189777851105\n",
      "Training | Average loss 0.15526142418384553\n",
      "Training iteration 0 | loss 0.15705113112926483\n",
      "Training iteration 1 | loss 0.160909965634346\n",
      "Training iteration 2 | loss 0.15664224326610565\n",
      "Training iteration 3 | loss 0.1520300805568695\n",
      "Training iteration 4 | loss 0.14772173762321472\n",
      "Training | Average loss 0.15487103164196014\n",
      "Training iteration 0 | loss 0.1664249300956726\n",
      "Training iteration 1 | loss 0.1512223184108734\n",
      "Training iteration 2 | loss 0.15671271085739136\n",
      "Training iteration 3 | loss 0.1534401923418045\n",
      "Training iteration 4 | loss 0.15044179558753967\n",
      "Training | Average loss 0.1556483894586563\n",
      "Training iteration 0 | loss 0.14771011471748352\n",
      "Training iteration 1 | loss 0.14116862416267395\n",
      "Training iteration 2 | loss 0.15926577150821686\n",
      "Training iteration 3 | loss 0.1603393256664276\n",
      "Training iteration 4 | loss 0.16177204251289368\n",
      "Training | Average loss 0.15405117571353913\n",
      "Training iteration 0 | loss 0.15043742954730988\n",
      "Training iteration 1 | loss 0.15608400106430054\n",
      "Training iteration 2 | loss 0.1735357642173767\n",
      "Training iteration 3 | loss 0.1507827490568161\n",
      "Training iteration 4 | loss 0.14432504773139954\n",
      "Training | Average loss 0.15503299832344056\n",
      "Training iteration 0 | loss 0.16198411583900452\n",
      "Training iteration 1 | loss 0.15798695385456085\n",
      "Training iteration 2 | loss 0.1453244388103485\n",
      "Training iteration 3 | loss 0.15231379866600037\n",
      "Training iteration 4 | loss 0.1505671739578247\n",
      "Training | Average loss 0.15363529622554778\n",
      "Training iteration 0 | loss 0.16513514518737793\n",
      "Training iteration 1 | loss 0.15959621965885162\n",
      "Training iteration 2 | loss 0.14890673756599426\n",
      "Training iteration 3 | loss 0.14155296981334686\n",
      "Training iteration 4 | loss 0.15327288210391998\n",
      "Training | Average loss 0.15369279086589813\n",
      "Training iteration 0 | loss 0.17498263716697693\n",
      "Training iteration 1 | loss 0.1526513397693634\n",
      "Training iteration 2 | loss 0.1462623029947281\n",
      "Training iteration 3 | loss 0.1622261106967926\n",
      "Training iteration 4 | loss 0.13077208399772644\n",
      "Training | Average loss 0.1533788949251175\n",
      "Training iteration 0 | loss 0.13896405696868896\n",
      "Training iteration 1 | loss 0.16301682591438293\n",
      "Training iteration 2 | loss 0.16163858771324158\n",
      "Training iteration 3 | loss 0.13891072571277618\n",
      "Training iteration 4 | loss 0.1685754507780075\n",
      "Training | Average loss 0.15422112941741944\n",
      "Training iteration 0 | loss 0.14631716907024384\n",
      "Training iteration 1 | loss 0.15779781341552734\n",
      "Training iteration 2 | loss 0.1638680398464203\n",
      "Training iteration 3 | loss 0.151047021150589\n",
      "Training iteration 4 | loss 0.1529785692691803\n",
      "Training | Average loss 0.15440172255039214\n",
      "Training iteration 0 | loss 0.16440486907958984\n",
      "Training iteration 1 | loss 0.16192352771759033\n",
      "Training iteration 2 | loss 0.1396968513727188\n",
      "Training iteration 3 | loss 0.15837472677230835\n",
      "Training iteration 4 | loss 0.15197496116161346\n",
      "Training | Average loss 0.15527498722076416\n",
      "Training iteration 0 | loss 0.13396871089935303\n",
      "Training iteration 1 | loss 0.15026424825191498\n",
      "Training iteration 2 | loss 0.17104841768741608\n",
      "Training iteration 3 | loss 0.1571369618177414\n",
      "Training iteration 4 | loss 0.15237708389759064\n",
      "Training | Average loss 0.15295908451080323\n",
      "Training iteration 0 | loss 0.14213432371616364\n",
      "Training iteration 1 | loss 0.14917591214179993\n",
      "Training iteration 2 | loss 0.1622903048992157\n",
      "Training iteration 3 | loss 0.16669906675815582\n",
      "Training iteration 4 | loss 0.14795568585395813\n",
      "Training | Average loss 0.15365105867385864\n",
      "Training iteration 0 | loss 0.15148131549358368\n",
      "Training iteration 1 | loss 0.15164466202259064\n",
      "Training iteration 2 | loss 0.15575405955314636\n",
      "Training iteration 3 | loss 0.1610008031129837\n",
      "Training iteration 4 | loss 0.1461651474237442\n",
      "Training | Average loss 0.1532091975212097\n",
      "Training iteration 0 | loss 0.14833855628967285\n",
      "Training iteration 1 | loss 0.1672881543636322\n",
      "Training iteration 2 | loss 0.14712010324001312\n",
      "Training iteration 3 | loss 0.1513388752937317\n",
      "Training iteration 4 | loss 0.14953607320785522\n",
      "Training | Average loss 0.15272435247898103\n",
      "Training iteration 0 | loss 0.17715364694595337\n",
      "Training iteration 1 | loss 0.1389860361814499\n",
      "Training iteration 2 | loss 0.14495815336704254\n",
      "Training iteration 3 | loss 0.1538551300764084\n",
      "Training iteration 4 | loss 0.15179622173309326\n",
      "Training | Average loss 0.1533498376607895\n",
      "Training iteration 0 | loss 0.15129540860652924\n",
      "Training iteration 1 | loss 0.14738096296787262\n",
      "Training iteration 2 | loss 0.145516499876976\n",
      "Training iteration 3 | loss 0.1611262857913971\n",
      "Training iteration 4 | loss 0.1568344682455063\n",
      "Training | Average loss 0.15243072509765626\n",
      "Training iteration 0 | loss 0.15205778181552887\n",
      "Training iteration 1 | loss 0.145349383354187\n",
      "Training iteration 2 | loss 0.15537294745445251\n",
      "Training iteration 3 | loss 0.1450583040714264\n",
      "Training iteration 4 | loss 0.16322925686836243\n",
      "Training | Average loss 0.15221353471279145\n",
      "Training iteration 0 | loss 0.1646767258644104\n",
      "Training iteration 1 | loss 0.14352360367774963\n",
      "Training iteration 2 | loss 0.14357717335224152\n",
      "Training iteration 3 | loss 0.16119937598705292\n",
      "Training iteration 4 | loss 0.15217429399490356\n",
      "Training | Average loss 0.1530302345752716\n",
      "Training iteration 0 | loss 0.16031742095947266\n",
      "Training iteration 1 | loss 0.13703206181526184\n",
      "Training iteration 2 | loss 0.158090740442276\n",
      "Training iteration 3 | loss 0.1378072202205658\n",
      "Training iteration 4 | loss 0.17026259005069733\n",
      "Training | Average loss 0.1527020066976547\n",
      "Training iteration 0 | loss 0.15550974011421204\n",
      "Training iteration 1 | loss 0.14641480147838593\n",
      "Training iteration 2 | loss 0.1690748631954193\n",
      "Training iteration 3 | loss 0.1451914757490158\n",
      "Training iteration 4 | loss 0.14146113395690918\n",
      "Training | Average loss 0.15153040289878844\n",
      "Training iteration 0 | loss 0.16735245287418365\n",
      "Training iteration 1 | loss 0.16110806167125702\n",
      "Training iteration 2 | loss 0.15054820477962494\n",
      "Training iteration 3 | loss 0.14057083427906036\n",
      "Training iteration 4 | loss 0.14000898599624634\n",
      "Training | Average loss 0.15191770792007447\n",
      "Training iteration 0 | loss 0.13844068348407745\n",
      "Training iteration 1 | loss 0.17585912346839905\n",
      "Training iteration 2 | loss 0.13735271990299225\n",
      "Training iteration 3 | loss 0.16141457855701447\n",
      "Training iteration 4 | loss 0.1436525285243988\n",
      "Training | Average loss 0.1513439267873764\n",
      "Training iteration 0 | loss 0.13658465445041656\n",
      "Training iteration 1 | loss 0.1616567075252533\n",
      "Training iteration 2 | loss 0.16065329313278198\n",
      "Training iteration 3 | loss 0.14960595965385437\n",
      "Training iteration 4 | loss 0.14611615240573883\n",
      "Training | Average loss 0.15092335343360902\n",
      "Training iteration 0 | loss 0.1647706925868988\n",
      "Training iteration 1 | loss 0.14343015849590302\n",
      "Training iteration 2 | loss 0.15512263774871826\n",
      "Training iteration 3 | loss 0.1514470875263214\n",
      "Training iteration 4 | loss 0.140519917011261\n",
      "Training | Average loss 0.1510580986738205\n",
      "Training iteration 0 | loss 0.14891478419303894\n",
      "Training iteration 1 | loss 0.14950956404209137\n",
      "Training iteration 2 | loss 0.13233332335948944\n",
      "Training iteration 3 | loss 0.1583879590034485\n",
      "Training iteration 4 | loss 0.16553883254528046\n",
      "Training | Average loss 0.15093689262866974\n",
      "Training iteration 0 | loss 0.15125267207622528\n",
      "Training iteration 1 | loss 0.13865675032138824\n",
      "Training iteration 2 | loss 0.14727598428726196\n",
      "Training iteration 3 | loss 0.1658821552991867\n",
      "Training iteration 4 | loss 0.15083158016204834\n",
      "Training | Average loss 0.1507798284292221\n",
      "Training iteration 0 | loss 0.14709590375423431\n",
      "Training iteration 1 | loss 0.1573375016450882\n",
      "Training iteration 2 | loss 0.15623566508293152\n",
      "Training iteration 3 | loss 0.14655640721321106\n",
      "Training iteration 4 | loss 0.14800472557544708\n",
      "Training | Average loss 0.15104604065418242\n",
      "Training iteration 0 | loss 0.14542680978775024\n",
      "Training iteration 1 | loss 0.1495150625705719\n",
      "Training iteration 2 | loss 0.15917041897773743\n",
      "Training iteration 3 | loss 0.13611416518688202\n",
      "Training iteration 4 | loss 0.16400420665740967\n",
      "Training | Average loss 0.15084613263607025\n",
      "Training iteration 0 | loss 0.15432657301425934\n",
      "Training iteration 1 | loss 0.15872406959533691\n",
      "Training iteration 2 | loss 0.13883808255195618\n",
      "Training iteration 3 | loss 0.1512579619884491\n",
      "Training iteration 4 | loss 0.15039066970348358\n",
      "Training | Average loss 0.150707471370697\n",
      "Training iteration 0 | loss 0.14827431738376617\n",
      "Training iteration 1 | loss 0.14327828586101532\n",
      "Training iteration 2 | loss 0.16856850683689117\n",
      "Training iteration 3 | loss 0.1520753800868988\n",
      "Training iteration 4 | loss 0.13836652040481567\n",
      "Training | Average loss 0.15011260211467742\n",
      "Training iteration 0 | loss 0.15020106732845306\n",
      "Training iteration 1 | loss 0.15780791640281677\n",
      "Training iteration 2 | loss 0.1576637476682663\n",
      "Training iteration 3 | loss 0.1401623785495758\n",
      "Training iteration 4 | loss 0.14591681957244873\n",
      "Training | Average loss 0.15035038590431213\n",
      "Training iteration 0 | loss 0.15178853273391724\n",
      "Training iteration 1 | loss 0.16303583979606628\n",
      "Training iteration 2 | loss 0.1279212385416031\n",
      "Training iteration 3 | loss 0.1517631560564041\n",
      "Training iteration 4 | loss 0.15793496370315552\n",
      "Training | Average loss 0.15048874616622926\n",
      "Training iteration 0 | loss 0.16495169699192047\n",
      "Training iteration 1 | loss 0.16016896069049835\n",
      "Training iteration 2 | loss 0.13813923299312592\n",
      "Training iteration 3 | loss 0.13524065911769867\n",
      "Training iteration 4 | loss 0.1528966724872589\n",
      "Training | Average loss 0.15027944445610047\n",
      "Training iteration 0 | loss 0.14534059166908264\n",
      "Training iteration 1 | loss 0.14724209904670715\n",
      "Training iteration 2 | loss 0.1636638045310974\n",
      "Training iteration 3 | loss 0.15278899669647217\n",
      "Training iteration 4 | loss 0.142191544175148\n",
      "Training | Average loss 0.15024540722370147\n",
      "Training iteration 0 | loss 0.15386293828487396\n",
      "Training iteration 1 | loss 0.1533156782388687\n",
      "Training iteration 2 | loss 0.15617413818836212\n",
      "Training iteration 3 | loss 0.15854549407958984\n",
      "Training iteration 4 | loss 0.1290305256843567\n",
      "Training | Average loss 0.15018575489521027\n",
      "Training iteration 0 | loss 0.1686355173587799\n",
      "Training iteration 1 | loss 0.14430180191993713\n",
      "Training iteration 2 | loss 0.15524473786354065\n",
      "Training iteration 3 | loss 0.13624484837055206\n",
      "Training iteration 4 | loss 0.1483316868543625\n",
      "Training | Average loss 0.15055171847343446\n",
      "Training iteration 0 | loss 0.13358257710933685\n",
      "Training iteration 1 | loss 0.1431209146976471\n",
      "Training iteration 2 | loss 0.1404195874929428\n",
      "Training iteration 3 | loss 0.15448294579982758\n",
      "Training iteration 4 | loss 0.17475751042366028\n",
      "Training | Average loss 0.14927270710468293\n",
      "Training iteration 0 | loss 0.1330758035182953\n",
      "Training iteration 1 | loss 0.162744402885437\n",
      "Training iteration 2 | loss 0.14791929721832275\n",
      "Training iteration 3 | loss 0.1437569111585617\n",
      "Training iteration 4 | loss 0.157992422580719\n",
      "Training | Average loss 0.14909776747226716\n",
      "Training iteration 0 | loss 0.13325387239456177\n",
      "Training iteration 1 | loss 0.16163229942321777\n",
      "Training iteration 2 | loss 0.15110470354557037\n",
      "Training iteration 3 | loss 0.1508823037147522\n",
      "Training iteration 4 | loss 0.15350031852722168\n",
      "Training | Average loss 0.15007469952106475\n",
      "Training iteration 0 | loss 0.15591156482696533\n",
      "Training iteration 1 | loss 0.14400114119052887\n",
      "Training iteration 2 | loss 0.15870241820812225\n",
      "Training iteration 3 | loss 0.15752878785133362\n",
      "Training iteration 4 | loss 0.13596630096435547\n",
      "Training | Average loss 0.15042204260826111\n",
      "Training iteration 0 | loss 0.15797361731529236\n",
      "Training iteration 1 | loss 0.1427316516637802\n",
      "Training iteration 2 | loss 0.15661831200122833\n",
      "Training iteration 3 | loss 0.13429829478263855\n",
      "Training iteration 4 | loss 0.15350283682346344\n",
      "Training | Average loss 0.14902494251728057\n",
      "Training iteration 0 | loss 0.14203982055187225\n",
      "Training iteration 1 | loss 0.14418566226959229\n",
      "Training iteration 2 | loss 0.15421201288700104\n",
      "Training iteration 3 | loss 0.14664091169834137\n",
      "Training iteration 4 | loss 0.15769203007221222\n",
      "Training | Average loss 0.14895408749580383\n",
      "Training iteration 0 | loss 0.1373031884431839\n",
      "Training iteration 1 | loss 0.1516227275133133\n",
      "Training iteration 2 | loss 0.13860061764717102\n",
      "Training iteration 3 | loss 0.16541463136672974\n",
      "Training iteration 4 | loss 0.14926178753376007\n",
      "Training | Average loss 0.1484405905008316\n",
      "Training iteration 0 | loss 0.14638817310333252\n",
      "Training iteration 1 | loss 0.15027722716331482\n",
      "Training iteration 2 | loss 0.13946844637393951\n",
      "Training iteration 3 | loss 0.1510804295539856\n",
      "Training iteration 4 | loss 0.15966418385505676\n",
      "Training | Average loss 0.14937569200992584\n",
      "Training iteration 0 | loss 0.1578707993030548\n",
      "Training iteration 1 | loss 0.14104855060577393\n",
      "Training iteration 2 | loss 0.16234661638736725\n",
      "Training iteration 3 | loss 0.1329840123653412\n",
      "Training iteration 4 | loss 0.14798371493816376\n",
      "Training | Average loss 0.14844673871994019\n",
      "Training iteration 0 | loss 0.1533033698797226\n",
      "Training iteration 1 | loss 0.1450364738702774\n",
      "Training iteration 2 | loss 0.15428172051906586\n",
      "Training iteration 3 | loss 0.14803822338581085\n",
      "Training iteration 4 | loss 0.1392035037279129\n",
      "Training | Average loss 0.14797265827655792\n",
      "Training iteration 0 | loss 0.15172085165977478\n",
      "Training iteration 1 | loss 0.15274034440517426\n",
      "Training iteration 2 | loss 0.1439068764448166\n",
      "Training iteration 3 | loss 0.13732129335403442\n",
      "Training iteration 4 | loss 0.15455301105976105\n",
      "Training | Average loss 0.14804847538471222\n",
      "Training iteration 0 | loss 0.14362500607967377\n",
      "Training iteration 1 | loss 0.14155404269695282\n",
      "Training iteration 2 | loss 0.15827007591724396\n",
      "Training iteration 3 | loss 0.15673580765724182\n",
      "Training iteration 4 | loss 0.14321665465831757\n",
      "Training | Average loss 0.14868031740188598\n",
      "Training iteration 0 | loss 0.14714542031288147\n",
      "Training iteration 1 | loss 0.16181141138076782\n",
      "Training iteration 2 | loss 0.14809063076972961\n",
      "Training iteration 3 | loss 0.14120067656040192\n",
      "Training iteration 4 | loss 0.14360186457633972\n",
      "Training | Average loss 0.1483700007200241\n",
      "Training iteration 0 | loss 0.15442566573619843\n",
      "Training iteration 1 | loss 0.13053035736083984\n",
      "Training iteration 2 | loss 0.13433685898780823\n",
      "Training iteration 3 | loss 0.16796208918094635\n",
      "Training iteration 4 | loss 0.15598368644714355\n",
      "Training | Average loss 0.14864773154258729\n",
      "Training iteration 0 | loss 0.15238170325756073\n",
      "Training iteration 1 | loss 0.1379380077123642\n",
      "Training iteration 2 | loss 0.16562537848949432\n",
      "Training iteration 3 | loss 0.1417163908481598\n",
      "Training iteration 4 | loss 0.1447080373764038\n",
      "Training | Average loss 0.14847390353679657\n",
      "Training iteration 0 | loss 0.16428884863853455\n",
      "Training iteration 1 | loss 0.1283891201019287\n",
      "Training iteration 2 | loss 0.15342183411121368\n",
      "Training iteration 3 | loss 0.1398167610168457\n",
      "Training iteration 4 | loss 0.15509401261806488\n",
      "Training | Average loss 0.1482021152973175\n",
      "Training iteration 0 | loss 0.14094343781471252\n",
      "Training iteration 1 | loss 0.15521664917469025\n",
      "Training iteration 2 | loss 0.1565367877483368\n",
      "Training iteration 3 | loss 0.14405815303325653\n",
      "Training iteration 4 | loss 0.13866901397705078\n",
      "Training | Average loss 0.14708480834960938\n",
      "Training iteration 0 | loss 0.15112051367759705\n",
      "Training iteration 1 | loss 0.14320886135101318\n",
      "Training iteration 2 | loss 0.1506480574607849\n",
      "Training iteration 3 | loss 0.1348312497138977\n",
      "Training iteration 4 | loss 0.15704834461212158\n",
      "Training | Average loss 0.14737140536308288\n",
      "Training iteration 0 | loss 0.1393849402666092\n",
      "Training iteration 1 | loss 0.13597214221954346\n",
      "Training iteration 2 | loss 0.15008483827114105\n",
      "Training iteration 3 | loss 0.17185696959495544\n",
      "Training iteration 4 | loss 0.14861898124217987\n",
      "Training | Average loss 0.1491835743188858\n",
      "Training iteration 0 | loss 0.14048819243907928\n",
      "Training iteration 1 | loss 0.16808301210403442\n",
      "Training iteration 2 | loss 0.15238992869853973\n",
      "Training iteration 3 | loss 0.14828288555145264\n",
      "Training iteration 4 | loss 0.13385248184204102\n",
      "Training | Average loss 0.14861930012702942\n",
      "Training iteration 0 | loss 0.14126242697238922\n",
      "Training iteration 1 | loss 0.12363267689943314\n",
      "Training iteration 2 | loss 0.1713072657585144\n",
      "Training iteration 3 | loss 0.15016253292560577\n",
      "Training iteration 4 | loss 0.15148670971393585\n",
      "Training | Average loss 0.14757032245397567\n",
      "Training iteration 0 | loss 0.14131499826908112\n",
      "Training iteration 1 | loss 0.1491130292415619\n",
      "Training iteration 2 | loss 0.12955348193645477\n",
      "Training iteration 3 | loss 0.16072048246860504\n",
      "Training iteration 4 | loss 0.1507655680179596\n",
      "Training | Average loss 0.1462935119867325\n",
      "Training iteration 0 | loss 0.15584588050842285\n",
      "Training iteration 1 | loss 0.14707739651203156\n",
      "Training iteration 2 | loss 0.14121614396572113\n",
      "Training iteration 3 | loss 0.14281849563121796\n",
      "Training iteration 4 | loss 0.15329931676387787\n",
      "Training | Average loss 0.14805144667625428\n",
      "Training iteration 0 | loss 0.1571776568889618\n",
      "Training iteration 1 | loss 0.1450509876012802\n",
      "Training iteration 2 | loss 0.1597374826669693\n",
      "Training iteration 3 | loss 0.13911138474941254\n",
      "Training iteration 4 | loss 0.13705265522003174\n",
      "Training | Average loss 0.14762603342533112\n",
      "Training iteration 0 | loss 0.14969459176063538\n",
      "Training iteration 1 | loss 0.1497984230518341\n",
      "Training iteration 2 | loss 0.15168486535549164\n",
      "Training iteration 3 | loss 0.14447946846485138\n",
      "Training iteration 4 | loss 0.13879208266735077\n",
      "Training | Average loss 0.14688988626003266\n",
      "Training iteration 0 | loss 0.1333017200231552\n",
      "Training iteration 1 | loss 0.14871539175510406\n",
      "Training iteration 2 | loss 0.1371271163225174\n",
      "Training iteration 3 | loss 0.15512007474899292\n",
      "Training iteration 4 | loss 0.15956641733646393\n",
      "Training | Average loss 0.1467661440372467\n",
      "Training iteration 0 | loss 0.12845665216445923\n",
      "Training iteration 1 | loss 0.15034468472003937\n",
      "Training iteration 2 | loss 0.14611749351024628\n",
      "Training iteration 3 | loss 0.1614113301038742\n",
      "Training iteration 4 | loss 0.14682504534721375\n",
      "Training | Average loss 0.14663104116916656\n",
      "Training iteration 0 | loss 0.1379198580980301\n",
      "Training iteration 1 | loss 0.14832322299480438\n",
      "Training iteration 2 | loss 0.14069916307926178\n",
      "Training iteration 3 | loss 0.1598895639181137\n",
      "Training iteration 4 | loss 0.14355647563934326\n",
      "Training | Average loss 0.14607765674591064\n",
      "Training iteration 0 | loss 0.16111059486865997\n",
      "Training iteration 1 | loss 0.1472800374031067\n",
      "Training iteration 2 | loss 0.1457081288099289\n",
      "Training iteration 3 | loss 0.1518770158290863\n",
      "Training iteration 4 | loss 0.13641662895679474\n",
      "Training | Average loss 0.1484784811735153\n",
      "Training iteration 0 | loss 0.13841967284679413\n",
      "Training iteration 1 | loss 0.14692966639995575\n",
      "Training iteration 2 | loss 0.14332887530326843\n",
      "Training iteration 3 | loss 0.15609438717365265\n",
      "Training iteration 4 | loss 0.1432620733976364\n",
      "Training | Average loss 0.14560693502426147\n",
      "Training iteration 0 | loss 0.14397454261779785\n",
      "Training iteration 1 | loss 0.12698647379875183\n",
      "Training iteration 2 | loss 0.1563822478055954\n",
      "Training iteration 3 | loss 0.1515643149614334\n",
      "Training iteration 4 | loss 0.15019458532333374\n",
      "Training | Average loss 0.14582043290138244\n",
      "Training iteration 0 | loss 0.12074393033981323\n",
      "Training iteration 1 | loss 0.1519053727388382\n",
      "Training iteration 2 | loss 0.160105898976326\n",
      "Training iteration 3 | loss 0.1656123697757721\n",
      "Training iteration 4 | loss 0.13564743101596832\n",
      "Training | Average loss 0.14680300056934356\n",
      "Training iteration 0 | loss 0.14821843802928925\n",
      "Training iteration 1 | loss 0.14126275479793549\n",
      "Training iteration 2 | loss 0.147940531373024\n",
      "Training iteration 3 | loss 0.15036587417125702\n",
      "Training iteration 4 | loss 0.14340899884700775\n",
      "Training | Average loss 0.1462393194437027\n",
      "Training iteration 0 | loss 0.15410247445106506\n",
      "Training iteration 1 | loss 0.13751013576984406\n",
      "Training iteration 2 | loss 0.14423438906669617\n",
      "Training iteration 3 | loss 0.14791104197502136\n",
      "Training iteration 4 | loss 0.15214133262634277\n",
      "Training | Average loss 0.14717987477779387\n",
      "Training iteration 0 | loss 0.13793030381202698\n",
      "Training iteration 1 | loss 0.14104332029819489\n",
      "Training iteration 2 | loss 0.166071355342865\n",
      "Training iteration 3 | loss 0.1462351381778717\n",
      "Training iteration 4 | loss 0.13218244910240173\n",
      "Training | Average loss 0.14469251334667205\n",
      "Training iteration 0 | loss 0.12482163310050964\n",
      "Training iteration 1 | loss 0.1483125537633896\n",
      "Training iteration 2 | loss 0.15578952431678772\n",
      "Training iteration 3 | loss 0.16105331480503082\n",
      "Training iteration 4 | loss 0.14097319543361664\n",
      "Training | Average loss 0.1461900442838669\n",
      "Training iteration 0 | loss 0.13653351366519928\n",
      "Training iteration 1 | loss 0.14660696685314178\n",
      "Training iteration 2 | loss 0.12571386992931366\n",
      "Training iteration 3 | loss 0.1489725410938263\n",
      "Training iteration 4 | loss 0.1717361956834793\n",
      "Training | Average loss 0.14591261744499207\n",
      "Training iteration 0 | loss 0.1383575201034546\n",
      "Training iteration 1 | loss 0.1437121480703354\n",
      "Training iteration 2 | loss 0.1538727581501007\n",
      "Training iteration 3 | loss 0.14544522762298584\n",
      "Training iteration 4 | loss 0.14299950003623962\n",
      "Training | Average loss 0.14487743079662324\n",
      "Training iteration 0 | loss 0.14701400697231293\n",
      "Training iteration 1 | loss 0.14361028373241425\n",
      "Training iteration 2 | loss 0.1379697322845459\n",
      "Training iteration 3 | loss 0.14986282587051392\n",
      "Training iteration 4 | loss 0.14517509937286377\n",
      "Training | Average loss 0.14472638964653015\n",
      "Training iteration 0 | loss 0.14946158230304718\n",
      "Training iteration 1 | loss 0.12186242640018463\n",
      "Training iteration 2 | loss 0.14338429272174835\n",
      "Training iteration 3 | loss 0.15176986157894135\n",
      "Training iteration 4 | loss 0.15805229544639587\n",
      "Training | Average loss 0.14490609169006347\n",
      "Training iteration 0 | loss 0.14288116991519928\n",
      "Training iteration 1 | loss 0.153932586312294\n",
      "Training iteration 2 | loss 0.13882683217525482\n",
      "Training iteration 3 | loss 0.15664388239383698\n",
      "Training iteration 4 | loss 0.12809772789478302\n",
      "Training | Average loss 0.1440764397382736\n",
      "Training iteration 0 | loss 0.1475663036108017\n",
      "Training iteration 1 | loss 0.15174904465675354\n",
      "Training iteration 2 | loss 0.13335931301116943\n",
      "Training iteration 3 | loss 0.14697085320949554\n",
      "Training iteration 4 | loss 0.1427229791879654\n",
      "Training | Average loss 0.14447369873523713\n",
      "Training iteration 0 | loss 0.15574048459529877\n",
      "Training iteration 1 | loss 0.14569610357284546\n",
      "Training iteration 2 | loss 0.1359591782093048\n",
      "Training iteration 3 | loss 0.13952676951885223\n",
      "Training iteration 4 | loss 0.1455899178981781\n",
      "Training | Average loss 0.14450249075889587\n",
      "Training iteration 0 | loss 0.13635045289993286\n",
      "Training iteration 1 | loss 0.1532171219587326\n",
      "Training iteration 2 | loss 0.14758381247520447\n",
      "Training iteration 3 | loss 0.14309094846248627\n",
      "Training iteration 4 | loss 0.14088097214698792\n",
      "Training | Average loss 0.1442246615886688\n",
      "Training iteration 0 | loss 0.13780488073825836\n",
      "Training iteration 1 | loss 0.15118157863616943\n",
      "Training iteration 2 | loss 0.16122369468212128\n",
      "Training iteration 3 | loss 0.14628545939922333\n",
      "Training iteration 4 | loss 0.12417702376842499\n",
      "Training | Average loss 0.14413452744483948\n",
      "Training iteration 0 | loss 0.1405676156282425\n",
      "Training iteration 1 | loss 0.1583123356103897\n",
      "Training iteration 2 | loss 0.140951007604599\n",
      "Training iteration 3 | loss 0.1508222222328186\n",
      "Training iteration 4 | loss 0.13599620759487152\n",
      "Training | Average loss 0.14532987773418427\n",
      "Training iteration 0 | loss 0.155979722738266\n",
      "Training iteration 1 | loss 0.12848876416683197\n",
      "Training iteration 2 | loss 0.12912452220916748\n",
      "Training iteration 3 | loss 0.15790116786956787\n",
      "Training iteration 4 | loss 0.14953459799289703\n",
      "Training | Average loss 0.14420575499534607\n",
      "Training iteration 0 | loss 0.15234492719173431\n",
      "Training iteration 1 | loss 0.13631990551948547\n",
      "Training iteration 2 | loss 0.15054485201835632\n",
      "Training iteration 3 | loss 0.13530513644218445\n",
      "Training iteration 4 | loss 0.14375336468219757\n",
      "Training | Average loss 0.14365363717079163\n",
      "Training iteration 0 | loss 0.1436883807182312\n",
      "Training iteration 1 | loss 0.14374393224716187\n",
      "Training iteration 2 | loss 0.13155347108840942\n",
      "Training iteration 3 | loss 0.15539045631885529\n",
      "Training iteration 4 | loss 0.14782282710075378\n",
      "Training | Average loss 0.14443981349468232\n",
      "Training iteration 0 | loss 0.14537279307842255\n",
      "Training iteration 1 | loss 0.14458100497722626\n",
      "Training iteration 2 | loss 0.1407897174358368\n",
      "Training iteration 3 | loss 0.1457844227552414\n",
      "Training iteration 4 | loss 0.14782220125198364\n",
      "Training | Average loss 0.14487002789974213\n",
      "Training iteration 0 | loss 0.1567911058664322\n",
      "Training iteration 1 | loss 0.14101196825504303\n",
      "Training iteration 2 | loss 0.14550210535526276\n",
      "Training iteration 3 | loss 0.1347842961549759\n",
      "Training iteration 4 | loss 0.1396438628435135\n",
      "Training | Average loss 0.14354666769504548\n",
      "Training iteration 0 | loss 0.1462138444185257\n",
      "Training iteration 1 | loss 0.14728078246116638\n",
      "Training iteration 2 | loss 0.13578830659389496\n",
      "Training iteration 3 | loss 0.14435970783233643\n",
      "Training iteration 4 | loss 0.14493340253829956\n",
      "Training | Average loss 0.1437152087688446\n",
      "Training iteration 0 | loss 0.149208202958107\n",
      "Training iteration 1 | loss 0.14819453656673431\n",
      "Training iteration 2 | loss 0.1421218067407608\n",
      "Training iteration 3 | loss 0.131605863571167\n",
      "Training iteration 4 | loss 0.14510998129844666\n",
      "Training | Average loss 0.14324807822704316\n",
      "Training iteration 0 | loss 0.14626345038414001\n",
      "Training iteration 1 | loss 0.1633276492357254\n",
      "Training iteration 2 | loss 0.1349252164363861\n",
      "Training iteration 3 | loss 0.1328183114528656\n",
      "Training iteration 4 | loss 0.13900043070316315\n",
      "Training | Average loss 0.14326701164245606\n",
      "Training iteration 0 | loss 0.15291887521743774\n",
      "Training iteration 1 | loss 0.1459483504295349\n",
      "Training iteration 2 | loss 0.14365644752979279\n",
      "Training iteration 3 | loss 0.13342154026031494\n",
      "Training iteration 4 | loss 0.14064344763755798\n",
      "Training | Average loss 0.14331773221492766\n",
      "Training iteration 0 | loss 0.14293982088565826\n",
      "Training iteration 1 | loss 0.12400978058576584\n",
      "Training iteration 2 | loss 0.1513538658618927\n",
      "Training iteration 3 | loss 0.14873714745044708\n",
      "Training iteration 4 | loss 0.14626024663448334\n",
      "Training | Average loss 0.14266017228364944\n",
      "Training iteration 0 | loss 0.13593533635139465\n",
      "Training iteration 1 | loss 0.14530004560947418\n",
      "Training iteration 2 | loss 0.1412736028432846\n",
      "Training iteration 3 | loss 0.14320886135101318\n",
      "Training iteration 4 | loss 0.15335087478160858\n",
      "Training | Average loss 0.14381374418735504\n",
      "Training iteration 0 | loss 0.14722895622253418\n",
      "Training iteration 1 | loss 0.13446010649204254\n",
      "Training iteration 2 | loss 0.15563146770000458\n",
      "Training iteration 3 | loss 0.15029126405715942\n",
      "Training iteration 4 | loss 0.1278483122587204\n",
      "Training | Average loss 0.14309202134609222\n",
      "Training iteration 0 | loss 0.1469639539718628\n",
      "Training iteration 1 | loss 0.13420183956623077\n",
      "Training iteration 2 | loss 0.14350593090057373\n",
      "Training iteration 3 | loss 0.1360468566417694\n",
      "Training iteration 4 | loss 0.15370969474315643\n",
      "Training | Average loss 0.14288565516471863\n",
      "Training iteration 0 | loss 0.14972436428070068\n",
      "Training iteration 1 | loss 0.131846621632576\n",
      "Training iteration 2 | loss 0.14181867241859436\n",
      "Training iteration 3 | loss 0.14501111209392548\n",
      "Training iteration 4 | loss 0.145669087767601\n",
      "Training | Average loss 0.1428139716386795\n",
      "Training iteration 0 | loss 0.1508079171180725\n",
      "Training iteration 1 | loss 0.14330504834651947\n",
      "Training iteration 2 | loss 0.15787339210510254\n",
      "Training iteration 3 | loss 0.1418473869562149\n",
      "Training iteration 4 | loss 0.12561656534671783\n",
      "Training | Average loss 0.14389006197452545\n",
      "Training iteration 0 | loss 0.1454092562198639\n",
      "Training iteration 1 | loss 0.14531147480010986\n",
      "Training iteration 2 | loss 0.14430175721645355\n",
      "Training iteration 3 | loss 0.1437736451625824\n",
      "Training iteration 4 | loss 0.13432319462299347\n",
      "Training | Average loss 0.14262386560440063\n",
      "Training iteration 0 | loss 0.14705753326416016\n",
      "Training iteration 1 | loss 0.1449747085571289\n",
      "Training iteration 2 | loss 0.14610423147678375\n",
      "Training iteration 3 | loss 0.13813389837741852\n",
      "Training iteration 4 | loss 0.1422191709280014\n",
      "Training | Average loss 0.14369790852069855\n",
      "Training iteration 0 | loss 0.151741623878479\n",
      "Training iteration 1 | loss 0.1497325748205185\n",
      "Training iteration 2 | loss 0.13911142945289612\n",
      "Training iteration 3 | loss 0.13578562438488007\n",
      "Training iteration 4 | loss 0.1358782947063446\n",
      "Training | Average loss 0.14244990944862365\n",
      "Training iteration 0 | loss 0.1497633010149002\n",
      "Training iteration 1 | loss 0.14252687990665436\n",
      "Training iteration 2 | loss 0.1296844482421875\n",
      "Training iteration 3 | loss 0.13778430223464966\n",
      "Training iteration 4 | loss 0.15065348148345947\n",
      "Training | Average loss 0.14208248257637024\n",
      "Training iteration 0 | loss 0.14140309393405914\n",
      "Training iteration 1 | loss 0.15277071297168732\n",
      "Training iteration 2 | loss 0.13598991930484772\n",
      "Training iteration 3 | loss 0.13937538862228394\n",
      "Training iteration 4 | loss 0.14210711419582367\n",
      "Training | Average loss 0.14232924580574036\n",
      "Training iteration 0 | loss 0.12643730640411377\n",
      "Training iteration 1 | loss 0.134965717792511\n",
      "Training iteration 2 | loss 0.1513989120721817\n",
      "Training iteration 3 | loss 0.14643549919128418\n",
      "Training iteration 4 | loss 0.14996516704559326\n",
      "Training | Average loss 0.14184052050113677\n",
      "Training iteration 0 | loss 0.1350778192281723\n",
      "Training iteration 1 | loss 0.1622813493013382\n",
      "Training iteration 2 | loss 0.14048604667186737\n",
      "Training iteration 3 | loss 0.13083913922309875\n",
      "Training iteration 4 | loss 0.14101633429527283\n",
      "Training | Average loss 0.1419401377439499\n",
      "Training iteration 0 | loss 0.14294809103012085\n",
      "Training iteration 1 | loss 0.14444740116596222\n",
      "Training iteration 2 | loss 0.15286080539226532\n",
      "Training iteration 3 | loss 0.14235763251781464\n",
      "Training iteration 4 | loss 0.12861672043800354\n",
      "Training | Average loss 0.1422461301088333\n",
      "Training iteration 0 | loss 0.13340559601783752\n",
      "Training iteration 1 | loss 0.13643480837345123\n",
      "Training iteration 2 | loss 0.14074939489364624\n",
      "Training iteration 3 | loss 0.154007688164711\n",
      "Training iteration 4 | loss 0.1428426206111908\n",
      "Training | Average loss 0.14148802161216736\n",
      "Training iteration 0 | loss 0.1368018537759781\n",
      "Training iteration 1 | loss 0.15032224357128143\n",
      "Training iteration 2 | loss 0.15318745374679565\n",
      "Training iteration 3 | loss 0.13170915842056274\n",
      "Training iteration 4 | loss 0.1373894065618515\n",
      "Training | Average loss 0.1418820232152939\n",
      "Training iteration 0 | loss 0.14780855178833008\n",
      "Training iteration 1 | loss 0.13656587898731232\n",
      "Training iteration 2 | loss 0.1423574835062027\n",
      "Training iteration 3 | loss 0.13574431836605072\n",
      "Training iteration 4 | loss 0.1448519229888916\n",
      "Training | Average loss 0.1414656311273575\n",
      "Training iteration 0 | loss 0.14480946958065033\n",
      "Training iteration 1 | loss 0.14204607903957367\n",
      "Training iteration 2 | loss 0.13109485805034637\n",
      "Training iteration 3 | loss 0.13866809010505676\n",
      "Training iteration 4 | loss 0.15133999288082123\n",
      "Training | Average loss 0.14159169793128967\n",
      "Training iteration 0 | loss 0.13234452903270721\n",
      "Training iteration 1 | loss 0.13569612801074982\n",
      "Training iteration 2 | loss 0.14385969936847687\n",
      "Training iteration 3 | loss 0.1527213156223297\n",
      "Training iteration 4 | loss 0.14571638405323029\n",
      "Training | Average loss 0.14206761121749878\n",
      "Training iteration 0 | loss 0.13352757692337036\n",
      "Training iteration 1 | loss 0.1514088362455368\n",
      "Training iteration 2 | loss 0.14202328026294708\n",
      "Training iteration 3 | loss 0.14257930219173431\n",
      "Training iteration 4 | loss 0.13828809559345245\n",
      "Training | Average loss 0.1415654182434082\n",
      "Training iteration 0 | loss 0.14146344363689423\n",
      "Training iteration 1 | loss 0.13593745231628418\n",
      "Training iteration 2 | loss 0.1318264901638031\n",
      "Training iteration 3 | loss 0.16322608292102814\n",
      "Training iteration 4 | loss 0.13407564163208008\n",
      "Training | Average loss 0.14130582213401793\n",
      "Training iteration 0 | loss 0.1465364396572113\n",
      "Training iteration 1 | loss 0.15192346274852753\n",
      "Training iteration 2 | loss 0.1347256302833557\n",
      "Training iteration 3 | loss 0.13657574355602264\n",
      "Training iteration 4 | loss 0.13537803292274475\n",
      "Training | Average loss 0.14102786183357238\n",
      "Training iteration 0 | loss 0.14684756100177765\n",
      "Training iteration 1 | loss 0.149338498711586\n",
      "Training iteration 2 | loss 0.12635022401809692\n",
      "Training iteration 3 | loss 0.14002643525600433\n",
      "Training iteration 4 | loss 0.1519431471824646\n",
      "Training | Average loss 0.1429011732339859\n",
      "Training iteration 0 | loss 0.1503831446170807\n",
      "Training iteration 1 | loss 0.12910456955432892\n",
      "Training iteration 2 | loss 0.14888770878314972\n",
      "Training iteration 3 | loss 0.140420600771904\n",
      "Training iteration 4 | loss 0.1415979564189911\n",
      "Training | Average loss 0.14207879602909088\n",
      "Training iteration 0 | loss 0.13585366308689117\n",
      "Training iteration 1 | loss 0.13551414012908936\n",
      "Training iteration 2 | loss 0.14732736349105835\n",
      "Training iteration 3 | loss 0.14849597215652466\n",
      "Training iteration 4 | loss 0.13785839080810547\n",
      "Training | Average loss 0.1410099059343338\n",
      "Training iteration 0 | loss 0.15576188266277313\n",
      "Training iteration 1 | loss 0.15057812631130219\n",
      "Training iteration 2 | loss 0.14108461141586304\n",
      "Training iteration 3 | loss 0.12850157916545868\n",
      "Training iteration 4 | loss 0.13314668834209442\n",
      "Training | Average loss 0.14181457757949828\n",
      "Training iteration 0 | loss 0.14781558513641357\n",
      "Training iteration 1 | loss 0.14472517371177673\n",
      "Training iteration 2 | loss 0.14840008318424225\n",
      "Training iteration 3 | loss 0.140229269862175\n",
      "Training iteration 4 | loss 0.12510833144187927\n",
      "Training | Average loss 0.14125568866729737\n",
      "Training iteration 0 | loss 0.13455051183700562\n",
      "Training iteration 1 | loss 0.1348702311515808\n",
      "Training iteration 2 | loss 0.14709985256195068\n",
      "Training iteration 3 | loss 0.15736998617649078\n",
      "Training iteration 4 | loss 0.13348592817783356\n",
      "Training | Average loss 0.1414753019809723\n",
      "Training iteration 0 | loss 0.13833002746105194\n",
      "Training iteration 1 | loss 0.14224708080291748\n",
      "Training iteration 2 | loss 0.1557968407869339\n",
      "Training iteration 3 | loss 0.13287676870822906\n",
      "Training iteration 4 | loss 0.1426142454147339\n",
      "Training | Average loss 0.14237299263477327\n",
      "Training iteration 0 | loss 0.14085359871387482\n",
      "Training iteration 1 | loss 0.1440533548593521\n",
      "Training iteration 2 | loss 0.13278743624687195\n",
      "Training iteration 3 | loss 0.1565799117088318\n",
      "Training iteration 4 | loss 0.13087093830108643\n",
      "Training | Average loss 0.1410290479660034\n",
      "Training iteration 0 | loss 0.12784548103809357\n",
      "Training iteration 1 | loss 0.14720818400382996\n",
      "Training iteration 2 | loss 0.14144106209278107\n",
      "Training iteration 3 | loss 0.14014816284179688\n",
      "Training iteration 4 | loss 0.14851723611354828\n",
      "Training | Average loss 0.14103202521800995\n",
      "Training iteration 0 | loss 0.13302674889564514\n",
      "Training iteration 1 | loss 0.13167285919189453\n",
      "Training iteration 2 | loss 0.14845021069049835\n",
      "Training iteration 3 | loss 0.14055508375167847\n",
      "Training iteration 4 | loss 0.14834626019001007\n",
      "Training | Average loss 0.1404102325439453\n",
      "Training iteration 0 | loss 0.1460476666688919\n",
      "Training iteration 1 | loss 0.15095637738704681\n",
      "Training iteration 2 | loss 0.1641474813222885\n",
      "Training iteration 3 | loss 0.1394823044538498\n",
      "Training iteration 4 | loss 0.12128487974405289\n",
      "Training | Average loss 0.144383741915226\n",
      "Training iteration 0 | loss 0.14067049324512482\n",
      "Training iteration 1 | loss 0.13745875656604767\n",
      "Training iteration 2 | loss 0.1491994857788086\n",
      "Training iteration 3 | loss 0.13860923051834106\n",
      "Training iteration 4 | loss 0.1381765455007553\n",
      "Training | Average loss 0.1408229023218155\n",
      "Training iteration 0 | loss 0.14393065869808197\n",
      "Training iteration 1 | loss 0.14646512269973755\n",
      "Training iteration 2 | loss 0.14701303839683533\n",
      "Training iteration 3 | loss 0.12409696727991104\n",
      "Training iteration 4 | loss 0.14242683351039886\n",
      "Training | Average loss 0.14078652411699294\n",
      "Training iteration 0 | loss 0.13624095916748047\n",
      "Training iteration 1 | loss 0.13751275837421417\n",
      "Training iteration 2 | loss 0.13495314121246338\n",
      "Training iteration 3 | loss 0.1517721563577652\n",
      "Training iteration 4 | loss 0.1472277194261551\n",
      "Training | Average loss 0.14154134690761566\n",
      "Training iteration 0 | loss 0.14470525085926056\n",
      "Training iteration 1 | loss 0.1395079791545868\n",
      "Training iteration 2 | loss 0.12510523200035095\n",
      "Training iteration 3 | loss 0.1467304676771164\n",
      "Training iteration 4 | loss 0.14864018559455872\n",
      "Training | Average loss 0.14093782305717467\n",
      "Training iteration 0 | loss 0.15099836885929108\n",
      "Training iteration 1 | loss 0.13777042925357819\n",
      "Training iteration 2 | loss 0.14014798402786255\n",
      "Training iteration 3 | loss 0.14826089143753052\n",
      "Training iteration 4 | loss 0.13001595437526703\n",
      "Training | Average loss 0.14143872559070586\n",
      "Training iteration 0 | loss 0.14268708229064941\n",
      "Training iteration 1 | loss 0.14388874173164368\n",
      "Training iteration 2 | loss 0.14389006793498993\n",
      "Training iteration 3 | loss 0.15477505326271057\n",
      "Training iteration 4 | loss 0.13058456778526306\n",
      "Training | Average loss 0.14316510260105134\n",
      "Training iteration 0 | loss 0.13547483086585999\n",
      "Training iteration 1 | loss 0.14208894968032837\n",
      "Training iteration 2 | loss 0.1448977291584015\n",
      "Training iteration 3 | loss 0.14623388648033142\n",
      "Training iteration 4 | loss 0.13440629839897156\n",
      "Training | Average loss 0.14062033891677855\n",
      "Training iteration 0 | loss 0.13064312934875488\n",
      "Training iteration 1 | loss 0.14518985152244568\n",
      "Training iteration 2 | loss 0.14987647533416748\n",
      "Training iteration 3 | loss 0.1497219204902649\n",
      "Training iteration 4 | loss 0.13098205626010895\n",
      "Training | Average loss 0.14128268659114837\n",
      "Training iteration 0 | loss 0.1396637111902237\n",
      "Training iteration 1 | loss 0.15454620122909546\n",
      "Training iteration 2 | loss 0.14075608551502228\n",
      "Training iteration 3 | loss 0.13454902172088623\n",
      "Training iteration 4 | loss 0.13691306114196777\n",
      "Training | Average loss 0.14128561615943908\n",
      "Training iteration 0 | loss 0.1271442025899887\n",
      "Training iteration 1 | loss 0.14037181437015533\n",
      "Training iteration 2 | loss 0.14095714688301086\n",
      "Training iteration 3 | loss 0.15287180244922638\n",
      "Training iteration 4 | loss 0.13813969492912292\n",
      "Training | Average loss 0.13989693224430083\n",
      "Training iteration 0 | loss 0.13314764201641083\n",
      "Training iteration 1 | loss 0.13746482133865356\n",
      "Training iteration 2 | loss 0.1377597153186798\n",
      "Training iteration 3 | loss 0.15055958926677704\n",
      "Training iteration 4 | loss 0.14609001576900482\n",
      "Training | Average loss 0.1410043567419052\n",
      "Training iteration 0 | loss 0.14637413620948792\n",
      "Training iteration 1 | loss 0.13492117822170258\n",
      "Training iteration 2 | loss 0.13991880416870117\n",
      "Training iteration 3 | loss 0.13533072173595428\n",
      "Training iteration 4 | loss 0.14653606712818146\n",
      "Training | Average loss 0.14061618149280547\n",
      "Training iteration 0 | loss 0.14698204398155212\n",
      "Training iteration 1 | loss 0.16290462017059326\n",
      "Training iteration 2 | loss 0.13447418808937073\n",
      "Training iteration 3 | loss 0.13329188525676727\n",
      "Training iteration 4 | loss 0.12410643696784973\n",
      "Training | Average loss 0.1403518348932266\n",
      "Training iteration 0 | loss 0.13980378210544586\n",
      "Training iteration 1 | loss 0.13392703235149384\n",
      "Training iteration 2 | loss 0.13677027821540833\n",
      "Training iteration 3 | loss 0.14862170815467834\n",
      "Training iteration 4 | loss 0.1418142169713974\n",
      "Training | Average loss 0.14018740355968476\n",
      "Training iteration 0 | loss 0.1241869181394577\n",
      "Training iteration 1 | loss 0.14523188769817352\n",
      "Training iteration 2 | loss 0.13582399487495422\n",
      "Training iteration 3 | loss 0.1506243646144867\n",
      "Training iteration 4 | loss 0.14160992205142975\n",
      "Training | Average loss 0.13949541747570038\n",
      "Training iteration 0 | loss 0.1344815492630005\n",
      "Training iteration 1 | loss 0.13092045485973358\n",
      "Training iteration 2 | loss 0.14451156556606293\n",
      "Training iteration 3 | loss 0.15215419232845306\n",
      "Training iteration 4 | loss 0.14000990986824036\n",
      "Training | Average loss 0.14041553437709808\n",
      "Training iteration 0 | loss 0.13928183913230896\n",
      "Training iteration 1 | loss 0.13074636459350586\n",
      "Training iteration 2 | loss 0.15339316427707672\n",
      "Training iteration 3 | loss 0.1450885832309723\n",
      "Training iteration 4 | loss 0.1412685066461563\n",
      "Training | Average loss 0.14195569157600402\n",
      "Training iteration 0 | loss 0.12489451467990875\n",
      "Training iteration 1 | loss 0.15195122361183167\n",
      "Training iteration 2 | loss 0.14024870097637177\n",
      "Training iteration 3 | loss 0.14284288883209229\n",
      "Training iteration 4 | loss 0.14080916345119476\n",
      "Training | Average loss 0.14014929831027984\n",
      "Training iteration 0 | loss 0.1526321917772293\n",
      "Training iteration 1 | loss 0.12807172536849976\n",
      "Training iteration 2 | loss 0.12656594812870026\n",
      "Training iteration 3 | loss 0.15223290026187897\n",
      "Training iteration 4 | loss 0.13846948742866516\n",
      "Training | Average loss 0.1395944505929947\n",
      "Training iteration 0 | loss 0.14930212497711182\n",
      "Training iteration 1 | loss 0.13756856322288513\n",
      "Training iteration 2 | loss 0.12447825074195862\n",
      "Training iteration 3 | loss 0.14624181389808655\n",
      "Training iteration 4 | loss 0.1394479125738144\n",
      "Training | Average loss 0.1394077330827713\n",
      "Training iteration 0 | loss 0.13320456445217133\n",
      "Training iteration 1 | loss 0.13328218460083008\n",
      "Training iteration 2 | loss 0.1391269862651825\n",
      "Training iteration 3 | loss 0.14259570837020874\n",
      "Training iteration 4 | loss 0.14802826941013336\n",
      "Training | Average loss 0.1392475426197052\n",
      "Training iteration 0 | loss 0.15120789408683777\n",
      "Training iteration 1 | loss 0.1451869159936905\n",
      "Training iteration 2 | loss 0.13059087097644806\n",
      "Training iteration 3 | loss 0.13717769086360931\n",
      "Training iteration 4 | loss 0.13352279365062714\n",
      "Training | Average loss 0.13953723311424254\n",
      "Training iteration 0 | loss 0.1336362510919571\n",
      "Training iteration 1 | loss 0.1451573371887207\n",
      "Training iteration 2 | loss 0.13720685243606567\n",
      "Training iteration 3 | loss 0.13029134273529053\n",
      "Training iteration 4 | loss 0.14797605574131012\n",
      "Training | Average loss 0.1388535678386688\n",
      "Training iteration 0 | loss 0.13429108262062073\n",
      "Training iteration 1 | loss 0.15073344111442566\n",
      "Training iteration 2 | loss 0.1366780698299408\n",
      "Training iteration 3 | loss 0.14176946878433228\n",
      "Training iteration 4 | loss 0.13158753514289856\n",
      "Training | Average loss 0.1390119194984436\n",
      "Training iteration 0 | loss 0.1281726062297821\n",
      "Training iteration 1 | loss 0.14116805791854858\n",
      "Training iteration 2 | loss 0.15843908488750458\n",
      "Training iteration 3 | loss 0.13258028030395508\n",
      "Training iteration 4 | loss 0.138752743601799\n",
      "Training | Average loss 0.13982255458831788\n",
      "Training iteration 0 | loss 0.132314532995224\n",
      "Training iteration 1 | loss 0.1517041027545929\n",
      "Training iteration 2 | loss 0.1462518274784088\n",
      "Training iteration 3 | loss 0.14742429554462433\n",
      "Training iteration 4 | loss 0.12216655910015106\n",
      "Training | Average loss 0.13997226357460021\n",
      "Training iteration 0 | loss 0.14776641130447388\n",
      "Training iteration 1 | loss 0.13207124173641205\n",
      "Training iteration 2 | loss 0.12809434533119202\n",
      "Training iteration 3 | loss 0.14907589554786682\n",
      "Training iteration 4 | loss 0.13771747052669525\n",
      "Training | Average loss 0.138945072889328\n",
      "Training iteration 0 | loss 0.14031143486499786\n",
      "Training iteration 1 | loss 0.12677523493766785\n",
      "Training iteration 2 | loss 0.1399490386247635\n",
      "Training iteration 3 | loss 0.15297368168830872\n",
      "Training iteration 4 | loss 0.13161873817443848\n",
      "Training | Average loss 0.13832562565803527\n",
      "Training iteration 0 | loss 0.12960121035575867\n",
      "Training iteration 1 | loss 0.14764270186424255\n",
      "Training iteration 2 | loss 0.13832640647888184\n",
      "Training iteration 3 | loss 0.13369621336460114\n",
      "Training iteration 4 | loss 0.14336949586868286\n",
      "Training | Average loss 0.13852720558643342\n",
      "Training iteration 0 | loss 0.1368994563817978\n",
      "Training iteration 1 | loss 0.14975398778915405\n",
      "Training iteration 2 | loss 0.14377877116203308\n",
      "Training iteration 3 | loss 0.1379888653755188\n",
      "Training iteration 4 | loss 0.12398681789636612\n",
      "Training | Average loss 0.13848157972097397\n",
      "Training iteration 0 | loss 0.13719743490219116\n",
      "Training iteration 1 | loss 0.1442849338054657\n",
      "Training iteration 2 | loss 0.13054080307483673\n",
      "Training iteration 3 | loss 0.13781306147575378\n",
      "Training iteration 4 | loss 0.14769722521305084\n",
      "Training | Average loss 0.13950669169425964\n",
      "Training iteration 0 | loss 0.14217686653137207\n",
      "Training iteration 1 | loss 0.1413067728281021\n",
      "Training iteration 2 | loss 0.1276552528142929\n",
      "Training iteration 3 | loss 0.14301598072052002\n",
      "Training iteration 4 | loss 0.14422741532325745\n",
      "Training | Average loss 0.13967645764350892\n",
      "Training iteration 0 | loss 0.1405630260705948\n",
      "Training iteration 1 | loss 0.14946994185447693\n",
      "Training iteration 2 | loss 0.14627154171466827\n",
      "Training iteration 3 | loss 0.12554021179676056\n",
      "Training iteration 4 | loss 0.13626143336296082\n",
      "Training | Average loss 0.13962123095989226\n",
      "Training iteration 0 | loss 0.1365409791469574\n",
      "Training iteration 1 | loss 0.1260496824979782\n",
      "Training iteration 2 | loss 0.15652185678482056\n",
      "Training iteration 3 | loss 0.13846202194690704\n",
      "Training iteration 4 | loss 0.1390812247991562\n",
      "Training | Average loss 0.13933115303516388\n",
      "Training iteration 0 | loss 0.13560520112514496\n",
      "Training iteration 1 | loss 0.132339745759964\n",
      "Training iteration 2 | loss 0.1526322215795517\n",
      "Training iteration 3 | loss 0.13980732858181\n",
      "Training iteration 4 | loss 0.1331399381160736\n",
      "Training | Average loss 0.13870488703250886\n",
      "Training iteration 0 | loss 0.12367385625839233\n",
      "Training iteration 1 | loss 0.14673887193202972\n",
      "Training iteration 2 | loss 0.1524726003408432\n",
      "Training iteration 3 | loss 0.13351503014564514\n",
      "Training iteration 4 | loss 0.13529975712299347\n",
      "Training | Average loss 0.13834002315998079\n",
      "Training iteration 0 | loss 0.14722909033298492\n",
      "Training iteration 1 | loss 0.13893139362335205\n",
      "Training iteration 2 | loss 0.13806205987930298\n",
      "Training iteration 3 | loss 0.13667748868465424\n",
      "Training iteration 4 | loss 0.1355794072151184\n",
      "Training | Average loss 0.13929588794708253\n",
      "Training iteration 0 | loss 0.14105330407619476\n",
      "Training iteration 1 | loss 0.14329186081886292\n",
      "Training iteration 2 | loss 0.14443126320838928\n",
      "Training iteration 3 | loss 0.13187459111213684\n",
      "Training iteration 4 | loss 0.1324673295021057\n",
      "Training | Average loss 0.1386236697435379\n",
      "Training iteration 0 | loss 0.14200328290462494\n",
      "Training iteration 1 | loss 0.13572579622268677\n",
      "Training iteration 2 | loss 0.1478378027677536\n",
      "Training iteration 3 | loss 0.13675455749034882\n",
      "Training iteration 4 | loss 0.12863823771476746\n",
      "Training | Average loss 0.13819193542003633\n",
      "Training iteration 0 | loss 0.1310558170080185\n",
      "Training iteration 1 | loss 0.12335582822561264\n",
      "Training iteration 2 | loss 0.1428256779909134\n",
      "Training iteration 3 | loss 0.14121834933757782\n",
      "Training iteration 4 | loss 0.15514402091503143\n",
      "Training | Average loss 0.13871993869543076\n",
      "Training iteration 0 | loss 0.13282600045204163\n",
      "Training iteration 1 | loss 0.15805630385875702\n",
      "Training iteration 2 | loss 0.14824873208999634\n",
      "Training iteration 3 | loss 0.13311977684497833\n",
      "Training iteration 4 | loss 0.12040005624294281\n",
      "Training | Average loss 0.13853017389774322\n",
      "Training iteration 0 | loss 0.13268882036209106\n",
      "Training iteration 1 | loss 0.14276950061321259\n",
      "Training iteration 2 | loss 0.1329105645418167\n",
      "Training iteration 3 | loss 0.1371317058801651\n",
      "Training iteration 4 | loss 0.14124034345149994\n",
      "Training | Average loss 0.1373481869697571\n",
      "Training iteration 0 | loss 0.1331312507390976\n",
      "Training iteration 1 | loss 0.1387219876050949\n",
      "Training iteration 2 | loss 0.13859568536281586\n",
      "Training iteration 3 | loss 0.14009585976600647\n",
      "Training iteration 4 | loss 0.1381712555885315\n",
      "Training | Average loss 0.13774320781230925\n",
      "Training iteration 0 | loss 0.1503247618675232\n",
      "Training iteration 1 | loss 0.14068518579006195\n",
      "Training iteration 2 | loss 0.1410982310771942\n",
      "Training iteration 3 | loss 0.13388121128082275\n",
      "Training iteration 4 | loss 0.12180323153734207\n",
      "Training | Average loss 0.13755852431058885\n",
      "Training iteration 0 | loss 0.136138916015625\n",
      "Training iteration 1 | loss 0.1325308233499527\n",
      "Training iteration 2 | loss 0.15198880434036255\n",
      "Training iteration 3 | loss 0.13596725463867188\n",
      "Training iteration 4 | loss 0.1294790357351303\n",
      "Training | Average loss 0.13722096681594848\n",
      "Training iteration 0 | loss 0.1432582587003708\n",
      "Training iteration 1 | loss 0.14331817626953125\n",
      "Training iteration 2 | loss 0.1365855187177658\n",
      "Training iteration 3 | loss 0.1318414956331253\n",
      "Training iteration 4 | loss 0.13177916407585144\n",
      "Training | Average loss 0.1373565226793289\n",
      "Training iteration 0 | loss 0.14254054427146912\n",
      "Training iteration 1 | loss 0.13998207449913025\n",
      "Training iteration 2 | loss 0.14185574650764465\n",
      "Training iteration 3 | loss 0.13114039599895477\n",
      "Training iteration 4 | loss 0.1351848840713501\n",
      "Training | Average loss 0.13814072906970978\n",
      "Training iteration 0 | loss 0.14489836990833282\n",
      "Training iteration 1 | loss 0.12663395702838898\n",
      "Training iteration 2 | loss 0.14867453277111053\n",
      "Training iteration 3 | loss 0.1190844401717186\n",
      "Training iteration 4 | loss 0.14859937131404877\n",
      "Training | Average loss 0.13757813423871995\n",
      "Training iteration 0 | loss 0.14210674166679382\n",
      "Training iteration 1 | loss 0.12126537412405014\n",
      "Training iteration 2 | loss 0.1401718705892563\n",
      "Training iteration 3 | loss 0.14628930389881134\n",
      "Training iteration 4 | loss 0.1398351937532425\n",
      "Training | Average loss 0.1379336968064308\n",
      "Training iteration 0 | loss 0.142938032746315\n",
      "Training iteration 1 | loss 0.14198043942451477\n",
      "Training iteration 2 | loss 0.13132978975772858\n",
      "Training iteration 3 | loss 0.1369023621082306\n",
      "Training iteration 4 | loss 0.13997964560985565\n",
      "Training | Average loss 0.13862605392932892\n",
      "Training iteration 0 | loss 0.14056499302387238\n",
      "Training iteration 1 | loss 0.13537293672561646\n",
      "Training iteration 2 | loss 0.13531027734279633\n",
      "Training iteration 3 | loss 0.12809714674949646\n",
      "Training iteration 4 | loss 0.1473439484834671\n",
      "Training | Average loss 0.13733786046504975\n",
      "Training iteration 0 | loss 0.12519022822380066\n",
      "Training iteration 1 | loss 0.15591447055339813\n",
      "Training iteration 2 | loss 0.12439937889575958\n",
      "Training iteration 3 | loss 0.13573582470417023\n",
      "Training iteration 4 | loss 0.14542809128761292\n",
      "Training | Average loss 0.13733359873294831\n",
      "Training iteration 0 | loss 0.1416219025850296\n",
      "Training iteration 1 | loss 0.14705023169517517\n",
      "Training iteration 2 | loss 0.1394154280424118\n",
      "Training iteration 3 | loss 0.1197638139128685\n",
      "Training iteration 4 | loss 0.13531070947647095\n",
      "Training | Average loss 0.13663241714239122\n",
      "Training iteration 0 | loss 0.12487995624542236\n",
      "Training iteration 1 | loss 0.13057512044906616\n",
      "Training iteration 2 | loss 0.14574399590492249\n",
      "Training iteration 3 | loss 0.13656961917877197\n",
      "Training iteration 4 | loss 0.14401273429393768\n",
      "Training | Average loss 0.13635628521442414\n",
      "Training iteration 0 | loss 0.1461009383201599\n",
      "Training iteration 1 | loss 0.13023091852664948\n",
      "Training iteration 2 | loss 0.13393573462963104\n",
      "Training iteration 3 | loss 0.13481968641281128\n",
      "Training iteration 4 | loss 0.1471213698387146\n",
      "Training | Average loss 0.13844172954559325\n",
      "Training iteration 0 | loss 0.13935120403766632\n",
      "Training iteration 1 | loss 0.13002777099609375\n",
      "Training iteration 2 | loss 0.13779757916927338\n",
      "Training iteration 3 | loss 0.13466201722621918\n",
      "Training iteration 4 | loss 0.1415158212184906\n",
      "Training | Average loss 0.13667087852954865\n",
      "Training iteration 0 | loss 0.13844123482704163\n",
      "Training iteration 1 | loss 0.13580457866191864\n",
      "Training iteration 2 | loss 0.1312996745109558\n",
      "Training iteration 3 | loss 0.12872882187366486\n",
      "Training iteration 4 | loss 0.1502220183610916\n",
      "Training | Average loss 0.1368992656469345\n",
      "Training iteration 0 | loss 0.13342617452144623\n",
      "Training iteration 1 | loss 0.15128560364246368\n",
      "Training iteration 2 | loss 0.13542057573795319\n",
      "Training iteration 3 | loss 0.13397343456745148\n",
      "Training iteration 4 | loss 0.12852028012275696\n",
      "Training | Average loss 0.1365252137184143\n",
      "Training iteration 0 | loss 0.12792202830314636\n",
      "Training iteration 1 | loss 0.12030168622732162\n",
      "Training iteration 2 | loss 0.14227427542209625\n",
      "Training iteration 3 | loss 0.13942241668701172\n",
      "Training iteration 4 | loss 0.15132728219032288\n",
      "Training | Average loss 0.13624953776597976\n",
      "Training iteration 0 | loss 0.14698784053325653\n",
      "Training iteration 1 | loss 0.1308847814798355\n",
      "Training iteration 2 | loss 0.12425947189331055\n",
      "Training iteration 3 | loss 0.15425057709217072\n",
      "Training iteration 4 | loss 0.14523331820964813\n",
      "Training | Average loss 0.1403231978416443\n",
      "Training iteration 0 | loss 0.14003875851631165\n",
      "Training iteration 1 | loss 0.13281993567943573\n",
      "Training iteration 2 | loss 0.12493149191141129\n",
      "Training iteration 3 | loss 0.14132390916347504\n",
      "Training iteration 4 | loss 0.1417151391506195\n",
      "Training | Average loss 0.13616584688425065\n",
      "Training iteration 0 | loss 0.14292943477630615\n",
      "Training iteration 1 | loss 0.13135889172554016\n",
      "Training iteration 2 | loss 0.13165691494941711\n",
      "Training iteration 3 | loss 0.13894513249397278\n",
      "Training iteration 4 | loss 0.14075209200382233\n",
      "Training | Average loss 0.1371284931898117\n",
      "Training iteration 0 | loss 0.14353816211223602\n",
      "Training iteration 1 | loss 0.12996013462543488\n",
      "Training iteration 2 | loss 0.1376660019159317\n",
      "Training iteration 3 | loss 0.148782417178154\n",
      "Training iteration 4 | loss 0.12132605165243149\n",
      "Training | Average loss 0.13625455349683763\n",
      "Training iteration 0 | loss 0.13351775705814362\n",
      "Training iteration 1 | loss 0.13228006660938263\n",
      "Training iteration 2 | loss 0.13601766526699066\n",
      "Training iteration 3 | loss 0.12965308129787445\n",
      "Training iteration 4 | loss 0.1535196304321289\n",
      "Training | Average loss 0.13699764013290405\n",
      "Training iteration 0 | loss 0.15131694078445435\n",
      "Training iteration 1 | loss 0.13802877068519592\n",
      "Training iteration 2 | loss 0.12994413077831268\n",
      "Training iteration 3 | loss 0.11760380119085312\n",
      "Training iteration 4 | loss 0.14881907403469086\n",
      "Training | Average loss 0.1371425434947014\n",
      "Training iteration 0 | loss 0.13458874821662903\n",
      "Training iteration 1 | loss 0.13973776996135712\n",
      "Training iteration 2 | loss 0.15215107798576355\n",
      "Training iteration 3 | loss 0.11896057426929474\n",
      "Training iteration 4 | loss 0.14286787807941437\n",
      "Training | Average loss 0.13766120970249177\n",
      "Training iteration 0 | loss 0.14325056970119476\n",
      "Training iteration 1 | loss 0.1433759480714798\n",
      "Training iteration 2 | loss 0.131078839302063\n",
      "Training iteration 3 | loss 0.152450293302536\n",
      "Training iteration 4 | loss 0.11462119221687317\n",
      "Training | Average loss 0.13695536851882933\n",
      "Training iteration 0 | loss 0.1325707882642746\n",
      "Training iteration 1 | loss 0.1374007910490036\n",
      "Training iteration 2 | loss 0.14759156107902527\n",
      "Training iteration 3 | loss 0.1331300437450409\n",
      "Training iteration 4 | loss 0.13076134026050568\n",
      "Training | Average loss 0.13629090487957002\n",
      "Training iteration 0 | loss 0.12573792040348053\n",
      "Training iteration 1 | loss 0.13267284631729126\n",
      "Training iteration 2 | loss 0.144090473651886\n",
      "Training iteration 3 | loss 0.13833245635032654\n",
      "Training iteration 4 | loss 0.13914108276367188\n",
      "Training | Average loss 0.13599495589733124\n",
      "Training iteration 0 | loss 0.13635236024856567\n",
      "Training iteration 1 | loss 0.1390136480331421\n",
      "Training iteration 2 | loss 0.14112251996994019\n",
      "Training iteration 3 | loss 0.12497007101774216\n",
      "Training iteration 4 | loss 0.13845069706439972\n",
      "Training | Average loss 0.13598185926675796\n",
      "Training iteration 0 | loss 0.15000538527965546\n",
      "Training iteration 1 | loss 0.13894467055797577\n",
      "Training iteration 2 | loss 0.13639414310455322\n",
      "Training iteration 3 | loss 0.1318189948797226\n",
      "Training iteration 4 | loss 0.1284964680671692\n",
      "Training | Average loss 0.13713193237781524\n",
      "Training iteration 0 | loss 0.1437188684940338\n",
      "Training iteration 1 | loss 0.13873130083084106\n",
      "Training iteration 2 | loss 0.13801085948944092\n",
      "Training iteration 3 | loss 0.1402793526649475\n",
      "Training iteration 4 | loss 0.12499065697193146\n",
      "Training | Average loss 0.13714620769023894\n",
      "Training iteration 0 | loss 0.11574451625347137\n",
      "Training iteration 1 | loss 0.1417960226535797\n",
      "Training iteration 2 | loss 0.14740437269210815\n",
      "Training iteration 3 | loss 0.13983675837516785\n",
      "Training iteration 4 | loss 0.1499839723110199\n",
      "Training | Average loss 0.1389531284570694\n",
      "Training iteration 0 | loss 0.13380718231201172\n",
      "Training iteration 1 | loss 0.15186575055122375\n",
      "Training iteration 2 | loss 0.12946198880672455\n",
      "Training iteration 3 | loss 0.13829974830150604\n",
      "Training iteration 4 | loss 0.13442689180374146\n",
      "Training | Average loss 0.1375723123550415\n",
      "Training iteration 0 | loss 0.13715873658657074\n",
      "Training iteration 1 | loss 0.1367928832769394\n",
      "Training iteration 2 | loss 0.1276172250509262\n",
      "Training iteration 3 | loss 0.1342267543077469\n",
      "Training iteration 4 | loss 0.14181099832057953\n",
      "Training | Average loss 0.13552131950855256\n",
      "Training iteration 0 | loss 0.13444511592388153\n",
      "Training iteration 1 | loss 0.12309464812278748\n",
      "Training iteration 2 | loss 0.14775896072387695\n",
      "Training iteration 3 | loss 0.1486324518918991\n",
      "Training iteration 4 | loss 0.12406118959188461\n",
      "Training | Average loss 0.13559847325086594\n",
      "Training iteration 0 | loss 0.1367110162973404\n",
      "Training iteration 1 | loss 0.14278334379196167\n",
      "Training iteration 2 | loss 0.13503938913345337\n",
      "Training iteration 3 | loss 0.13944250345230103\n",
      "Training iteration 4 | loss 0.12758462131023407\n",
      "Training | Average loss 0.1363121747970581\n",
      "Training iteration 0 | loss 0.1311752200126648\n",
      "Training iteration 1 | loss 0.1376211792230606\n",
      "Training iteration 2 | loss 0.14692556858062744\n",
      "Training iteration 3 | loss 0.13226629793643951\n",
      "Training iteration 4 | loss 0.12926876544952393\n",
      "Training | Average loss 0.13545140624046326\n",
      "Training iteration 0 | loss 0.13227635622024536\n",
      "Training iteration 1 | loss 0.13636226952075958\n",
      "Training iteration 2 | loss 0.15249967575073242\n",
      "Training iteration 3 | loss 0.12394891679286957\n",
      "Training iteration 4 | loss 0.1314629465341568\n",
      "Training | Average loss 0.13531003296375274\n",
      "Training iteration 0 | loss 0.13712956011295319\n",
      "Training iteration 1 | loss 0.12191519886255264\n",
      "Training iteration 2 | loss 0.1468811333179474\n",
      "Training iteration 3 | loss 0.1319347470998764\n",
      "Training iteration 4 | loss 0.14131991565227509\n",
      "Training | Average loss 0.13583611100912094\n",
      "Training iteration 0 | loss 0.1284252256155014\n",
      "Training iteration 1 | loss 0.14201797544956207\n",
      "Training iteration 2 | loss 0.12391465157270432\n",
      "Training iteration 3 | loss 0.13773393630981445\n",
      "Training iteration 4 | loss 0.148906871676445\n",
      "Training | Average loss 0.13619973212480546\n",
      "Training iteration 0 | loss 0.13793720304965973\n",
      "Training iteration 1 | loss 0.12977521121501923\n",
      "Training iteration 2 | loss 0.14891763031482697\n",
      "Training iteration 3 | loss 0.12601295113563538\n",
      "Training iteration 4 | loss 0.13438419997692108\n",
      "Training | Average loss 0.13540543913841246\n",
      "Training iteration 0 | loss 0.12319676578044891\n",
      "Training iteration 1 | loss 0.14931686222553253\n",
      "Training iteration 2 | loss 0.1391775757074356\n",
      "Training iteration 3 | loss 0.14078722894191742\n",
      "Training iteration 4 | loss 0.12890422344207764\n",
      "Training | Average loss 0.1362765312194824\n",
      "Training iteration 0 | loss 0.14311984181404114\n",
      "Training iteration 1 | loss 0.13168974220752716\n",
      "Training iteration 2 | loss 0.12828688323497772\n",
      "Training iteration 3 | loss 0.13533896207809448\n",
      "Training iteration 4 | loss 0.13754670321941376\n",
      "Training | Average loss 0.13519642651081085\n",
      "Training iteration 0 | loss 0.1436135321855545\n",
      "Training iteration 1 | loss 0.140550434589386\n",
      "Training iteration 2 | loss 0.13122917711734772\n",
      "Training iteration 3 | loss 0.12218992412090302\n",
      "Training iteration 4 | loss 0.1403457522392273\n",
      "Training | Average loss 0.1355857640504837\n",
      "Training iteration 0 | loss 0.1457972377538681\n",
      "Training iteration 1 | loss 0.1348029375076294\n",
      "Training iteration 2 | loss 0.12760746479034424\n",
      "Training iteration 3 | loss 0.13448725640773773\n",
      "Training iteration 4 | loss 0.13721232116222382\n",
      "Training | Average loss 0.13598144352436065\n",
      "Training iteration 0 | loss 0.1323752999305725\n",
      "Training iteration 1 | loss 0.13715152442455292\n",
      "Training iteration 2 | loss 0.1366967260837555\n",
      "Training iteration 3 | loss 0.1326456069946289\n",
      "Training iteration 4 | loss 0.1404813528060913\n",
      "Training | Average loss 0.13587010204792022\n",
      "Training iteration 0 | loss 0.12790311872959137\n",
      "Training iteration 1 | loss 0.13735690712928772\n",
      "Training iteration 2 | loss 0.14046765863895416\n",
      "Training iteration 3 | loss 0.1361386775970459\n",
      "Training iteration 4 | loss 0.1424417644739151\n",
      "Training | Average loss 0.13686162531375884\n",
      "Training iteration 0 | loss 0.13851840794086456\n",
      "Training iteration 1 | loss 0.14133867621421814\n",
      "Training iteration 2 | loss 0.12121626734733582\n",
      "Training iteration 3 | loss 0.13018465042114258\n",
      "Training iteration 4 | loss 0.14340756833553314\n",
      "Training | Average loss 0.13493311405181885\n",
      "Training iteration 0 | loss 0.13920903205871582\n",
      "Training iteration 1 | loss 0.12409202009439468\n",
      "Training iteration 2 | loss 0.1418704092502594\n",
      "Training iteration 3 | loss 0.13989491760730743\n",
      "Training iteration 4 | loss 0.13477569818496704\n",
      "Training | Average loss 0.13596841543912888\n",
      "Training iteration 0 | loss 0.1265670359134674\n",
      "Training iteration 1 | loss 0.14808489382266998\n",
      "Training iteration 2 | loss 0.1414848119020462\n",
      "Training iteration 3 | loss 0.1435045301914215\n",
      "Training iteration 4 | loss 0.11991821974515915\n",
      "Training | Average loss 0.13591189831495284\n",
      "Training iteration 0 | loss 0.13990946114063263\n",
      "Training iteration 1 | loss 0.13593731820583344\n",
      "Training iteration 2 | loss 0.13690166175365448\n",
      "Training iteration 3 | loss 0.12451408803462982\n",
      "Training iteration 4 | loss 0.13898013532161713\n",
      "Training | Average loss 0.1352485328912735\n",
      "Training iteration 0 | loss 0.12724970281124115\n",
      "Training iteration 1 | loss 0.1320033073425293\n",
      "Training iteration 2 | loss 0.14162203669548035\n",
      "Training iteration 3 | loss 0.13204030692577362\n",
      "Training iteration 4 | loss 0.14633408188819885\n",
      "Training | Average loss 0.13584988713264465\n",
      "Training iteration 0 | loss 0.12290839105844498\n",
      "Training iteration 1 | loss 0.15278339385986328\n",
      "Training iteration 2 | loss 0.13026568293571472\n",
      "Training iteration 3 | loss 0.14880892634391785\n",
      "Training iteration 4 | loss 0.13137593865394592\n",
      "Training | Average loss 0.13722846657037735\n",
      "Training iteration 0 | loss 0.12237849831581116\n",
      "Training iteration 1 | loss 0.1360606700181961\n",
      "Training iteration 2 | loss 0.13573668897151947\n",
      "Training iteration 3 | loss 0.1350926011800766\n",
      "Training iteration 4 | loss 0.14733721315860748\n",
      "Training | Average loss 0.13532113432884216\n",
      "Training iteration 0 | loss 0.13014967739582062\n",
      "Training iteration 1 | loss 0.1368217170238495\n",
      "Training iteration 2 | loss 0.13129960000514984\n",
      "Training iteration 3 | loss 0.14169752597808838\n",
      "Training iteration 4 | loss 0.1425524801015854\n",
      "Training | Average loss 0.13650420010089875\n",
      "Training iteration 0 | loss 0.13518013060092926\n",
      "Training iteration 1 | loss 0.14147424697875977\n",
      "Training iteration 2 | loss 0.12593761086463928\n",
      "Training iteration 3 | loss 0.14380556344985962\n",
      "Training iteration 4 | loss 0.12880393862724304\n",
      "Training | Average loss 0.1350402981042862\n",
      "Training iteration 0 | loss 0.1298447698354721\n",
      "Training iteration 1 | loss 0.13368608057498932\n",
      "Training iteration 2 | loss 0.12947863340377808\n",
      "Training iteration 3 | loss 0.13850542902946472\n",
      "Training iteration 4 | loss 0.13997632265090942\n",
      "Training | Average loss 0.13429824709892274\n",
      "Training iteration 0 | loss 0.13053388893604279\n",
      "Training iteration 1 | loss 0.1393488645553589\n",
      "Training iteration 2 | loss 0.14301727712154388\n",
      "Training iteration 3 | loss 0.12396158277988434\n",
      "Training iteration 4 | loss 0.1448269784450531\n",
      "Training | Average loss 0.1363377183675766\n",
      "Training iteration 0 | loss 0.1334058791399002\n",
      "Training iteration 1 | loss 0.1361512392759323\n",
      "Training iteration 2 | loss 0.13846446573734283\n",
      "Training iteration 3 | loss 0.12635473906993866\n",
      "Training iteration 4 | loss 0.13940006494522095\n",
      "Training | Average loss 0.134755277633667\n",
      "Training iteration 0 | loss 0.13006821274757385\n",
      "Training iteration 1 | loss 0.12308818101882935\n",
      "Training iteration 2 | loss 0.1381330043077469\n",
      "Training iteration 3 | loss 0.1356215924024582\n",
      "Training iteration 4 | loss 0.14956393837928772\n",
      "Training | Average loss 0.1352949857711792\n",
      "Training iteration 0 | loss 0.13492904603481293\n",
      "Training iteration 1 | loss 0.13648721575737\n",
      "Training iteration 2 | loss 0.1409754753112793\n",
      "Training iteration 3 | loss 0.12490034103393555\n",
      "Training iteration 4 | loss 0.13941729068756104\n",
      "Training | Average loss 0.13534187376499177\n",
      "Training iteration 0 | loss 0.13324899971485138\n",
      "Training iteration 1 | loss 0.14615294337272644\n",
      "Training iteration 2 | loss 0.13344018161296844\n",
      "Training iteration 3 | loss 0.13568510115146637\n",
      "Training iteration 4 | loss 0.12393755465745926\n",
      "Training | Average loss 0.13449295610189438\n",
      "Training iteration 0 | loss 0.1353682428598404\n",
      "Training iteration 1 | loss 0.1350964605808258\n",
      "Training iteration 2 | loss 0.13910029828548431\n",
      "Training iteration 3 | loss 0.12544111907482147\n",
      "Training iteration 4 | loss 0.13591797649860382\n",
      "Training | Average loss 0.13418481945991517\n",
      "Training iteration 0 | loss 0.13617387413978577\n",
      "Training iteration 1 | loss 0.13161107897758484\n",
      "Training iteration 2 | loss 0.13516414165496826\n",
      "Training iteration 3 | loss 0.13596224784851074\n",
      "Training iteration 4 | loss 0.13480493426322937\n",
      "Training | Average loss 0.1347432553768158\n",
      "Training iteration 0 | loss 0.13529765605926514\n",
      "Training iteration 1 | loss 0.13628287613391876\n",
      "Training iteration 2 | loss 0.11980835348367691\n",
      "Training iteration 3 | loss 0.14361892640590668\n",
      "Training iteration 4 | loss 0.13727664947509766\n",
      "Training | Average loss 0.13445689231157304\n",
      "Training iteration 0 | loss 0.12495436519384384\n",
      "Training iteration 1 | loss 0.14483006298542023\n",
      "Training iteration 2 | loss 0.13285088539123535\n",
      "Training iteration 3 | loss 0.1315648853778839\n",
      "Training iteration 4 | loss 0.13944241404533386\n",
      "Training | Average loss 0.13472852259874343\n",
      "Training iteration 0 | loss 0.1316688358783722\n",
      "Training iteration 1 | loss 0.14246264100074768\n",
      "Training iteration 2 | loss 0.1461351066827774\n",
      "Training iteration 3 | loss 0.12620244920253754\n",
      "Training iteration 4 | loss 0.1263357251882553\n",
      "Training | Average loss 0.13456095159053802\n",
      "Training iteration 0 | loss 0.14477263391017914\n",
      "Training iteration 1 | loss 0.14109325408935547\n",
      "Training iteration 2 | loss 0.13064166903495789\n",
      "Training iteration 3 | loss 0.1312403678894043\n",
      "Training iteration 4 | loss 0.1280594915151596\n",
      "Training | Average loss 0.13516148328781127\n",
      "Training iteration 0 | loss 0.1396067589521408\n",
      "Training iteration 1 | loss 0.13665050268173218\n",
      "Training iteration 2 | loss 0.12822090089321136\n",
      "Training iteration 3 | loss 0.13882294297218323\n",
      "Training iteration 4 | loss 0.12860269844532013\n",
      "Training | Average loss 0.13438076078891753\n",
      "Training iteration 0 | loss 0.1350809782743454\n",
      "Training iteration 1 | loss 0.12931668758392334\n",
      "Training iteration 2 | loss 0.143746480345726\n",
      "Training iteration 3 | loss 0.12808464467525482\n",
      "Training iteration 4 | loss 0.13486099243164062\n",
      "Training | Average loss 0.13421795666217803\n",
      "Training iteration 0 | loss 0.12287908047437668\n",
      "Training iteration 1 | loss 0.13719294965267181\n",
      "Training iteration 2 | loss 0.14806829392910004\n",
      "Training iteration 3 | loss 0.1384442001581192\n",
      "Training iteration 4 | loss 0.1290660947561264\n",
      "Training | Average loss 0.13513012379407882\n",
      "Training iteration 0 | loss 0.12920229136943817\n",
      "Training iteration 1 | loss 0.1392068713903427\n",
      "Training iteration 2 | loss 0.13518528640270233\n",
      "Training iteration 3 | loss 0.12779301404953003\n",
      "Training iteration 4 | loss 0.14008425176143646\n",
      "Training | Average loss 0.13429434299468995\n",
      "Training iteration 0 | loss 0.12928597629070282\n",
      "Training iteration 1 | loss 0.13416418433189392\n",
      "Training iteration 2 | loss 0.13258244097232819\n",
      "Training iteration 3 | loss 0.14090599119663239\n",
      "Training iteration 4 | loss 0.13810215890407562\n",
      "Training | Average loss 0.13500815033912658\n",
      "Training iteration 0 | loss 0.13534832000732422\n",
      "Training iteration 1 | loss 0.15494054555892944\n",
      "Training iteration 2 | loss 0.12989597022533417\n",
      "Training iteration 3 | loss 0.1282324492931366\n",
      "Training iteration 4 | loss 0.1252366453409195\n",
      "Training | Average loss 0.13473078608512878\n",
      "Training iteration 0 | loss 0.1366799771785736\n",
      "Training iteration 1 | loss 0.14595438539981842\n",
      "Training iteration 2 | loss 0.1446683704853058\n",
      "Training iteration 3 | loss 0.12505993247032166\n",
      "Training iteration 4 | loss 0.12474287301301956\n",
      "Training | Average loss 0.1354211077094078\n",
      "Training iteration 0 | loss 0.13716132938861847\n",
      "Training iteration 1 | loss 0.12471909075975418\n",
      "Training iteration 2 | loss 0.13479159772396088\n",
      "Training iteration 3 | loss 0.14726518094539642\n",
      "Training iteration 4 | loss 0.12532934546470642\n",
      "Training | Average loss 0.13385330885648727\n",
      "Training iteration 0 | loss 0.13860781490802765\n",
      "Training iteration 1 | loss 0.12303569167852402\n",
      "Training iteration 2 | loss 0.1324463188648224\n",
      "Training iteration 3 | loss 0.14667417109012604\n",
      "Training iteration 4 | loss 0.1374959647655487\n",
      "Training | Average loss 0.13565199226140975\n",
      "Training iteration 0 | loss 0.14118917286396027\n",
      "Training iteration 1 | loss 0.12200527638196945\n",
      "Training iteration 2 | loss 0.14737768471240997\n",
      "Training iteration 3 | loss 0.14002561569213867\n",
      "Training iteration 4 | loss 0.13416314125061035\n",
      "Training | Average loss 0.13695217818021774\n",
      "Training iteration 0 | loss 0.1353958696126938\n",
      "Training iteration 1 | loss 0.14710012078285217\n",
      "Training iteration 2 | loss 0.13154128193855286\n",
      "Training iteration 3 | loss 0.14016568660736084\n",
      "Training iteration 4 | loss 0.11799613386392593\n",
      "Training | Average loss 0.13443981856107712\n",
      "Training iteration 0 | loss 0.1367025375366211\n",
      "Training iteration 1 | loss 0.13317321240901947\n",
      "Training iteration 2 | loss 0.1278323531150818\n",
      "Training iteration 3 | loss 0.1335110068321228\n",
      "Training iteration 4 | loss 0.14384831488132477\n",
      "Training | Average loss 0.13501348495483398\n",
      "Training iteration 0 | loss 0.13311727344989777\n",
      "Training iteration 1 | loss 0.130702942609787\n",
      "Training iteration 2 | loss 0.13075047731399536\n",
      "Training iteration 3 | loss 0.14203675091266632\n",
      "Training iteration 4 | loss 0.1446276605129242\n",
      "Training | Average loss 0.13624702095985414\n",
      "Training iteration 0 | loss 0.13784189522266388\n",
      "Training iteration 1 | loss 0.1561834216117859\n",
      "Training iteration 2 | loss 0.12628056108951569\n",
      "Training iteration 3 | loss 0.14506934583187103\n",
      "Training iteration 4 | loss 0.12293610721826553\n",
      "Training | Average loss 0.13766226619482042\n",
      "Training iteration 0 | loss 0.15393026173114777\n",
      "Training iteration 1 | loss 0.1325724869966507\n",
      "Training iteration 2 | loss 0.12913672626018524\n",
      "Training iteration 3 | loss 0.1331275850534439\n",
      "Training iteration 4 | loss 0.12768210470676422\n",
      "Training | Average loss 0.13528983294963837\n",
      "Training iteration 0 | loss 0.13221019506454468\n",
      "Training iteration 1 | loss 0.14976556599140167\n",
      "Training iteration 2 | loss 0.1306137591600418\n",
      "Training iteration 3 | loss 0.12981580197811127\n",
      "Training iteration 4 | loss 0.12902279198169708\n",
      "Training | Average loss 0.1342856228351593\n",
      "Training iteration 0 | loss 0.1406865417957306\n",
      "Training iteration 1 | loss 0.14464372396469116\n",
      "Training iteration 2 | loss 0.12703044712543488\n",
      "Training iteration 3 | loss 0.13902677595615387\n",
      "Training iteration 4 | loss 0.12464006245136261\n",
      "Training | Average loss 0.1352055102586746\n",
      "Training iteration 0 | loss 0.14611002802848816\n",
      "Training iteration 1 | loss 0.14914701879024506\n",
      "Training iteration 2 | loss 0.12377314269542694\n",
      "Training iteration 3 | loss 0.1230289563536644\n",
      "Training iteration 4 | loss 0.12781833112239838\n",
      "Training | Average loss 0.1339754953980446\n",
      "Training iteration 0 | loss 0.13654862344264984\n",
      "Training iteration 1 | loss 0.14255888760089874\n",
      "Training iteration 2 | loss 0.12955893576145172\n",
      "Training iteration 3 | loss 0.13384948670864105\n",
      "Training iteration 4 | loss 0.12393589317798615\n",
      "Training | Average loss 0.1332903653383255\n",
      "Training iteration 0 | loss 0.1319837123155594\n",
      "Training iteration 1 | loss 0.12749814987182617\n",
      "Training iteration 2 | loss 0.1344314068555832\n",
      "Training iteration 3 | loss 0.1361551433801651\n",
      "Training iteration 4 | loss 0.1390993893146515\n",
      "Training | Average loss 0.13383356034755706\n",
      "Training iteration 0 | loss 0.1372428983449936\n",
      "Training iteration 1 | loss 0.1282007396221161\n",
      "Training iteration 2 | loss 0.12869633734226227\n",
      "Training iteration 3 | loss 0.14557363092899323\n",
      "Training iteration 4 | loss 0.1344020962715149\n",
      "Training | Average loss 0.13482314050197602\n",
      "Training iteration 0 | loss 0.14059804379940033\n",
      "Training iteration 1 | loss 0.1311781257390976\n",
      "Training iteration 2 | loss 0.13644903898239136\n",
      "Training iteration 3 | loss 0.1295851767063141\n",
      "Training iteration 4 | loss 0.13134638965129852\n",
      "Training | Average loss 0.13383135497570037\n",
      "Training iteration 0 | loss 0.13170623779296875\n",
      "Training iteration 1 | loss 0.14125747978687286\n",
      "Training iteration 2 | loss 0.13570688664913177\n",
      "Training iteration 3 | loss 0.13919486105442047\n",
      "Training iteration 4 | loss 0.12914113700389862\n",
      "Training | Average loss 0.1354013204574585\n",
      "Training iteration 0 | loss 0.13297264277935028\n",
      "Training iteration 1 | loss 0.1244657039642334\n",
      "Training iteration 2 | loss 0.13856405019760132\n",
      "Training iteration 3 | loss 0.13355378806591034\n",
      "Training iteration 4 | loss 0.13660559058189392\n",
      "Training | Average loss 0.13323235511779785\n",
      "Training iteration 0 | loss 0.14120320975780487\n",
      "Training iteration 1 | loss 0.12442298233509064\n",
      "Training iteration 2 | loss 0.12232545018196106\n",
      "Training iteration 3 | loss 0.1397780030965805\n",
      "Training iteration 4 | loss 0.14248213171958923\n",
      "Training | Average loss 0.13404235541820525\n",
      "Training iteration 0 | loss 0.14704012870788574\n",
      "Training iteration 1 | loss 0.1211930364370346\n",
      "Training iteration 2 | loss 0.12962907552719116\n",
      "Training iteration 3 | loss 0.13868805766105652\n",
      "Training iteration 4 | loss 0.13651810586452484\n",
      "Training | Average loss 0.13461368083953856\n",
      "Training iteration 0 | loss 0.1356602907180786\n",
      "Training iteration 1 | loss 0.13383665680885315\n",
      "Training iteration 2 | loss 0.13301204144954681\n",
      "Training iteration 3 | loss 0.13036969304084778\n",
      "Training iteration 4 | loss 0.13590872287750244\n",
      "Training | Average loss 0.13375748097896575\n",
      "Training iteration 0 | loss 0.1278451681137085\n",
      "Training iteration 1 | loss 0.126174658536911\n",
      "Training iteration 2 | loss 0.13434931635856628\n",
      "Training iteration 3 | loss 0.1373603790998459\n",
      "Training iteration 4 | loss 0.14034898579120636\n",
      "Training | Average loss 0.1332157015800476\n",
      "Training iteration 0 | loss 0.13805876672267914\n",
      "Training iteration 1 | loss 0.12838631868362427\n",
      "Training iteration 2 | loss 0.13200530409812927\n",
      "Training iteration 3 | loss 0.1309095174074173\n",
      "Training iteration 4 | loss 0.1386556625366211\n",
      "Training | Average loss 0.1336031138896942\n",
      "Training iteration 0 | loss 0.12566331028938293\n",
      "Training iteration 1 | loss 0.1286696493625641\n",
      "Training iteration 2 | loss 0.1383054107427597\n",
      "Training iteration 3 | loss 0.15017253160476685\n",
      "Training iteration 4 | loss 0.12823256850242615\n",
      "Training | Average loss 0.13420869410037994\n",
      "Training iteration 0 | loss 0.13016179203987122\n",
      "Training iteration 1 | loss 0.1488470733165741\n",
      "Training iteration 2 | loss 0.12366592139005661\n",
      "Training iteration 3 | loss 0.135024756193161\n",
      "Training iteration 4 | loss 0.13504590094089508\n",
      "Training | Average loss 0.1345490887761116\n",
      "Training iteration 0 | loss 0.1320463865995407\n",
      "Training iteration 1 | loss 0.13432951271533966\n",
      "Training iteration 2 | loss 0.13456004858016968\n",
      "Training iteration 3 | loss 0.1327625960111618\n",
      "Training iteration 4 | loss 0.1348184049129486\n",
      "Training | Average loss 0.1337033897638321\n",
      "Training iteration 0 | loss 0.14400750398635864\n",
      "Training iteration 1 | loss 0.1305345743894577\n",
      "Training iteration 2 | loss 0.15150412917137146\n",
      "Training iteration 3 | loss 0.13036422431468964\n",
      "Training iteration 4 | loss 0.12525981664657593\n",
      "Training | Average loss 0.13633404970169066\n",
      "Training iteration 0 | loss 0.13005048036575317\n",
      "Training iteration 1 | loss 0.1255662441253662\n",
      "Training iteration 2 | loss 0.1417248696088791\n",
      "Training iteration 3 | loss 0.1400739997625351\n",
      "Training iteration 4 | loss 0.12857936322689056\n",
      "Training | Average loss 0.13319899141788483\n",
      "Training iteration 0 | loss 0.1292477697134018\n",
      "Training iteration 1 | loss 0.13927879929542542\n",
      "Training iteration 2 | loss 0.13005539774894714\n",
      "Training iteration 3 | loss 0.13522282242774963\n",
      "Training iteration 4 | loss 0.1362021118402481\n",
      "Training | Average loss 0.13400138020515442\n",
      "Training iteration 0 | loss 0.13968083262443542\n",
      "Training iteration 1 | loss 0.12599779665470123\n",
      "Training iteration 2 | loss 0.13297714293003082\n",
      "Training iteration 3 | loss 0.13508212566375732\n",
      "Training iteration 4 | loss 0.13609780371189117\n",
      "Training | Average loss 0.1339671403169632\n",
      "Training iteration 0 | loss 0.12052623927593231\n",
      "Training iteration 1 | loss 0.13510170578956604\n",
      "Training iteration 2 | loss 0.133453369140625\n",
      "Training iteration 3 | loss 0.13429531455039978\n",
      "Training iteration 4 | loss 0.14190243184566498\n",
      "Training | Average loss 0.13305581212043763\n",
      "Training iteration 0 | loss 0.1306007355451584\n",
      "Training iteration 1 | loss 0.13142326474189758\n",
      "Training iteration 2 | loss 0.12888380885124207\n",
      "Training iteration 3 | loss 0.13567307591438293\n",
      "Training iteration 4 | loss 0.1389058381319046\n",
      "Training | Average loss 0.1330973446369171\n",
      "Training iteration 0 | loss 0.1295856386423111\n",
      "Training iteration 1 | loss 0.12826061248779297\n",
      "Training iteration 2 | loss 0.12458740919828415\n",
      "Training iteration 3 | loss 0.12573426961898804\n",
      "Training iteration 4 | loss 0.15311050415039062\n",
      "Training | Average loss 0.13225568681955338\n",
      "Training iteration 0 | loss 0.1340433806180954\n",
      "Training iteration 1 | loss 0.13942532241344452\n",
      "Training iteration 2 | loss 0.12160040438175201\n",
      "Training iteration 3 | loss 0.12981785833835602\n",
      "Training iteration 4 | loss 0.13872578740119934\n",
      "Training | Average loss 0.13272255063056945\n",
      "Training iteration 0 | loss 0.13668954372406006\n",
      "Training iteration 1 | loss 0.12503975629806519\n",
      "Training iteration 2 | loss 0.1312377005815506\n",
      "Training iteration 3 | loss 0.1481025218963623\n",
      "Training iteration 4 | loss 0.12631434202194214\n",
      "Training | Average loss 0.13347677290439605\n",
      "Training iteration 0 | loss 0.13725914061069489\n",
      "Training iteration 1 | loss 0.13229909539222717\n",
      "Training iteration 2 | loss 0.13109642267227173\n",
      "Training iteration 3 | loss 0.13778312504291534\n",
      "Training iteration 4 | loss 0.12631262838840485\n",
      "Training | Average loss 0.1329500824213028\n",
      "Training iteration 0 | loss 0.1343892365694046\n",
      "Training iteration 1 | loss 0.1485370546579361\n",
      "Training iteration 2 | loss 0.12214365601539612\n",
      "Training iteration 3 | loss 0.12262274324893951\n",
      "Training iteration 4 | loss 0.14157132804393768\n",
      "Training | Average loss 0.1338528037071228\n",
      "Training iteration 0 | loss 0.13009706139564514\n",
      "Training iteration 1 | loss 0.14019498229026794\n",
      "Training iteration 2 | loss 0.12935736775398254\n",
      "Training iteration 3 | loss 0.14091338217258453\n",
      "Training iteration 4 | loss 0.1290966421365738\n",
      "Training | Average loss 0.1339318871498108\n",
      "Training iteration 0 | loss 0.14429117739200592\n",
      "Training iteration 1 | loss 0.12777645885944366\n",
      "Training iteration 2 | loss 0.11945673078298569\n",
      "Training iteration 3 | loss 0.1326877325773239\n",
      "Training iteration 4 | loss 0.1375875473022461\n",
      "Training | Average loss 0.13235992938280106\n",
      "Training iteration 0 | loss 0.13502900302410126\n",
      "Training iteration 1 | loss 0.12892913818359375\n",
      "Training iteration 2 | loss 0.14024139940738678\n",
      "Training iteration 3 | loss 0.1416153907775879\n",
      "Training iteration 4 | loss 0.12008029967546463\n",
      "Training | Average loss 0.13317904621362686\n",
      "Training iteration 0 | loss 0.13807745277881622\n",
      "Training iteration 1 | loss 0.13635168969631195\n",
      "Training iteration 2 | loss 0.13044407963752747\n",
      "Training iteration 3 | loss 0.12419412285089493\n",
      "Training iteration 4 | loss 0.13730989396572113\n",
      "Training | Average loss 0.13327544778585435\n",
      "Training iteration 0 | loss 0.12924985587596893\n",
      "Training iteration 1 | loss 0.13287803530693054\n",
      "Training iteration 2 | loss 0.14363548159599304\n",
      "Training iteration 3 | loss 0.12730054557323456\n",
      "Training iteration 4 | loss 0.13129453361034393\n",
      "Training | Average loss 0.1328716903924942\n",
      "Training iteration 0 | loss 0.13548070192337036\n",
      "Training iteration 1 | loss 0.13589419424533844\n",
      "Training iteration 2 | loss 0.12397625297307968\n",
      "Training iteration 3 | loss 0.14215371012687683\n",
      "Training iteration 4 | loss 0.12596359848976135\n",
      "Training | Average loss 0.13269369155168534\n",
      "Training iteration 0 | loss 0.13844601809978485\n",
      "Training iteration 1 | loss 0.12812983989715576\n",
      "Training iteration 2 | loss 0.12748855352401733\n",
      "Training iteration 3 | loss 0.1352788209915161\n",
      "Training iteration 4 | loss 0.13737212121486664\n",
      "Training | Average loss 0.13334307074546814\n",
      "Training iteration 0 | loss 0.13226209580898285\n",
      "Training iteration 1 | loss 0.14660336077213287\n",
      "Training iteration 2 | loss 0.1287994533777237\n",
      "Training iteration 3 | loss 0.1359298974275589\n",
      "Training iteration 4 | loss 0.12282251566648483\n",
      "Training | Average loss 0.13328346461057664\n",
      "Training iteration 0 | loss 0.14313603937625885\n",
      "Training iteration 1 | loss 0.13991503417491913\n",
      "Training iteration 2 | loss 0.11623427271842957\n",
      "Training iteration 3 | loss 0.1316831409931183\n",
      "Training iteration 4 | loss 0.1395912915468216\n",
      "Training | Average loss 0.13411195576190948\n",
      "Training iteration 0 | loss 0.11795753240585327\n",
      "Training iteration 1 | loss 0.1283074915409088\n",
      "Training iteration 2 | loss 0.13601140677928925\n",
      "Training iteration 3 | loss 0.13536769151687622\n",
      "Training iteration 4 | loss 0.14699110388755798\n",
      "Training | Average loss 0.1329270452260971\n",
      "Training iteration 0 | loss 0.12450738996267319\n",
      "Training iteration 1 | loss 0.1311674267053604\n",
      "Training iteration 2 | loss 0.12839797139167786\n",
      "Training iteration 3 | loss 0.14634710550308228\n",
      "Training iteration 4 | loss 0.13114827871322632\n",
      "Training | Average loss 0.132313634455204\n",
      "Training iteration 0 | loss 0.12646888196468353\n",
      "Training iteration 1 | loss 0.1422961950302124\n",
      "Training iteration 2 | loss 0.136342853307724\n",
      "Training iteration 3 | loss 0.14535999298095703\n",
      "Training iteration 4 | loss 0.11166775226593018\n",
      "Training | Average loss 0.13242713510990142\n",
      "Training iteration 0 | loss 0.1366652399301529\n",
      "Training iteration 1 | loss 0.1279510259628296\n",
      "Training iteration 2 | loss 0.14317576587200165\n",
      "Training iteration 3 | loss 0.13311664760112762\n",
      "Training iteration 4 | loss 0.12201984226703644\n",
      "Training | Average loss 0.13258570432662964\n",
      "Training iteration 0 | loss 0.13245053589344025\n",
      "Training iteration 1 | loss 0.13156764209270477\n",
      "Training iteration 2 | loss 0.14395983517169952\n",
      "Training iteration 3 | loss 0.13815787434577942\n",
      "Training iteration 4 | loss 0.12912170588970184\n",
      "Training | Average loss 0.13505151867866516\n",
      "Training iteration 0 | loss 0.13845011591911316\n",
      "Training iteration 1 | loss 0.11919764429330826\n",
      "Training iteration 2 | loss 0.13233205676078796\n",
      "Training iteration 3 | loss 0.13601325452327728\n",
      "Training iteration 4 | loss 0.13483849167823792\n",
      "Training | Average loss 0.1321663126349449\n",
      "Training iteration 0 | loss 0.12770353257656097\n",
      "Training iteration 1 | loss 0.1309928297996521\n",
      "Training iteration 2 | loss 0.14110144972801208\n",
      "Training iteration 3 | loss 0.13607290387153625\n",
      "Training iteration 4 | loss 0.12749363481998444\n",
      "Training | Average loss 0.13267287015914916\n",
      "Training iteration 0 | loss 0.12639352679252625\n",
      "Training iteration 1 | loss 0.13161003589630127\n",
      "Training iteration 2 | loss 0.14913612604141235\n",
      "Training iteration 3 | loss 0.12805764377117157\n",
      "Training iteration 4 | loss 0.1388797163963318\n",
      "Training | Average loss 0.13481540977954865\n",
      "Training iteration 0 | loss 0.12223023921251297\n",
      "Training iteration 1 | loss 0.15025965869426727\n",
      "Training iteration 2 | loss 0.134812593460083\n",
      "Training iteration 3 | loss 0.12797030806541443\n",
      "Training iteration 4 | loss 0.1283080130815506\n",
      "Training | Average loss 0.13271616250276566\n",
      "Training iteration 0 | loss 0.1317494809627533\n",
      "Training iteration 1 | loss 0.14483608305454254\n",
      "Training iteration 2 | loss 0.15013955533504486\n",
      "Training iteration 3 | loss 0.12228227406740189\n",
      "Training iteration 4 | loss 0.11950462311506271\n",
      "Training | Average loss 0.13370240330696107\n",
      "Training iteration 0 | loss 0.1339091807603836\n",
      "Training iteration 1 | loss 0.12797388434410095\n",
      "Training iteration 2 | loss 0.13043907284736633\n",
      "Training iteration 3 | loss 0.1280135065317154\n",
      "Training iteration 4 | loss 0.14050887525081635\n",
      "Training | Average loss 0.13216890394687653\n",
      "Training iteration 0 | loss 0.13407164812088013\n",
      "Training iteration 1 | loss 0.1236022412776947\n",
      "Training iteration 2 | loss 0.12363294512033463\n",
      "Training iteration 3 | loss 0.1377318948507309\n",
      "Training iteration 4 | loss 0.14219290018081665\n",
      "Training | Average loss 0.1322463259100914\n",
      "Training iteration 0 | loss 0.13239949941635132\n",
      "Training iteration 1 | loss 0.12705488502979279\n",
      "Training iteration 2 | loss 0.1322764754295349\n",
      "Training iteration 3 | loss 0.1394786685705185\n",
      "Training iteration 4 | loss 0.12895303964614868\n",
      "Training | Average loss 0.13203251361846924\n",
      "Training iteration 0 | loss 0.14946387708187103\n",
      "Training iteration 1 | loss 0.12723837792873383\n",
      "Training iteration 2 | loss 0.12631027400493622\n",
      "Training iteration 3 | loss 0.13514253497123718\n",
      "Training iteration 4 | loss 0.12224171310663223\n",
      "Training | Average loss 0.1320793554186821\n",
      "Training iteration 0 | loss 0.14007174968719482\n",
      "Training iteration 1 | loss 0.1318712681531906\n",
      "Training iteration 2 | loss 0.1358662247657776\n",
      "Training iteration 3 | loss 0.12711337208747864\n",
      "Training iteration 4 | loss 0.1236891895532608\n",
      "Training | Average loss 0.1317223608493805\n",
      "Training iteration 0 | loss 0.13164962828159332\n",
      "Training iteration 1 | loss 0.1378239542245865\n",
      "Training iteration 2 | loss 0.12524211406707764\n",
      "Training iteration 3 | loss 0.14575262367725372\n",
      "Training iteration 4 | loss 0.12794679403305054\n",
      "Training | Average loss 0.13368302285671235\n",
      "Training iteration 0 | loss 0.12873809039592743\n",
      "Training iteration 1 | loss 0.1311207264661789\n",
      "Training iteration 2 | loss 0.13299405574798584\n",
      "Training iteration 3 | loss 0.1326514333486557\n",
      "Training iteration 4 | loss 0.1361466646194458\n",
      "Training | Average loss 0.13233019411563873\n",
      "Training iteration 0 | loss 0.12617623805999756\n",
      "Training iteration 1 | loss 0.1391865760087967\n",
      "Training iteration 2 | loss 0.12383626401424408\n",
      "Training iteration 3 | loss 0.12879833579063416\n",
      "Training iteration 4 | loss 0.13952644169330597\n",
      "Training | Average loss 0.1315047711133957\n",
      "Training iteration 0 | loss 0.13343174755573273\n",
      "Training iteration 1 | loss 0.13260485231876373\n",
      "Training iteration 2 | loss 0.1342189460992813\n",
      "Training iteration 3 | loss 0.1375107616186142\n",
      "Training iteration 4 | loss 0.12069928646087646\n",
      "Training | Average loss 0.13169311881065368\n",
      "Training iteration 0 | loss 0.14043934643268585\n",
      "Training iteration 1 | loss 0.12189985811710358\n",
      "Training iteration 2 | loss 0.13459840416908264\n",
      "Training iteration 3 | loss 0.1296529769897461\n",
      "Training iteration 4 | loss 0.13896505534648895\n",
      "Training | Average loss 0.13311112821102142\n",
      "Training iteration 0 | loss 0.14080968499183655\n",
      "Training iteration 1 | loss 0.13054677844047546\n",
      "Training iteration 2 | loss 0.11380573362112045\n",
      "Training iteration 3 | loss 0.13605277240276337\n",
      "Training iteration 4 | loss 0.13573847711086273\n",
      "Training | Average loss 0.13139068931341172\n",
      "Training iteration 0 | loss 0.1256761997938156\n",
      "Training iteration 1 | loss 0.14307603240013123\n",
      "Training iteration 2 | loss 0.13250571489334106\n",
      "Training iteration 3 | loss 0.1346682608127594\n",
      "Training iteration 4 | loss 0.12257057428359985\n",
      "Training | Average loss 0.13169935643672942\n",
      "Training iteration 0 | loss 0.1275508552789688\n",
      "Training iteration 1 | loss 0.1363791823387146\n",
      "Training iteration 2 | loss 0.12750409543514252\n",
      "Training iteration 3 | loss 0.13883483409881592\n",
      "Training iteration 4 | loss 0.12804535031318665\n",
      "Training | Average loss 0.1316628634929657\n",
      "Training iteration 0 | loss 0.12792189419269562\n",
      "Training iteration 1 | loss 0.13440220057964325\n",
      "Training iteration 2 | loss 0.1339220106601715\n",
      "Training iteration 3 | loss 0.1319509893655777\n",
      "Training iteration 4 | loss 0.1318107545375824\n",
      "Training | Average loss 0.1320015698671341\n",
      "Training iteration 0 | loss 0.132205069065094\n",
      "Training iteration 1 | loss 0.12873762845993042\n",
      "Training iteration 2 | loss 0.1248261034488678\n",
      "Training iteration 3 | loss 0.1288147270679474\n",
      "Training iteration 4 | loss 0.1438618153333664\n",
      "Training | Average loss 0.1316890686750412\n",
      "Training iteration 0 | loss 0.1326378583908081\n",
      "Training iteration 1 | loss 0.13934633135795593\n",
      "Training iteration 2 | loss 0.13914015889167786\n",
      "Training iteration 3 | loss 0.12412923574447632\n",
      "Training iteration 4 | loss 0.12745170295238495\n",
      "Training | Average loss 0.13254105746746064\n",
      "Training iteration 0 | loss 0.1282326579093933\n",
      "Training iteration 1 | loss 0.13350516557693481\n",
      "Training iteration 2 | loss 0.1295132040977478\n",
      "Training iteration 3 | loss 0.13343243300914764\n",
      "Training iteration 4 | loss 0.13515153527259827\n",
      "Training | Average loss 0.13196699917316437\n",
      "Training iteration 0 | loss 0.13072681427001953\n",
      "Training iteration 1 | loss 0.1356576681137085\n",
      "Training iteration 2 | loss 0.12566858530044556\n",
      "Training iteration 3 | loss 0.13532878458499908\n",
      "Training iteration 4 | loss 0.13143812119960785\n",
      "Training | Average loss 0.1317639946937561\n",
      "Training iteration 0 | loss 0.13109144568443298\n",
      "Training iteration 1 | loss 0.12728463113307953\n",
      "Training iteration 2 | loss 0.13721856474876404\n",
      "Training iteration 3 | loss 0.1310223489999771\n",
      "Training iteration 4 | loss 0.1304636001586914\n",
      "Training | Average loss 0.13141611814498902\n",
      "Training iteration 0 | loss 0.1464836597442627\n",
      "Training iteration 1 | loss 0.12423824518918991\n",
      "Training iteration 2 | loss 0.13161897659301758\n",
      "Training iteration 3 | loss 0.136651411652565\n",
      "Training iteration 4 | loss 0.1228390634059906\n",
      "Training | Average loss 0.13236627131700515\n",
      "Training iteration 0 | loss 0.13287054002285004\n",
      "Training iteration 1 | loss 0.13517984747886658\n",
      "Training iteration 2 | loss 0.1267753541469574\n",
      "Training iteration 3 | loss 0.11353638768196106\n",
      "Training iteration 4 | loss 0.14920487999916077\n",
      "Training | Average loss 0.13151340186595917\n",
      "Training iteration 0 | loss 0.13285188376903534\n",
      "Training iteration 1 | loss 0.13043251633644104\n",
      "Training iteration 2 | loss 0.12481201440095901\n",
      "Training iteration 3 | loss 0.14431948959827423\n",
      "Training iteration 4 | loss 0.13205815851688385\n",
      "Training | Average loss 0.1328948125243187\n",
      "Training iteration 0 | loss 0.1255066692829132\n",
      "Training iteration 1 | loss 0.13102656602859497\n",
      "Training iteration 2 | loss 0.14105847477912903\n",
      "Training iteration 3 | loss 0.13951660692691803\n",
      "Training iteration 4 | loss 0.12422171980142593\n",
      "Training | Average loss 0.13226600736379623\n",
      "Training iteration 0 | loss 0.1387571096420288\n",
      "Training iteration 1 | loss 0.13707929849624634\n",
      "Training iteration 2 | loss 0.11877410113811493\n",
      "Training iteration 3 | loss 0.11573091894388199\n",
      "Training iteration 4 | loss 0.14657025039196014\n",
      "Training | Average loss 0.13138233572244645\n",
      "Training iteration 0 | loss 0.12662389874458313\n",
      "Training iteration 1 | loss 0.13029532134532928\n",
      "Training iteration 2 | loss 0.13060352206230164\n",
      "Training iteration 3 | loss 0.14745627343654633\n",
      "Training iteration 4 | loss 0.12181847542524338\n",
      "Training | Average loss 0.13135949820280074\n",
      "Training iteration 0 | loss 0.14062269032001495\n",
      "Training iteration 1 | loss 0.12761840224266052\n",
      "Training iteration 2 | loss 0.1370658129453659\n",
      "Training iteration 3 | loss 0.13075238466262817\n",
      "Training iteration 4 | loss 0.12038108706474304\n",
      "Training | Average loss 0.13128807544708251\n",
      "Training iteration 0 | loss 0.12052015215158463\n",
      "Training iteration 1 | loss 0.1324075609445572\n",
      "Training iteration 2 | loss 0.13367125391960144\n",
      "Training iteration 3 | loss 0.1462419033050537\n",
      "Training iteration 4 | loss 0.12888847291469574\n",
      "Training | Average loss 0.13234586864709855\n",
      "Training iteration 0 | loss 0.1534447968006134\n",
      "Training iteration 1 | loss 0.13931655883789062\n",
      "Training iteration 2 | loss 0.13354164361953735\n",
      "Training iteration 3 | loss 0.11930089443922043\n",
      "Training iteration 4 | loss 0.11988770216703415\n",
      "Training | Average loss 0.1330983191728592\n",
      "Training iteration 0 | loss 0.14150279760360718\n",
      "Training iteration 1 | loss 0.12465416640043259\n",
      "Training iteration 2 | loss 0.13040800392627716\n",
      "Training iteration 3 | loss 0.13302874565124512\n",
      "Training iteration 4 | loss 0.12845003604888916\n",
      "Training | Average loss 0.13160874992609023\n",
      "Training iteration 0 | loss 0.12253759801387787\n",
      "Training iteration 1 | loss 0.13214842975139618\n",
      "Training iteration 2 | loss 0.14123320579528809\n",
      "Training iteration 3 | loss 0.1398470401763916\n",
      "Training iteration 4 | loss 0.12127838283777237\n",
      "Training | Average loss 0.13140893131494522\n",
      "Training iteration 0 | loss 0.12879233062267303\n",
      "Training iteration 1 | loss 0.1312505453824997\n",
      "Training iteration 2 | loss 0.1250257045030594\n",
      "Training iteration 3 | loss 0.13133886456489563\n",
      "Training iteration 4 | loss 0.14112110435962677\n",
      "Training | Average loss 0.13150570988655091\n",
      "Training iteration 0 | loss 0.13717906177043915\n",
      "Training iteration 1 | loss 0.13917039334774017\n",
      "Training iteration 2 | loss 0.120417520403862\n",
      "Training iteration 3 | loss 0.14274992048740387\n",
      "Training iteration 4 | loss 0.12177957594394684\n",
      "Training | Average loss 0.1322592943906784\n",
      "Training iteration 0 | loss 0.13096624612808228\n",
      "Training iteration 1 | loss 0.1339406967163086\n",
      "Training iteration 2 | loss 0.14118614792823792\n",
      "Training iteration 3 | loss 0.13013918697834015\n",
      "Training iteration 4 | loss 0.12784938514232635\n",
      "Training | Average loss 0.13281633257865905\n",
      "Training iteration 0 | loss 0.13635511696338654\n",
      "Training iteration 1 | loss 0.12993329763412476\n",
      "Training iteration 2 | loss 0.13497097790241241\n",
      "Training iteration 3 | loss 0.13250087201595306\n",
      "Training iteration 4 | loss 0.12558504939079285\n",
      "Training | Average loss 0.13186906278133392\n",
      "Training iteration 0 | loss 0.1278817057609558\n",
      "Training iteration 1 | loss 0.13227586448192596\n",
      "Training iteration 2 | loss 0.1457057148218155\n",
      "Training iteration 3 | loss 0.13251085579395294\n",
      "Training iteration 4 | loss 0.1336112767457962\n",
      "Training | Average loss 0.13439708352088928\n",
      "Training iteration 0 | loss 0.13308118283748627\n",
      "Training iteration 1 | loss 0.1361720860004425\n",
      "Training iteration 2 | loss 0.13641896843910217\n",
      "Training iteration 3 | loss 0.11889837682247162\n",
      "Training iteration 4 | loss 0.14337055385112762\n",
      "Training | Average loss 0.13358823359012603\n",
      "Training iteration 0 | loss 0.1369021236896515\n",
      "Training iteration 1 | loss 0.13220612704753876\n",
      "Training iteration 2 | loss 0.1347736269235611\n",
      "Training iteration 3 | loss 0.1483692228794098\n",
      "Training iteration 4 | loss 0.13622592389583588\n",
      "Training | Average loss 0.1376954048871994\n",
      "Training iteration 0 | loss 0.15100586414337158\n",
      "Training iteration 1 | loss 0.1256006956100464\n",
      "Training iteration 2 | loss 0.13143062591552734\n",
      "Training iteration 3 | loss 0.1341814398765564\n",
      "Training iteration 4 | loss 0.1326569765806198\n",
      "Training | Average loss 0.1349751204252243\n",
      "Training iteration 0 | loss 0.13998942077159882\n",
      "Training iteration 1 | loss 0.1292005330324173\n",
      "Training iteration 2 | loss 0.1353539526462555\n",
      "Training iteration 3 | loss 0.13882377743721008\n",
      "Training iteration 4 | loss 0.14007730782032013\n",
      "Training | Average loss 0.13668899834156037\n",
      "Training iteration 0 | loss 0.1285075545310974\n",
      "Training iteration 1 | loss 0.12884405255317688\n",
      "Training iteration 2 | loss 0.136899933218956\n",
      "Training iteration 3 | loss 0.13911086320877075\n",
      "Training iteration 4 | loss 0.12585893273353577\n",
      "Training | Average loss 0.13184426724910736\n",
      "Training iteration 0 | loss 0.13735242187976837\n",
      "Training iteration 1 | loss 0.11822135001420975\n",
      "Training iteration 2 | loss 0.1284284144639969\n",
      "Training iteration 3 | loss 0.13645406067371368\n",
      "Training iteration 4 | loss 0.14032459259033203\n",
      "Training | Average loss 0.13215616792440416\n",
      "Training iteration 0 | loss 0.12790626287460327\n",
      "Training iteration 1 | loss 0.1407405436038971\n",
      "Training iteration 2 | loss 0.1410219371318817\n",
      "Training iteration 3 | loss 0.14113576710224152\n",
      "Training iteration 4 | loss 0.10939450562000275\n",
      "Training | Average loss 0.13203980326652526\n",
      "Training iteration 0 | loss 0.11311448365449905\n",
      "Training iteration 1 | loss 0.12954626977443695\n",
      "Training iteration 2 | loss 0.1629832237958908\n",
      "Training iteration 3 | loss 0.12932449579238892\n",
      "Training iteration 4 | loss 0.1332964450120926\n",
      "Training | Average loss 0.13365298360586167\n",
      "Training iteration 0 | loss 0.1326438933610916\n",
      "Training iteration 1 | loss 0.1255578100681305\n",
      "Training iteration 2 | loss 0.1395169347524643\n",
      "Training iteration 3 | loss 0.1328994482755661\n",
      "Training iteration 4 | loss 0.1360345482826233\n",
      "Training | Average loss 0.13333052694797515\n",
      "Training iteration 0 | loss 0.1163230910897255\n",
      "Training iteration 1 | loss 0.1358342468738556\n",
      "Training iteration 2 | loss 0.13212962448596954\n",
      "Training iteration 3 | loss 0.135165274143219\n",
      "Training iteration 4 | loss 0.1393609195947647\n",
      "Training | Average loss 0.13176263123750687\n",
      "Training iteration 0 | loss 0.1401451975107193\n",
      "Training iteration 1 | loss 0.1164330318570137\n",
      "Training iteration 2 | loss 0.13094866275787354\n",
      "Training iteration 3 | loss 0.12730003893375397\n",
      "Training iteration 4 | loss 0.14962482452392578\n",
      "Training | Average loss 0.13289035111665726\n",
      "Training iteration 0 | loss 0.14274926483631134\n",
      "Training iteration 1 | loss 0.11497782915830612\n",
      "Training iteration 2 | loss 0.1324634999036789\n",
      "Training iteration 3 | loss 0.14586958289146423\n",
      "Training iteration 4 | loss 0.1215115487575531\n",
      "Training | Average loss 0.13151434510946275\n",
      "Training iteration 0 | loss 0.1334393471479416\n",
      "Training iteration 1 | loss 0.12086815387010574\n",
      "Training iteration 2 | loss 0.1339682936668396\n",
      "Training iteration 3 | loss 0.13046622276306152\n",
      "Training iteration 4 | loss 0.13816551864147186\n",
      "Training | Average loss 0.13138150721788405\n",
      "Training iteration 0 | loss 0.1508672535419464\n",
      "Training iteration 1 | loss 0.1382109820842743\n",
      "Training iteration 2 | loss 0.1296435445547104\n",
      "Training iteration 3 | loss 0.12059708684682846\n",
      "Training iteration 4 | loss 0.120907723903656\n",
      "Training | Average loss 0.13204531818628312\n",
      "Training iteration 0 | loss 0.14207300543785095\n",
      "Training iteration 1 | loss 0.13744965195655823\n",
      "Training iteration 2 | loss 0.12921898066997528\n",
      "Training iteration 3 | loss 0.12805409729480743\n",
      "Training iteration 4 | loss 0.11915922164916992\n",
      "Training | Average loss 0.13119099140167237\n",
      "Training iteration 0 | loss 0.1304759830236435\n",
      "Training iteration 1 | loss 0.1394391655921936\n",
      "Training iteration 2 | loss 0.13449859619140625\n",
      "Training iteration 3 | loss 0.12551942467689514\n",
      "Training iteration 4 | loss 0.12521696090698242\n",
      "Training | Average loss 0.13103002607822417\n",
      "Training iteration 0 | loss 0.13012053072452545\n",
      "Training iteration 1 | loss 0.13225005567073822\n",
      "Training iteration 2 | loss 0.12026751041412354\n",
      "Training iteration 3 | loss 0.14013317227363586\n",
      "Training iteration 4 | loss 0.12816748023033142\n",
      "Training | Average loss 0.1301877498626709\n",
      "Training iteration 0 | loss 0.11999793350696564\n",
      "Training iteration 1 | loss 0.12788721919059753\n",
      "Training iteration 2 | loss 0.13861964643001556\n",
      "Training iteration 3 | loss 0.1285594403743744\n",
      "Training iteration 4 | loss 0.1361941695213318\n",
      "Training | Average loss 0.130251681804657\n",
      "Training iteration 0 | loss 0.1390332132577896\n",
      "Training iteration 1 | loss 0.12091782689094543\n",
      "Training iteration 2 | loss 0.1242893636226654\n",
      "Training iteration 3 | loss 0.13654643297195435\n",
      "Training iteration 4 | loss 0.13251692056655884\n",
      "Training | Average loss 0.13066075146198272\n",
      "Training iteration 0 | loss 0.1259583830833435\n",
      "Training iteration 1 | loss 0.13206340372562408\n",
      "Training iteration 2 | loss 0.13277877867221832\n",
      "Training iteration 3 | loss 0.13561506569385529\n",
      "Training iteration 4 | loss 0.12784603238105774\n",
      "Training | Average loss 0.1308523327112198\n",
      "Training iteration 0 | loss 0.14234088361263275\n",
      "Training iteration 1 | loss 0.13546636700630188\n",
      "Training iteration 2 | loss 0.12384171038866043\n",
      "Training iteration 3 | loss 0.13222813606262207\n",
      "Training iteration 4 | loss 0.12406957894563675\n",
      "Training | Average loss 0.1315893352031708\n",
      "Training iteration 0 | loss 0.12486682832241058\n",
      "Training iteration 1 | loss 0.1281844526529312\n",
      "Training iteration 2 | loss 0.13733628392219543\n",
      "Training iteration 3 | loss 0.13449770212173462\n",
      "Training iteration 4 | loss 0.1272992342710495\n",
      "Training | Average loss 0.13043690025806426\n",
      "Training iteration 0 | loss 0.122919462621212\n",
      "Training iteration 1 | loss 0.12999962270259857\n",
      "Training iteration 2 | loss 0.12660209834575653\n",
      "Training iteration 3 | loss 0.12946853041648865\n",
      "Training iteration 4 | loss 0.1437031477689743\n",
      "Training | Average loss 0.13053857237100602\n",
      "Training iteration 0 | loss 0.12546223402023315\n",
      "Training iteration 1 | loss 0.1444244682788849\n",
      "Training iteration 2 | loss 0.1242438331246376\n",
      "Training iteration 3 | loss 0.13274335861206055\n",
      "Training iteration 4 | loss 0.1351046860218048\n",
      "Training | Average loss 0.1323957160115242\n",
      "Training iteration 0 | loss 0.12438854575157166\n",
      "Training iteration 1 | loss 0.12368094176054001\n",
      "Training iteration 2 | loss 0.14287684857845306\n",
      "Training iteration 3 | loss 0.12636511027812958\n",
      "Training iteration 4 | loss 0.13753977417945862\n",
      "Training | Average loss 0.1309702441096306\n",
      "Training iteration 0 | loss 0.13209909200668335\n",
      "Training iteration 1 | loss 0.1398734748363495\n",
      "Training iteration 2 | loss 0.13262024521827698\n",
      "Training iteration 3 | loss 0.1298026293516159\n",
      "Training iteration 4 | loss 0.12106319516897202\n",
      "Training | Average loss 0.13109172731637955\n",
      "Training iteration 0 | loss 0.13064169883728027\n",
      "Training iteration 1 | loss 0.1425808072090149\n",
      "Training iteration 2 | loss 0.1279049515724182\n",
      "Training iteration 3 | loss 0.11831466108560562\n",
      "Training iteration 4 | loss 0.13615183532238007\n",
      "Training | Average loss 0.1311187908053398\n",
      "Training iteration 0 | loss 0.1320163905620575\n",
      "Training iteration 1 | loss 0.13532327115535736\n",
      "Training iteration 2 | loss 0.12670597434043884\n",
      "Training iteration 3 | loss 0.1344037801027298\n",
      "Training iteration 4 | loss 0.1271899938583374\n",
      "Training | Average loss 0.13112788200378417\n",
      "Training iteration 0 | loss 0.12362132966518402\n",
      "Training iteration 1 | loss 0.13004697859287262\n",
      "Training iteration 2 | loss 0.1443251669406891\n",
      "Training iteration 3 | loss 0.13417302072048187\n",
      "Training iteration 4 | loss 0.12793348729610443\n",
      "Training | Average loss 0.1320199966430664\n",
      "Training iteration 0 | loss 0.121510811150074\n",
      "Training iteration 1 | loss 0.13631702959537506\n",
      "Training iteration 2 | loss 0.13656939566135406\n",
      "Training iteration 3 | loss 0.1297174096107483\n",
      "Training iteration 4 | loss 0.13384094834327698\n",
      "Training | Average loss 0.13159111887216568\n",
      "Training iteration 0 | loss 0.1339750736951828\n",
      "Training iteration 1 | loss 0.13083212077617645\n",
      "Training iteration 2 | loss 0.12947684526443481\n",
      "Training iteration 3 | loss 0.13694681227207184\n",
      "Training iteration 4 | loss 0.12340150028467178\n",
      "Training | Average loss 0.13092647045850753\n",
      "Training iteration 0 | loss 0.13820719718933105\n",
      "Training iteration 1 | loss 0.12678463757038116\n",
      "Training iteration 2 | loss 0.12006955593824387\n",
      "Training iteration 3 | loss 0.13503330945968628\n",
      "Training iteration 4 | loss 0.13590972125530243\n",
      "Training | Average loss 0.13120088428258897\n",
      "Training iteration 0 | loss 0.14319932460784912\n",
      "Training iteration 1 | loss 0.11961495876312256\n",
      "Training iteration 2 | loss 0.12261942028999329\n",
      "Training iteration 3 | loss 0.1376875340938568\n",
      "Training iteration 4 | loss 0.1287558674812317\n",
      "Training | Average loss 0.1303754210472107\n",
      "Training iteration 0 | loss 0.11288034170866013\n",
      "Training iteration 1 | loss 0.142684206366539\n",
      "Training iteration 2 | loss 0.12612059712409973\n",
      "Training iteration 3 | loss 0.13199511170387268\n",
      "Training iteration 4 | loss 0.13592420518398285\n",
      "Training | Average loss 0.12992089241743088\n",
      "Training iteration 0 | loss 0.12706425786018372\n",
      "Training iteration 1 | loss 0.12288904935121536\n",
      "Training iteration 2 | loss 0.13321273028850555\n",
      "Training iteration 3 | loss 0.12821678817272186\n",
      "Training iteration 4 | loss 0.13933555781841278\n",
      "Training | Average loss 0.13014367669820787\n",
      "Training iteration 0 | loss 0.1233721375465393\n",
      "Training iteration 1 | loss 0.12362497299909592\n",
      "Training iteration 2 | loss 0.13921493291854858\n",
      "Training iteration 3 | loss 0.12141846120357513\n",
      "Training iteration 4 | loss 0.14564435184001923\n",
      "Training | Average loss 0.13065497130155562\n",
      "Training iteration 0 | loss 0.1221781000494957\n",
      "Training iteration 1 | loss 0.13414187729358673\n",
      "Training iteration 2 | loss 0.11259030550718307\n",
      "Training iteration 3 | loss 0.13890576362609863\n",
      "Training iteration 4 | loss 0.14796897768974304\n",
      "Training | Average loss 0.13115700483322143\n",
      "Training iteration 0 | loss 0.13521909713745117\n",
      "Training iteration 1 | loss 0.11800209432840347\n",
      "Training iteration 2 | loss 0.12288135290145874\n",
      "Training iteration 3 | loss 0.12193399667739868\n",
      "Training iteration 4 | loss 0.1522231101989746\n",
      "Training | Average loss 0.13005193024873735\n",
      "Training iteration 0 | loss 0.12481517344713211\n",
      "Training iteration 1 | loss 0.14672143757343292\n",
      "Training iteration 2 | loss 0.1257951706647873\n",
      "Training iteration 3 | loss 0.1372441053390503\n",
      "Training iteration 4 | loss 0.11611464619636536\n",
      "Training | Average loss 0.1301381066441536\n",
      "Training iteration 0 | loss 0.14369530975818634\n",
      "Training iteration 1 | loss 0.12017335742712021\n",
      "Training iteration 2 | loss 0.13002626597881317\n",
      "Training iteration 3 | loss 0.12937690317630768\n",
      "Training iteration 4 | loss 0.1283033937215805\n",
      "Training | Average loss 0.13031504601240157\n",
      "Training iteration 0 | loss 0.13329583406448364\n",
      "Training iteration 1 | loss 0.1328010857105255\n",
      "Training iteration 2 | loss 0.1292688101530075\n",
      "Training iteration 3 | loss 0.1210816502571106\n",
      "Training iteration 4 | loss 0.1412385106086731\n",
      "Training | Average loss 0.13153717815876007\n",
      "Training iteration 0 | loss 0.1358482837677002\n",
      "Training iteration 1 | loss 0.12181182205677032\n",
      "Training iteration 2 | loss 0.12102645635604858\n",
      "Training iteration 3 | loss 0.13771051168441772\n",
      "Training iteration 4 | loss 0.13851450383663177\n",
      "Training | Average loss 0.13098231554031373\n",
      "Training iteration 0 | loss 0.1385207325220108\n",
      "Training iteration 1 | loss 0.13955166935920715\n",
      "Training iteration 2 | loss 0.1206568256020546\n",
      "Training iteration 3 | loss 0.12209609150886536\n",
      "Training iteration 4 | loss 0.13194522261619568\n",
      "Training | Average loss 0.13055410832166672\n",
      "Training iteration 0 | loss 0.12248288840055466\n",
      "Training iteration 1 | loss 0.13967619836330414\n",
      "Training iteration 2 | loss 0.12766097486019135\n",
      "Training iteration 3 | loss 0.1301332414150238\n",
      "Training iteration 4 | loss 0.13695232570171356\n",
      "Training | Average loss 0.1313811257481575\n",
      "Training iteration 0 | loss 0.12179416418075562\n",
      "Training iteration 1 | loss 0.13335487246513367\n",
      "Training iteration 2 | loss 0.13195958733558655\n",
      "Training iteration 3 | loss 0.13794848322868347\n",
      "Training iteration 4 | loss 0.12618021667003632\n",
      "Training | Average loss 0.13024746477603913\n",
      "Training iteration 0 | loss 0.13017849624156952\n",
      "Training iteration 1 | loss 0.13090363144874573\n",
      "Training iteration 2 | loss 0.12570546567440033\n",
      "Training iteration 3 | loss 0.13018956780433655\n",
      "Training iteration 4 | loss 0.13407151401042938\n",
      "Training | Average loss 0.1302097350358963\n",
      "Training iteration 0 | loss 0.13002066314220428\n",
      "Training iteration 1 | loss 0.13618837296962738\n",
      "Training iteration 2 | loss 0.13653357326984406\n",
      "Training iteration 3 | loss 0.12378527969121933\n",
      "Training iteration 4 | loss 0.12530088424682617\n",
      "Training | Average loss 0.13036575466394423\n",
      "Training iteration 0 | loss 0.1170361191034317\n",
      "Training iteration 1 | loss 0.13034671545028687\n",
      "Training iteration 2 | loss 0.13014942407608032\n",
      "Training iteration 3 | loss 0.1340971291065216\n",
      "Training iteration 4 | loss 0.1406332403421402\n",
      "Training | Average loss 0.13045252561569215\n",
      "Training iteration 0 | loss 0.13645829260349274\n",
      "Training iteration 1 | loss 0.12742841243743896\n",
      "Training iteration 2 | loss 0.12304998189210892\n",
      "Training iteration 3 | loss 0.13411878049373627\n",
      "Training iteration 4 | loss 0.13612596690654755\n",
      "Training | Average loss 0.1314362868666649\n",
      "Training iteration 0 | loss 0.13248541951179504\n",
      "Training iteration 1 | loss 0.12525132298469543\n",
      "Training iteration 2 | loss 0.1404886245727539\n",
      "Training iteration 3 | loss 0.12314629554748535\n",
      "Training iteration 4 | loss 0.13119658827781677\n",
      "Training | Average loss 0.1305136501789093\n",
      "Training iteration 0 | loss 0.14092302322387695\n",
      "Training iteration 1 | loss 0.1332118958234787\n",
      "Training iteration 2 | loss 0.12892387807369232\n",
      "Training iteration 3 | loss 0.12596464157104492\n",
      "Training iteration 4 | loss 0.12231496721506119\n",
      "Training | Average loss 0.1302676811814308\n",
      "Training iteration 0 | loss 0.12334234267473221\n",
      "Training iteration 1 | loss 0.13555791974067688\n",
      "Training iteration 2 | loss 0.13012737035751343\n",
      "Training iteration 3 | loss 0.13788698613643646\n",
      "Training iteration 4 | loss 0.12759988009929657\n",
      "Training | Average loss 0.13090289980173112\n",
      "Training iteration 0 | loss 0.13508382439613342\n",
      "Training iteration 1 | loss 0.13638003170490265\n",
      "Training iteration 2 | loss 0.12780481576919556\n",
      "Training iteration 3 | loss 0.13025344908237457\n",
      "Training iteration 4 | loss 0.12194813787937164\n",
      "Training | Average loss 0.13029405176639558\n",
      "Training iteration 0 | loss 0.1281341314315796\n",
      "Training iteration 1 | loss 0.13646350800991058\n",
      "Training iteration 2 | loss 0.1234496533870697\n",
      "Training iteration 3 | loss 0.12428012490272522\n",
      "Training iteration 4 | loss 0.14058661460876465\n",
      "Training | Average loss 0.13058280646800996\n",
      "Training iteration 0 | loss 0.126599982380867\n",
      "Training iteration 1 | loss 0.11862156540155411\n",
      "Training iteration 2 | loss 0.14407876133918762\n",
      "Training iteration 3 | loss 0.13036943972110748\n",
      "Training iteration 4 | loss 0.13283173739910126\n",
      "Training | Average loss 0.13050029724836348\n",
      "Training iteration 0 | loss 0.13188289105892181\n",
      "Training iteration 1 | loss 0.13830448687076569\n",
      "Training iteration 2 | loss 0.1334128975868225\n",
      "Training iteration 3 | loss 0.11980216950178146\n",
      "Training iteration 4 | loss 0.13217701017856598\n",
      "Training | Average loss 0.1311158910393715\n",
      "Training iteration 0 | loss 0.11971485614776611\n",
      "Training iteration 1 | loss 0.13288232684135437\n",
      "Training iteration 2 | loss 0.13995030522346497\n",
      "Training iteration 3 | loss 0.13268843293190002\n",
      "Training iteration 4 | loss 0.124873585999012\n",
      "Training | Average loss 0.1300219014286995\n",
      "Training iteration 0 | loss 0.13522738218307495\n",
      "Training iteration 1 | loss 0.12086670845746994\n",
      "Training iteration 2 | loss 0.12233247607946396\n",
      "Training iteration 3 | loss 0.14748206734657288\n",
      "Training iteration 4 | loss 0.12482931464910507\n",
      "Training | Average loss 0.13014758974313737\n",
      "Training iteration 0 | loss 0.1263534277677536\n",
      "Training iteration 1 | loss 0.12856106460094452\n",
      "Training iteration 2 | loss 0.13775427639484406\n",
      "Training iteration 3 | loss 0.12768830358982086\n",
      "Training iteration 4 | loss 0.1315830647945404\n",
      "Training | Average loss 0.1303880274295807\n",
      "Training iteration 0 | loss 0.13235613703727722\n",
      "Training iteration 1 | loss 0.12153733521699905\n",
      "Training iteration 2 | loss 0.12108588218688965\n",
      "Training iteration 3 | loss 0.13438139855861664\n",
      "Training iteration 4 | loss 0.14179368317127228\n",
      "Training | Average loss 0.13023088723421097\n",
      "Training iteration 0 | loss 0.1332230567932129\n",
      "Training iteration 1 | loss 0.12024836987257004\n",
      "Training iteration 2 | loss 0.13058264553546906\n",
      "Training iteration 3 | loss 0.1375260204076767\n",
      "Training iteration 4 | loss 0.13255180418491364\n",
      "Training | Average loss 0.13082637935876845\n",
      "Training iteration 0 | loss 0.13488972187042236\n",
      "Training iteration 1 | loss 0.135855570435524\n",
      "Training iteration 2 | loss 0.12505140900611877\n",
      "Training iteration 3 | loss 0.13035888969898224\n",
      "Training iteration 4 | loss 0.13090404868125916\n",
      "Training | Average loss 0.1314119279384613\n",
      "Training iteration 0 | loss 0.12713623046875\n",
      "Training iteration 1 | loss 0.1329159140586853\n",
      "Training iteration 2 | loss 0.12134616076946259\n",
      "Training iteration 3 | loss 0.13096635043621063\n",
      "Training iteration 4 | loss 0.13851971924304962\n",
      "Training | Average loss 0.13017687499523162\n",
      "Training iteration 0 | loss 0.14012902975082397\n",
      "Training iteration 1 | loss 0.12197236716747284\n",
      "Training iteration 2 | loss 0.1433519870042801\n",
      "Training iteration 3 | loss 0.11712280660867691\n",
      "Training iteration 4 | loss 0.12950488924980164\n",
      "Training | Average loss 0.13041621595621108\n",
      "Training iteration 0 | loss 0.11997276544570923\n",
      "Training iteration 1 | loss 0.13136783242225647\n",
      "Training iteration 2 | loss 0.13349168002605438\n",
      "Training iteration 3 | loss 0.13081730902194977\n",
      "Training iteration 4 | loss 0.13799968361854553\n",
      "Training | Average loss 0.13072985410690308\n",
      "Training iteration 0 | loss 0.1344268023967743\n",
      "Training iteration 1 | loss 0.12427252531051636\n",
      "Training iteration 2 | loss 0.1437063217163086\n",
      "Training iteration 3 | loss 0.12141259014606476\n",
      "Training iteration 4 | loss 0.12535220384597778\n",
      "Training | Average loss 0.12983408868312835\n",
      "Training iteration 0 | loss 0.13538038730621338\n",
      "Training iteration 1 | loss 0.12663587927818298\n",
      "Training iteration 2 | loss 0.1341533660888672\n",
      "Training iteration 3 | loss 0.13598354160785675\n",
      "Training iteration 4 | loss 0.11592880636453629\n",
      "Training | Average loss 0.1296163961291313\n",
      "Training iteration 0 | loss 0.1340368539094925\n",
      "Training iteration 1 | loss 0.1317407488822937\n",
      "Training iteration 2 | loss 0.11662459373474121\n",
      "Training iteration 3 | loss 0.133782297372818\n",
      "Training iteration 4 | loss 0.1346975713968277\n",
      "Training | Average loss 0.1301764130592346\n",
      "Training iteration 0 | loss 0.13209515810012817\n",
      "Training iteration 1 | loss 0.13000649213790894\n",
      "Training iteration 2 | loss 0.11512619256973267\n",
      "Training iteration 3 | loss 0.13873893022537231\n",
      "Training iteration 4 | loss 0.13560546934604645\n",
      "Training | Average loss 0.1303144484758377\n",
      "Training iteration 0 | loss 0.13941334187984467\n",
      "Training iteration 1 | loss 0.12940764427185059\n",
      "Training iteration 2 | loss 0.13460591435432434\n",
      "Training iteration 3 | loss 0.12756668031215668\n",
      "Training iteration 4 | loss 0.12173423916101456\n",
      "Training | Average loss 0.13054556399583817\n",
      "Training iteration 0 | loss 0.14558373391628265\n",
      "Training iteration 1 | loss 0.14700719714164734\n",
      "Training iteration 2 | loss 0.12765321135520935\n",
      "Training iteration 3 | loss 0.10950469225645065\n",
      "Training iteration 4 | loss 0.12736058235168457\n",
      "Training | Average loss 0.13142188340425492\n",
      "Training iteration 0 | loss 0.12613117694854736\n",
      "Training iteration 1 | loss 0.13471347093582153\n",
      "Training iteration 2 | loss 0.1268722265958786\n",
      "Training iteration 3 | loss 0.12466835975646973\n",
      "Training iteration 4 | loss 0.14179299771785736\n",
      "Training | Average loss 0.1308356463909149\n",
      "Training iteration 0 | loss 0.12770259380340576\n",
      "Training iteration 1 | loss 0.14115679264068604\n",
      "Training iteration 2 | loss 0.12832212448120117\n",
      "Training iteration 3 | loss 0.13134555518627167\n",
      "Training iteration 4 | loss 0.12772327661514282\n",
      "Training | Average loss 0.1312500685453415\n",
      "Training iteration 0 | loss 0.13149815797805786\n",
      "Training iteration 1 | loss 0.12858077883720398\n",
      "Training iteration 2 | loss 0.12311887741088867\n",
      "Training iteration 3 | loss 0.12940613925457\n",
      "Training iteration 4 | loss 0.13502207398414612\n",
      "Training | Average loss 0.12952520549297333\n",
      "Training iteration 0 | loss 0.13443240523338318\n",
      "Training iteration 1 | loss 0.12685126066207886\n",
      "Training iteration 2 | loss 0.12694518268108368\n",
      "Training iteration 3 | loss 0.1278955489397049\n",
      "Training iteration 4 | loss 0.14064304530620575\n",
      "Training | Average loss 0.13135348856449128\n",
      "Training iteration 0 | loss 0.12393561750650406\n",
      "Training iteration 1 | loss 0.15709637105464935\n",
      "Training iteration 2 | loss 0.12519831955432892\n",
      "Training iteration 3 | loss 0.12509796023368835\n",
      "Training iteration 4 | loss 0.12415404617786407\n",
      "Training | Average loss 0.13109646290540694\n",
      "Training iteration 0 | loss 0.14008179306983948\n",
      "Training iteration 1 | loss 0.14284901320934296\n",
      "Training iteration 2 | loss 0.10829902440309525\n",
      "Training iteration 3 | loss 0.12157108634710312\n",
      "Training iteration 4 | loss 0.13557937741279602\n",
      "Training | Average loss 0.12967605888843536\n",
      "Training iteration 0 | loss 0.12802045047283173\n",
      "Training iteration 1 | loss 0.1339350789785385\n",
      "Training iteration 2 | loss 0.1277768760919571\n",
      "Training iteration 3 | loss 0.1302575170993805\n",
      "Training iteration 4 | loss 0.1282825917005539\n",
      "Training | Average loss 0.12965450286865235\n",
      "Training iteration 0 | loss 0.14607734978199005\n",
      "Training iteration 1 | loss 0.12833945453166962\n",
      "Training iteration 2 | loss 0.12251345068216324\n",
      "Training iteration 3 | loss 0.12761348485946655\n",
      "Training iteration 4 | loss 0.12175675481557846\n",
      "Training | Average loss 0.1292600989341736\n",
      "Training iteration 0 | loss 0.1221577450633049\n",
      "Training iteration 1 | loss 0.13911223411560059\n",
      "Training iteration 2 | loss 0.11898503452539444\n",
      "Training iteration 3 | loss 0.13149674236774445\n",
      "Training iteration 4 | loss 0.1349523961544037\n",
      "Training | Average loss 0.1293408304452896\n",
      "Training iteration 0 | loss 0.13140325248241425\n",
      "Training iteration 1 | loss 0.12237057834863663\n",
      "Training iteration 2 | loss 0.12159131467342377\n",
      "Training iteration 3 | loss 0.12366389483213425\n",
      "Training iteration 4 | loss 0.15053361654281616\n",
      "Training | Average loss 0.12991253137588502\n",
      "Training iteration 0 | loss 0.12488775700330734\n",
      "Training iteration 1 | loss 0.13207174837589264\n",
      "Training iteration 2 | loss 0.12504316866397858\n",
      "Training iteration 3 | loss 0.13491344451904297\n",
      "Training iteration 4 | loss 0.13022981584072113\n",
      "Training | Average loss 0.12942918688058852\n",
      "Training iteration 0 | loss 0.12000556290149689\n",
      "Training iteration 1 | loss 0.12728342413902283\n",
      "Training iteration 2 | loss 0.14148667454719543\n",
      "Training iteration 3 | loss 0.11796393245458603\n",
      "Training iteration 4 | loss 0.13896307349205017\n",
      "Training | Average loss 0.12914053350687027\n",
      "Training iteration 0 | loss 0.13052114844322205\n",
      "Training iteration 1 | loss 0.13683800399303436\n",
      "Training iteration 2 | loss 0.13539236783981323\n",
      "Training iteration 3 | loss 0.12674640119075775\n",
      "Training iteration 4 | loss 0.12094303965568542\n",
      "Training | Average loss 0.13008819222450257\n",
      "Training iteration 0 | loss 0.13297508656978607\n",
      "Training iteration 1 | loss 0.11692512780427933\n",
      "Training iteration 2 | loss 0.12472168356180191\n",
      "Training iteration 3 | loss 0.1311514973640442\n",
      "Training iteration 4 | loss 0.146088108420372\n",
      "Training | Average loss 0.1303723007440567\n",
      "Training iteration 0 | loss 0.12802734971046448\n",
      "Training iteration 1 | loss 0.1340673565864563\n",
      "Training iteration 2 | loss 0.13963203132152557\n",
      "Training iteration 3 | loss 0.1262061446905136\n",
      "Training iteration 4 | loss 0.12628939747810364\n",
      "Training | Average loss 0.13084445595741273\n",
      "Training iteration 0 | loss 0.14195743203163147\n",
      "Training iteration 1 | loss 0.11983199417591095\n",
      "Training iteration 2 | loss 0.12775136530399323\n",
      "Training iteration 3 | loss 0.138872429728508\n",
      "Training iteration 4 | loss 0.12061922997236252\n",
      "Training | Average loss 0.12980649024248123\n",
      "Training iteration 0 | loss 0.1286868453025818\n",
      "Training iteration 1 | loss 0.13929960131645203\n",
      "Training iteration 2 | loss 0.12655852735042572\n",
      "Training iteration 3 | loss 0.12604083120822906\n",
      "Training iteration 4 | loss 0.1341698169708252\n",
      "Training | Average loss 0.13095112442970275\n",
      "Training iteration 0 | loss 0.1340193897485733\n",
      "Training iteration 1 | loss 0.12542977929115295\n",
      "Training iteration 2 | loss 0.136681467294693\n",
      "Training iteration 3 | loss 0.13110846281051636\n",
      "Training iteration 4 | loss 0.12931641936302185\n",
      "Training | Average loss 0.1313111037015915\n",
      "Training iteration 0 | loss 0.12234187871217728\n",
      "Training iteration 1 | loss 0.12522242963314056\n",
      "Training iteration 2 | loss 0.13154807686805725\n",
      "Training iteration 3 | loss 0.1306220144033432\n",
      "Training iteration 4 | loss 0.14453032612800598\n",
      "Training | Average loss 0.13085294514894485\n",
      "Training iteration 0 | loss 0.12226559966802597\n",
      "Training iteration 1 | loss 0.12362060695886612\n",
      "Training iteration 2 | loss 0.141651913523674\n",
      "Training iteration 3 | loss 0.13225723803043365\n",
      "Training iteration 4 | loss 0.12905628979206085\n",
      "Training | Average loss 0.12977032959461213\n",
      "Training iteration 0 | loss 0.12756723165512085\n",
      "Training iteration 1 | loss 0.12975655496120453\n",
      "Training iteration 2 | loss 0.11845380812883377\n",
      "Training iteration 3 | loss 0.13268092274665833\n",
      "Training iteration 4 | loss 0.1381600946187973\n",
      "Training | Average loss 0.12932372242212295\n",
      "Training iteration 0 | loss 0.12221986055374146\n",
      "Training iteration 1 | loss 0.12908031046390533\n",
      "Training iteration 2 | loss 0.1366693675518036\n",
      "Training iteration 3 | loss 0.12553632259368896\n",
      "Training iteration 4 | loss 0.13574548065662384\n",
      "Training | Average loss 0.12985026836395264\n",
      "Training iteration 0 | loss 0.1312280297279358\n",
      "Training iteration 1 | loss 0.13021884858608246\n",
      "Training iteration 2 | loss 0.1387205719947815\n",
      "Training iteration 3 | loss 0.1284104585647583\n",
      "Training iteration 4 | loss 0.12782642245292664\n",
      "Training | Average loss 0.13128086626529695\n",
      "Training iteration 0 | loss 0.12557125091552734\n",
      "Training iteration 1 | loss 0.13819584250450134\n",
      "Training iteration 2 | loss 0.12575875222682953\n",
      "Training iteration 3 | loss 0.12949097156524658\n",
      "Training iteration 4 | loss 0.13737773895263672\n",
      "Training | Average loss 0.1312789112329483\n",
      "Training iteration 0 | loss 0.1398012936115265\n",
      "Training iteration 1 | loss 0.13043351471424103\n",
      "Training iteration 2 | loss 0.14530731737613678\n",
      "Training iteration 3 | loss 0.1127074658870697\n",
      "Training iteration 4 | loss 0.12117009609937668\n",
      "Training | Average loss 0.12988393753767014\n",
      "Training iteration 0 | loss 0.13476620614528656\n",
      "Training iteration 1 | loss 0.1304081827402115\n",
      "Training iteration 2 | loss 0.12228737026453018\n",
      "Training iteration 3 | loss 0.1328359991312027\n",
      "Training iteration 4 | loss 0.1283111572265625\n",
      "Training | Average loss 0.12972178310155869\n",
      "Training iteration 0 | loss 0.13955001533031464\n",
      "Training iteration 1 | loss 0.13381615281105042\n",
      "Training iteration 2 | loss 0.12822325527668\n",
      "Training iteration 3 | loss 0.12704217433929443\n",
      "Training iteration 4 | loss 0.12368699163198471\n",
      "Training | Average loss 0.13046371787786484\n",
      "Training iteration 0 | loss 0.14008116722106934\n",
      "Training iteration 1 | loss 0.13444474339485168\n",
      "Training iteration 2 | loss 0.1365075409412384\n",
      "Training iteration 3 | loss 0.12447331100702286\n",
      "Training iteration 4 | loss 0.12369630485773087\n",
      "Training | Average loss 0.13184061348438264\n",
      "Training iteration 0 | loss 0.13466507196426392\n",
      "Training iteration 1 | loss 0.1456827074289322\n",
      "Training iteration 2 | loss 0.1149669662117958\n",
      "Training iteration 3 | loss 0.12287463247776031\n",
      "Training iteration 4 | loss 0.14231041073799133\n",
      "Training | Average loss 0.1320999577641487\n",
      "Training iteration 0 | loss 0.12770281732082367\n",
      "Training iteration 1 | loss 0.14766627550125122\n",
      "Training iteration 2 | loss 0.12628525495529175\n",
      "Training iteration 3 | loss 0.13006386160850525\n",
      "Training iteration 4 | loss 0.15488053858280182\n",
      "Training | Average loss 0.13731974959373475\n",
      "Training iteration 0 | loss 0.14368456602096558\n",
      "Training iteration 1 | loss 0.13207417726516724\n",
      "Training iteration 2 | loss 0.127232626080513\n",
      "Training iteration 3 | loss 0.1439274251461029\n",
      "Training iteration 4 | loss 0.11873597651720047\n",
      "Training | Average loss 0.13313095420598983\n",
      "Training iteration 0 | loss 0.1192011833190918\n",
      "Training iteration 1 | loss 0.1493152529001236\n",
      "Training iteration 2 | loss 0.1293400526046753\n",
      "Training iteration 3 | loss 0.13484954833984375\n",
      "Training iteration 4 | loss 0.13506582379341125\n",
      "Training | Average loss 0.13355437219142913\n",
      "Training iteration 0 | loss 0.1284385323524475\n",
      "Training iteration 1 | loss 0.131463885307312\n",
      "Training iteration 2 | loss 0.12709303200244904\n",
      "Training iteration 3 | loss 0.14640572667121887\n",
      "Training iteration 4 | loss 0.12883992493152618\n",
      "Training | Average loss 0.13244822025299072\n",
      "Training iteration 0 | loss 0.13278478384017944\n",
      "Training iteration 1 | loss 0.13554847240447998\n",
      "Training iteration 2 | loss 0.12215181440114975\n",
      "Training iteration 3 | loss 0.12397443503141403\n",
      "Training iteration 4 | loss 0.13514722883701324\n",
      "Training | Average loss 0.1299213469028473\n",
      "Training iteration 0 | loss 0.1282215565443039\n",
      "Training iteration 1 | loss 0.13023912906646729\n",
      "Training iteration 2 | loss 0.13623037934303284\n",
      "Training iteration 3 | loss 0.1329827457666397\n",
      "Training iteration 4 | loss 0.12900687754154205\n",
      "Training | Average loss 0.13133613765239716\n",
      "Training iteration 0 | loss 0.13922682404518127\n",
      "Training iteration 1 | loss 0.11697126179933548\n",
      "Training iteration 2 | loss 0.12361768633127213\n",
      "Training iteration 3 | loss 0.11948943138122559\n",
      "Training iteration 4 | loss 0.16304130852222443\n",
      "Training | Average loss 0.13246930241584778\n",
      "Training iteration 0 | loss 0.13637329638004303\n",
      "Training iteration 1 | loss 0.13343636691570282\n",
      "Training iteration 2 | loss 0.1212620958685875\n",
      "Training iteration 3 | loss 0.1292949765920639\n",
      "Training iteration 4 | loss 0.1263069361448288\n",
      "Training | Average loss 0.1293347343802452\n",
      "Training iteration 0 | loss 0.1327325701713562\n",
      "Training iteration 1 | loss 0.13480998575687408\n",
      "Training iteration 2 | loss 0.13007214665412903\n",
      "Training iteration 3 | loss 0.1304374486207962\n",
      "Training iteration 4 | loss 0.1244177296757698\n",
      "Training | Average loss 0.13049397617578506\n",
      "Training iteration 0 | loss 0.129835307598114\n",
      "Training iteration 1 | loss 0.1292026937007904\n",
      "Training iteration 2 | loss 0.13698530197143555\n",
      "Training iteration 3 | loss 0.12213844060897827\n",
      "Training iteration 4 | loss 0.13360027968883514\n",
      "Training | Average loss 0.1303524047136307\n",
      "Training iteration 0 | loss 0.13276618719100952\n",
      "Training iteration 1 | loss 0.12524792551994324\n",
      "Training iteration 2 | loss 0.12944702804088593\n",
      "Training iteration 3 | loss 0.1292824000120163\n",
      "Training iteration 4 | loss 0.13123857975006104\n",
      "Training | Average loss 0.12959642410278321\n",
      "Training iteration 0 | loss 0.13490936160087585\n",
      "Training iteration 1 | loss 0.12359505891799927\n",
      "Training iteration 2 | loss 0.13399085402488708\n",
      "Training iteration 3 | loss 0.12304089218378067\n",
      "Training iteration 4 | loss 0.1346159279346466\n",
      "Training | Average loss 0.13003041893243789\n",
      "Training iteration 0 | loss 0.11737770587205887\n",
      "Training iteration 1 | loss 0.13950634002685547\n",
      "Training iteration 2 | loss 0.12100990861654282\n",
      "Training iteration 3 | loss 0.14108052849769592\n",
      "Training iteration 4 | loss 0.12739890813827515\n",
      "Training | Average loss 0.12927467823028566\n",
      "Training iteration 0 | loss 0.12404832243919373\n",
      "Training iteration 1 | loss 0.14566609263420105\n",
      "Training iteration 2 | loss 0.12168344855308533\n",
      "Training iteration 3 | loss 0.13580270111560822\n",
      "Training iteration 4 | loss 0.12116359919309616\n",
      "Training | Average loss 0.1296728327870369\n",
      "Training iteration 0 | loss 0.12105393409729004\n",
      "Training iteration 1 | loss 0.1353444904088974\n",
      "Training iteration 2 | loss 0.12064285576343536\n",
      "Training iteration 3 | loss 0.14813336730003357\n",
      "Training iteration 4 | loss 0.1258571445941925\n",
      "Training | Average loss 0.1302063584327698\n",
      "Training iteration 0 | loss 0.13836419582366943\n",
      "Training iteration 1 | loss 0.1302880197763443\n",
      "Training iteration 2 | loss 0.1329866349697113\n",
      "Training iteration 3 | loss 0.13163354992866516\n",
      "Training iteration 4 | loss 0.12153886258602142\n",
      "Training | Average loss 0.13096225261688232\n",
      "Training iteration 0 | loss 0.14161108434200287\n",
      "Training iteration 1 | loss 0.1365998089313507\n",
      "Training iteration 2 | loss 0.12658436596393585\n",
      "Training iteration 3 | loss 0.11044815182685852\n",
      "Training iteration 4 | loss 0.13990801572799683\n",
      "Training | Average loss 0.13103028535842895\n",
      "Training iteration 0 | loss 0.11831895262002945\n",
      "Training iteration 1 | loss 0.1299552321434021\n",
      "Training iteration 2 | loss 0.13712219893932343\n",
      "Training iteration 3 | loss 0.12659117579460144\n",
      "Training iteration 4 | loss 0.14877058565616608\n",
      "Training | Average loss 0.1321516290307045\n",
      "Training iteration 0 | loss 0.11860927939414978\n",
      "Training iteration 1 | loss 0.13088871538639069\n",
      "Training iteration 2 | loss 0.13350380957126617\n",
      "Training iteration 3 | loss 0.13448165357112885\n",
      "Training iteration 4 | loss 0.13945886492729187\n",
      "Training | Average loss 0.13138846457004547\n",
      "Training iteration 0 | loss 0.11902085691690445\n",
      "Training iteration 1 | loss 0.13899168372154236\n",
      "Training iteration 2 | loss 0.13122138381004333\n",
      "Training iteration 3 | loss 0.13108423352241516\n",
      "Training iteration 4 | loss 0.12511397898197174\n",
      "Training | Average loss 0.1290864273905754\n",
      "Training iteration 0 | loss 0.1256083846092224\n",
      "Training iteration 1 | loss 0.12923181056976318\n",
      "Training iteration 2 | loss 0.11864849925041199\n",
      "Training iteration 3 | loss 0.1458868384361267\n",
      "Training iteration 4 | loss 0.13519012928009033\n",
      "Training | Average loss 0.13091313242912292\n",
      "Training iteration 0 | loss 0.12322939932346344\n",
      "Training iteration 1 | loss 0.14167480170726776\n",
      "Training iteration 2 | loss 0.12722422182559967\n",
      "Training iteration 3 | loss 0.1253315806388855\n",
      "Training iteration 4 | loss 0.12775492668151855\n",
      "Training | Average loss 0.12904298603534697\n",
      "Training iteration 0 | loss 0.11941523104906082\n",
      "Training iteration 1 | loss 0.13461516797542572\n",
      "Training iteration 2 | loss 0.13110578060150146\n",
      "Training iteration 3 | loss 0.13483740389347076\n",
      "Training iteration 4 | loss 0.1303592473268509\n",
      "Training | Average loss 0.13006656616926193\n",
      "Training iteration 0 | loss 0.12585420906543732\n",
      "Training iteration 1 | loss 0.1314631849527359\n",
      "Training iteration 2 | loss 0.15025687217712402\n",
      "Training iteration 3 | loss 0.11500930786132812\n",
      "Training iteration 4 | loss 0.12936097383499146\n",
      "Training | Average loss 0.13038890957832336\n",
      "Training iteration 0 | loss 0.12976282835006714\n",
      "Training iteration 1 | loss 0.1312503218650818\n",
      "Training iteration 2 | loss 0.12225787341594696\n",
      "Training iteration 3 | loss 0.12815791368484497\n",
      "Training iteration 4 | loss 0.1357293426990509\n",
      "Training | Average loss 0.12943165600299836\n",
      "Training iteration 0 | loss 0.12143704295158386\n",
      "Training iteration 1 | loss 0.1349235624074936\n",
      "Training iteration 2 | loss 0.11672989279031754\n",
      "Training iteration 3 | loss 0.13553114235401154\n",
      "Training iteration 4 | loss 0.13560959696769714\n",
      "Training | Average loss 0.12884624749422074\n",
      "Training iteration 0 | loss 0.1492367535829544\n",
      "Training iteration 1 | loss 0.11664111912250519\n",
      "Training iteration 2 | loss 0.1266668438911438\n",
      "Training iteration 3 | loss 0.12887635827064514\n",
      "Training iteration 4 | loss 0.12227017432451248\n",
      "Training | Average loss 0.1287382498383522\n",
      "Training iteration 0 | loss 0.1331794708967209\n",
      "Training iteration 1 | loss 0.12778988480567932\n",
      "Training iteration 2 | loss 0.1310393214225769\n",
      "Training iteration 3 | loss 0.12487940490245819\n",
      "Training iteration 4 | loss 0.12854591012001038\n",
      "Training | Average loss 0.12908679842948914\n",
      "Training iteration 0 | loss 0.12212719023227692\n",
      "Training iteration 1 | loss 0.13039341568946838\n",
      "Training iteration 2 | loss 0.1328408569097519\n",
      "Training iteration 3 | loss 0.1267658770084381\n",
      "Training iteration 4 | loss 0.13185971975326538\n",
      "Training | Average loss 0.12879741191864014\n",
      "Training iteration 0 | loss 0.13085077702999115\n",
      "Training iteration 1 | loss 0.1258118748664856\n",
      "Training iteration 2 | loss 0.1249704584479332\n",
      "Training iteration 3 | loss 0.12100664526224136\n",
      "Training iteration 4 | loss 0.14588868618011475\n",
      "Training | Average loss 0.12970568835735322\n",
      "Training iteration 0 | loss 0.1332055926322937\n",
      "Training iteration 1 | loss 0.12388867139816284\n",
      "Training iteration 2 | loss 0.12157750874757767\n",
      "Training iteration 3 | loss 0.13545119762420654\n",
      "Training iteration 4 | loss 0.13651779294013977\n",
      "Training | Average loss 0.1301281526684761\n",
      "Training iteration 0 | loss 0.11386209726333618\n",
      "Training iteration 1 | loss 0.12366966903209686\n",
      "Training iteration 2 | loss 0.14218418300151825\n",
      "Training iteration 3 | loss 0.1402820199728012\n",
      "Training iteration 4 | loss 0.12695227563381195\n",
      "Training | Average loss 0.12939004898071288\n",
      "Training iteration 0 | loss 0.13409169018268585\n",
      "Training iteration 1 | loss 0.13180214166641235\n",
      "Training iteration 2 | loss 0.12107132375240326\n",
      "Training iteration 3 | loss 0.12625910341739655\n",
      "Training iteration 4 | loss 0.13623736798763275\n",
      "Training | Average loss 0.12989232540130616\n",
      "Training iteration 0 | loss 0.12518557906150818\n",
      "Training iteration 1 | loss 0.12754441797733307\n",
      "Training iteration 2 | loss 0.12941844761371613\n",
      "Training iteration 3 | loss 0.14250794053077698\n",
      "Training iteration 4 | loss 0.12451337277889252\n",
      "Training | Average loss 0.12983395159244537\n",
      "Training iteration 0 | loss 0.12280949205160141\n",
      "Training iteration 1 | loss 0.1381836086511612\n",
      "Training iteration 2 | loss 0.1272820234298706\n",
      "Training iteration 3 | loss 0.13337473571300507\n",
      "Training iteration 4 | loss 0.1287187933921814\n",
      "Training | Average loss 0.13007373064756395\n",
      "Training iteration 0 | loss 0.13962027430534363\n",
      "Training iteration 1 | loss 0.13717485964298248\n",
      "Training iteration 2 | loss 0.12445726245641708\n",
      "Training iteration 3 | loss 0.11917582154273987\n",
      "Training iteration 4 | loss 0.13282375037670135\n",
      "Training | Average loss 0.13065039366483688\n",
      "Training iteration 0 | loss 0.12058421224355698\n",
      "Training iteration 1 | loss 0.12701521813869476\n",
      "Training iteration 2 | loss 0.12225218117237091\n",
      "Training iteration 3 | loss 0.13775135576725006\n",
      "Training iteration 4 | loss 0.13819193840026855\n",
      "Training | Average loss 0.12915898114442825\n",
      "Training iteration 0 | loss 0.13241976499557495\n",
      "Training iteration 1 | loss 0.12849631905555725\n",
      "Training iteration 2 | loss 0.12489083409309387\n",
      "Training iteration 3 | loss 0.13165470957756042\n",
      "Training iteration 4 | loss 0.12658388912677765\n",
      "Training | Average loss 0.12880910336971282\n",
      "Training iteration 0 | loss 0.1415073722600937\n",
      "Training iteration 1 | loss 0.12620148062705994\n",
      "Training iteration 2 | loss 0.12724605202674866\n",
      "Training iteration 3 | loss 0.12431325018405914\n",
      "Training iteration 4 | loss 0.12826038897037506\n",
      "Training | Average loss 0.1295057088136673\n",
      "Training iteration 0 | loss 0.12928113341331482\n",
      "Training iteration 1 | loss 0.1320098340511322\n",
      "Training iteration 2 | loss 0.12778043746948242\n",
      "Training iteration 3 | loss 0.12640048563480377\n",
      "Training iteration 4 | loss 0.13105233013629913\n",
      "Training | Average loss 0.12930484414100646\n",
      "Training iteration 0 | loss 0.12847039103507996\n",
      "Training iteration 1 | loss 0.13063493371009827\n",
      "Training iteration 2 | loss 0.12227246165275574\n",
      "Training iteration 3 | loss 0.1350627988576889\n",
      "Training iteration 4 | loss 0.12623566389083862\n",
      "Training | Average loss 0.1285352498292923\n",
      "Training iteration 0 | loss 0.13437466323375702\n",
      "Training iteration 1 | loss 0.13247591257095337\n",
      "Training iteration 2 | loss 0.13512946665287018\n",
      "Training iteration 3 | loss 0.12373191863298416\n",
      "Training iteration 4 | loss 0.11711999028921127\n",
      "Training | Average loss 0.1285663902759552\n",
      "Training iteration 0 | loss 0.11607879400253296\n",
      "Training iteration 1 | loss 0.12036782503128052\n",
      "Training iteration 2 | loss 0.14573566615581512\n",
      "Training iteration 3 | loss 0.126642107963562\n",
      "Training iteration 4 | loss 0.13621200621128082\n",
      "Training | Average loss 0.1290072798728943\n",
      "Training iteration 0 | loss 0.12703505158424377\n",
      "Training iteration 1 | loss 0.12708668410778046\n",
      "Training iteration 2 | loss 0.12276656180620193\n",
      "Training iteration 3 | loss 0.12637826800346375\n",
      "Training iteration 4 | loss 0.1495700180530548\n",
      "Training | Average loss 0.13056731671094896\n",
      "Training iteration 0 | loss 0.11507031321525574\n",
      "Training iteration 1 | loss 0.13346271216869354\n",
      "Training iteration 2 | loss 0.1235579401254654\n",
      "Training iteration 3 | loss 0.14969851076602936\n",
      "Training iteration 4 | loss 0.12998415529727936\n",
      "Training | Average loss 0.13035472631454467\n",
      "Training iteration 0 | loss 0.13818886876106262\n",
      "Training iteration 1 | loss 0.11820866912603378\n",
      "Training iteration 2 | loss 0.13012748956680298\n",
      "Training iteration 3 | loss 0.1265343278646469\n",
      "Training iteration 4 | loss 0.1292010396718979\n",
      "Training | Average loss 0.12845207899808883\n",
      "Training iteration 0 | loss 0.14145316183567047\n",
      "Training iteration 1 | loss 0.13498909771442413\n",
      "Training iteration 2 | loss 0.125448077917099\n",
      "Training iteration 3 | loss 0.1321265548467636\n",
      "Training iteration 4 | loss 0.1120162159204483\n",
      "Training | Average loss 0.1292066216468811\n",
      "Training iteration 0 | loss 0.1303645223379135\n",
      "Training iteration 1 | loss 0.13106375932693481\n",
      "Training iteration 2 | loss 0.11777594685554504\n",
      "Training iteration 3 | loss 0.14269040524959564\n",
      "Training iteration 4 | loss 0.12919485569000244\n",
      "Training | Average loss 0.13021789789199828\n",
      "Training iteration 0 | loss 0.12062335759401321\n",
      "Training iteration 1 | loss 0.1284189373254776\n",
      "Training iteration 2 | loss 0.12632259726524353\n",
      "Training iteration 3 | loss 0.14209970831871033\n",
      "Training iteration 4 | loss 0.1263921856880188\n",
      "Training | Average loss 0.1287713572382927\n",
      "Training iteration 0 | loss 0.1361292004585266\n",
      "Training iteration 1 | loss 0.11826621741056442\n",
      "Training iteration 2 | loss 0.13072428107261658\n",
      "Training iteration 3 | loss 0.12480366230010986\n",
      "Training iteration 4 | loss 0.13812892138957977\n",
      "Training | Average loss 0.12961045652627945\n",
      "Training iteration 0 | loss 0.12201385200023651\n",
      "Training iteration 1 | loss 0.1335119903087616\n",
      "Training iteration 2 | loss 0.12441357225179672\n",
      "Training iteration 3 | loss 0.1256309449672699\n",
      "Training iteration 4 | loss 0.1372779905796051\n",
      "Training | Average loss 0.12856967002153397\n",
      "Training iteration 0 | loss 0.12597782909870148\n",
      "Training iteration 1 | loss 0.12017346173524857\n",
      "Training iteration 2 | loss 0.13826487958431244\n",
      "Training iteration 3 | loss 0.12678243219852448\n",
      "Training iteration 4 | loss 0.1320009082555771\n",
      "Training | Average loss 0.1286399021744728\n",
      "Training iteration 0 | loss 0.12571093440055847\n",
      "Training iteration 1 | loss 0.12524442374706268\n",
      "Training iteration 2 | loss 0.14096800982952118\n",
      "Training iteration 3 | loss 0.11747695505619049\n",
      "Training iteration 4 | loss 0.1330450177192688\n",
      "Training | Average loss 0.12848906815052033\n",
      "Training iteration 0 | loss 0.13122422993183136\n",
      "Training iteration 1 | loss 0.12335796654224396\n",
      "Training iteration 2 | loss 0.13671644032001495\n",
      "Training iteration 3 | loss 0.13443593680858612\n",
      "Training iteration 4 | loss 0.12676821649074554\n",
      "Training | Average loss 0.13050055801868438\n",
      "Training iteration 0 | loss 0.12260624766349792\n",
      "Training iteration 1 | loss 0.12650439143180847\n",
      "Training iteration 2 | loss 0.13924762606620789\n",
      "Training iteration 3 | loss 0.12680576741695404\n",
      "Training iteration 4 | loss 0.13110791146755219\n",
      "Training | Average loss 0.1292543888092041\n",
      "Training iteration 0 | loss 0.14198151230812073\n",
      "Training iteration 1 | loss 0.12275532633066177\n",
      "Training iteration 2 | loss 0.1272077113389969\n",
      "Training iteration 3 | loss 0.133207306265831\n",
      "Training iteration 4 | loss 0.12110786139965057\n",
      "Training | Average loss 0.12925194352865219\n",
      "Training iteration 0 | loss 0.11767512559890747\n",
      "Training iteration 1 | loss 0.13538074493408203\n",
      "Training iteration 2 | loss 0.13222923874855042\n",
      "Training iteration 3 | loss 0.12508819997310638\n",
      "Training iteration 4 | loss 0.1339779794216156\n",
      "Training | Average loss 0.12887025773525237\n",
      "Training iteration 0 | loss 0.14288261532783508\n",
      "Training iteration 1 | loss 0.12706604599952698\n",
      "Training iteration 2 | loss 0.12279646098613739\n",
      "Training iteration 3 | loss 0.12218209356069565\n",
      "Training iteration 4 | loss 0.1396932303905487\n",
      "Training | Average loss 0.13092408925294877\n",
      "Training iteration 0 | loss 0.1370953470468521\n",
      "Training iteration 1 | loss 0.13285012543201447\n",
      "Training iteration 2 | loss 0.13378587365150452\n",
      "Training iteration 3 | loss 0.12431864440441132\n",
      "Training iteration 4 | loss 0.11902615427970886\n",
      "Training | Average loss 0.12941522896289825\n",
      "Training iteration 0 | loss 0.1361711621284485\n",
      "Training iteration 1 | loss 0.12627369165420532\n",
      "Training iteration 2 | loss 0.13047215342521667\n",
      "Training iteration 3 | loss 0.12400341778993607\n",
      "Training iteration 4 | loss 0.1276322603225708\n",
      "Training | Average loss 0.12891053706407546\n",
      "Training iteration 0 | loss 0.12259926646947861\n",
      "Training iteration 1 | loss 0.12280260026454926\n",
      "Training iteration 2 | loss 0.14091429114341736\n",
      "Training iteration 3 | loss 0.11877704411745071\n",
      "Training iteration 4 | loss 0.13968095183372498\n",
      "Training | Average loss 0.12895483076572417\n",
      "Training iteration 0 | loss 0.12251394987106323\n",
      "Training iteration 1 | loss 0.12742745876312256\n",
      "Training iteration 2 | loss 0.1254749596118927\n",
      "Training iteration 3 | loss 0.12766946852207184\n",
      "Training iteration 4 | loss 0.13934975862503052\n",
      "Training | Average loss 0.12848711907863616\n",
      "Training iteration 0 | loss 0.12598299980163574\n",
      "Training iteration 1 | loss 0.12571997940540314\n",
      "Training iteration 2 | loss 0.13220956921577454\n",
      "Training iteration 3 | loss 0.12465571612119675\n",
      "Training iteration 4 | loss 0.13700811564922333\n",
      "Training | Average loss 0.1291152760386467\n",
      "Training iteration 0 | loss 0.12107431143522263\n",
      "Training iteration 1 | loss 0.12795868515968323\n",
      "Training iteration 2 | loss 0.12901462614536285\n",
      "Training iteration 3 | loss 0.13996505737304688\n",
      "Training iteration 4 | loss 0.12427649646997452\n",
      "Training | Average loss 0.12845783531665803\n",
      "Training iteration 0 | loss 0.13132889568805695\n",
      "Training iteration 1 | loss 0.132857546210289\n",
      "Training iteration 2 | loss 0.11639080941677094\n",
      "Training iteration 3 | loss 0.13736674189567566\n",
      "Training iteration 4 | loss 0.12503711879253387\n",
      "Training | Average loss 0.1285962224006653\n",
      "Training iteration 0 | loss 0.13630926609039307\n",
      "Training iteration 1 | loss 0.12767069041728973\n",
      "Training iteration 2 | loss 0.11583496630191803\n",
      "Training iteration 3 | loss 0.13241222500801086\n",
      "Training iteration 4 | loss 0.1332995593547821\n",
      "Training | Average loss 0.12910534143447877\n",
      "Training iteration 0 | loss 0.12532749772071838\n",
      "Training iteration 1 | loss 0.1319357305765152\n",
      "Training iteration 2 | loss 0.12117980420589447\n",
      "Training iteration 3 | loss 0.1295885443687439\n",
      "Training iteration 4 | loss 0.13577881455421448\n",
      "Training | Average loss 0.1287620782852173\n",
      "Training iteration 0 | loss 0.1261720508337021\n",
      "Training iteration 1 | loss 0.13303044438362122\n",
      "Training iteration 2 | loss 0.12198822945356369\n",
      "Training iteration 3 | loss 0.13349561393260956\n",
      "Training iteration 4 | loss 0.12990272045135498\n",
      "Training | Average loss 0.1289178118109703\n",
      "Training iteration 0 | loss 0.11678379774093628\n",
      "Training iteration 1 | loss 0.13645951449871063\n",
      "Training iteration 2 | loss 0.12076383084058762\n",
      "Training iteration 3 | loss 0.1371140331029892\n",
      "Training iteration 4 | loss 0.13382992148399353\n",
      "Training | Average loss 0.12899021953344345\n",
      "Training iteration 0 | loss 0.11908067017793655\n",
      "Training iteration 1 | loss 0.12284430116415024\n",
      "Training iteration 2 | loss 0.1446136236190796\n",
      "Training iteration 3 | loss 0.13908950984477997\n",
      "Training iteration 4 | loss 0.11895669996738434\n",
      "Training | Average loss 0.12891696095466615\n",
      "Training iteration 0 | loss 0.13513213396072388\n",
      "Training iteration 1 | loss 0.13150320947170258\n",
      "Training iteration 2 | loss 0.13772453367710114\n",
      "Training iteration 3 | loss 0.1294916570186615\n",
      "Training iteration 4 | loss 0.11030495166778564\n",
      "Training | Average loss 0.12883129715919495\n",
      "Training iteration 0 | loss 0.1295982301235199\n",
      "Training iteration 1 | loss 0.12992645800113678\n",
      "Training iteration 2 | loss 0.1327621042728424\n",
      "Training iteration 3 | loss 0.12010687589645386\n",
      "Training iteration 4 | loss 0.1290944516658783\n",
      "Training | Average loss 0.12829762399196626\n",
      "Training iteration 0 | loss 0.12238019704818726\n",
      "Training iteration 1 | loss 0.14736054837703705\n",
      "Training iteration 2 | loss 0.13079723715782166\n",
      "Training iteration 3 | loss 0.12969012558460236\n",
      "Training iteration 4 | loss 0.11046400666236877\n",
      "Training | Average loss 0.12813842296600342\n",
      "Training iteration 0 | loss 0.1323617398738861\n",
      "Training iteration 1 | loss 0.12657657265663147\n",
      "Training iteration 2 | loss 0.12485619634389877\n",
      "Training iteration 3 | loss 0.12992075085639954\n",
      "Training iteration 4 | loss 0.13044239580631256\n",
      "Training | Average loss 0.12883153110742568\n",
      "Training iteration 0 | loss 0.11815260350704193\n",
      "Training iteration 1 | loss 0.1372949779033661\n",
      "Training iteration 2 | loss 0.1256224513053894\n",
      "Training iteration 3 | loss 0.13623401522636414\n",
      "Training iteration 4 | loss 0.1295374035835266\n",
      "Training | Average loss 0.12936829030513763\n",
      "Training iteration 0 | loss 0.1273532211780548\n",
      "Training iteration 1 | loss 0.1442447453737259\n",
      "Training iteration 2 | loss 0.12443286925554276\n",
      "Training iteration 3 | loss 0.12482918053865433\n",
      "Training iteration 4 | loss 0.12508518993854523\n",
      "Training | Average loss 0.1291890412569046\n",
      "Training iteration 0 | loss 0.13107670843601227\n",
      "Training iteration 1 | loss 0.1381763368844986\n",
      "Training iteration 2 | loss 0.12378895282745361\n",
      "Training iteration 3 | loss 0.10926879197359085\n",
      "Training iteration 4 | loss 0.13717660307884216\n",
      "Training | Average loss 0.1278974786400795\n",
      "Training iteration 0 | loss 0.14273573458194733\n",
      "Training iteration 1 | loss 0.1153387799859047\n",
      "Training iteration 2 | loss 0.12396112829446793\n",
      "Training iteration 3 | loss 0.12617021799087524\n",
      "Training iteration 4 | loss 0.13214506208896637\n",
      "Training | Average loss 0.1280701845884323\n",
      "Training iteration 0 | loss 0.1254805028438568\n",
      "Training iteration 1 | loss 0.13171978294849396\n",
      "Training iteration 2 | loss 0.13796056807041168\n",
      "Training iteration 3 | loss 0.12217628210783005\n",
      "Training iteration 4 | loss 0.12375233322381973\n",
      "Training | Average loss 0.12821789383888244\n",
      "Training iteration 0 | loss 0.1388978362083435\n",
      "Training iteration 1 | loss 0.12272439897060394\n",
      "Training iteration 2 | loss 0.13312074542045593\n",
      "Training iteration 3 | loss 0.131430521607399\n",
      "Training iteration 4 | loss 0.12006428092718124\n",
      "Training | Average loss 0.1292475566267967\n",
      "Training iteration 0 | loss 0.11854613572359085\n",
      "Training iteration 1 | loss 0.13111135363578796\n",
      "Training iteration 2 | loss 0.13796915113925934\n",
      "Training iteration 3 | loss 0.12876039743423462\n",
      "Training iteration 4 | loss 0.12395379692316055\n",
      "Training | Average loss 0.12806816697120665\n",
      "Training iteration 0 | loss 0.12881401181221008\n",
      "Training iteration 1 | loss 0.12354285269975662\n",
      "Training iteration 2 | loss 0.12761035561561584\n",
      "Training iteration 3 | loss 0.1314554661512375\n",
      "Training iteration 4 | loss 0.13079580664634705\n",
      "Training | Average loss 0.1284436985850334\n",
      "Training iteration 0 | loss 0.13361892104148865\n",
      "Training iteration 1 | loss 0.13080717623233795\n",
      "Training iteration 2 | loss 0.12424471229314804\n",
      "Training iteration 3 | loss 0.1392829269170761\n",
      "Training iteration 4 | loss 0.12126763164997101\n",
      "Training | Average loss 0.12984427362680434\n",
      "Training iteration 0 | loss 0.12152955681085587\n",
      "Training iteration 1 | loss 0.1285092979669571\n",
      "Training iteration 2 | loss 0.13335174322128296\n",
      "Training iteration 3 | loss 0.12399981170892715\n",
      "Training iteration 4 | loss 0.13285641372203827\n",
      "Training | Average loss 0.12804936468601227\n",
      "Training iteration 0 | loss 0.12426763027906418\n",
      "Training iteration 1 | loss 0.12933950126171112\n",
      "Training iteration 2 | loss 0.12014806270599365\n",
      "Training iteration 3 | loss 0.1395302712917328\n",
      "Training iteration 4 | loss 0.12593293190002441\n",
      "Training | Average loss 0.12784367948770523\n",
      "Training iteration 0 | loss 0.125899538397789\n",
      "Training iteration 1 | loss 0.13778233528137207\n",
      "Training iteration 2 | loss 0.12904079258441925\n",
      "Training iteration 3 | loss 0.12387190014123917\n",
      "Training iteration 4 | loss 0.12617363035678864\n",
      "Training | Average loss 0.1285536393523216\n",
      "Training iteration 0 | loss 0.1251824051141739\n",
      "Training iteration 1 | loss 0.12978726625442505\n",
      "Training iteration 2 | loss 0.13533316552639008\n",
      "Training iteration 3 | loss 0.12643739581108093\n",
      "Training iteration 4 | loss 0.125424325466156\n",
      "Training | Average loss 0.1284329116344452\n",
      "Training iteration 0 | loss 0.11978746205568314\n",
      "Training iteration 1 | loss 0.12155608832836151\n",
      "Training iteration 2 | loss 0.1316373646259308\n",
      "Training iteration 3 | loss 0.12864260375499725\n",
      "Training iteration 4 | loss 0.14154097437858582\n",
      "Training | Average loss 0.1286328986287117\n",
      "Training iteration 0 | loss 0.1289089024066925\n",
      "Training iteration 1 | loss 0.1316014975309372\n",
      "Training iteration 2 | loss 0.1250075399875641\n",
      "Training iteration 3 | loss 0.11594048887491226\n",
      "Training iteration 4 | loss 0.13948090374469757\n",
      "Training | Average loss 0.12818786650896072\n",
      "Training iteration 0 | loss 0.13899767398834229\n",
      "Training iteration 1 | loss 0.13607051968574524\n",
      "Training iteration 2 | loss 0.1233498826622963\n",
      "Training iteration 3 | loss 0.12936106324195862\n",
      "Training iteration 4 | loss 0.12731342017650604\n",
      "Training | Average loss 0.1310185119509697\n",
      "Training iteration 0 | loss 0.1216529831290245\n",
      "Training iteration 1 | loss 0.11491251736879349\n",
      "Training iteration 2 | loss 0.1429876983165741\n",
      "Training iteration 3 | loss 0.12673591077327728\n",
      "Training iteration 4 | loss 0.13251712918281555\n",
      "Training | Average loss 0.12776124775409697\n",
      "Training iteration 0 | loss 0.128349170088768\n",
      "Training iteration 1 | loss 0.12713365256786346\n",
      "Training iteration 2 | loss 0.13519012928009033\n",
      "Training iteration 3 | loss 0.12458919733762741\n",
      "Training iteration 4 | loss 0.12438686192035675\n",
      "Training | Average loss 0.12792980223894118\n",
      "Training iteration 0 | loss 0.11428659409284592\n",
      "Training iteration 1 | loss 0.12972232699394226\n",
      "Training iteration 2 | loss 0.127103790640831\n",
      "Training iteration 3 | loss 0.13405939936637878\n",
      "Training iteration 4 | loss 0.1352534294128418\n",
      "Training | Average loss 0.12808510810136794\n",
      "Training iteration 0 | loss 0.11781907081604004\n",
      "Training iteration 1 | loss 0.13128337264060974\n",
      "Training iteration 2 | loss 0.1344723403453827\n",
      "Training iteration 3 | loss 0.12357296049594879\n",
      "Training iteration 4 | loss 0.137333944439888\n",
      "Training | Average loss 0.12889633774757386\n",
      "Training iteration 0 | loss 0.12367159128189087\n",
      "Training iteration 1 | loss 0.13604815304279327\n",
      "Training iteration 2 | loss 0.134622722864151\n",
      "Training iteration 3 | loss 0.1304401308298111\n",
      "Training iteration 4 | loss 0.11536732316017151\n",
      "Training | Average loss 0.12802998423576356\n",
      "Training iteration 0 | loss 0.12149260938167572\n",
      "Training iteration 1 | loss 0.12366891652345657\n",
      "Training iteration 2 | loss 0.13246476650238037\n",
      "Training iteration 3 | loss 0.12618501484394073\n",
      "Training iteration 4 | loss 0.1381235271692276\n",
      "Training | Average loss 0.1283869668841362\n",
      "Training iteration 0 | loss 0.1261679232120514\n",
      "Training iteration 1 | loss 0.13473409414291382\n",
      "Training iteration 2 | loss 0.11344041675329208\n",
      "Training iteration 3 | loss 0.12963977456092834\n",
      "Training iteration 4 | loss 0.1349906325340271\n",
      "Training | Average loss 0.12779456824064256\n",
      "Training iteration 0 | loss 0.11274778097867966\n",
      "Training iteration 1 | loss 0.12568733096122742\n",
      "Training iteration 2 | loss 0.12881164252758026\n",
      "Training iteration 3 | loss 0.12400628626346588\n",
      "Training iteration 4 | loss 0.14652538299560547\n",
      "Training | Average loss 0.12755568474531173\n",
      "Training iteration 0 | loss 0.13046804070472717\n",
      "Training iteration 1 | loss 0.1233791634440422\n",
      "Training iteration 2 | loss 0.12885095179080963\n",
      "Training iteration 3 | loss 0.12312459200620651\n",
      "Training iteration 4 | loss 0.14040282368659973\n",
      "Training | Average loss 0.12924511432647706\n",
      "Training iteration 0 | loss 0.13599646091461182\n",
      "Training iteration 1 | loss 0.1308022141456604\n",
      "Training iteration 2 | loss 0.1218109130859375\n",
      "Training iteration 3 | loss 0.12697245180606842\n",
      "Training iteration 4 | loss 0.13012805581092834\n",
      "Training | Average loss 0.12914201915264129\n",
      "Training iteration 0 | loss 0.12070854753255844\n",
      "Training iteration 1 | loss 0.1252693235874176\n",
      "Training iteration 2 | loss 0.12374049425125122\n",
      "Training iteration 3 | loss 0.13054214417934418\n",
      "Training iteration 4 | loss 0.1402588039636612\n",
      "Training | Average loss 0.12810386270284652\n",
      "Training iteration 0 | loss 0.13336826860904694\n",
      "Training iteration 1 | loss 0.12181826680898666\n",
      "Training iteration 2 | loss 0.13388106226921082\n",
      "Training iteration 3 | loss 0.13243354856967926\n",
      "Training iteration 4 | loss 0.12039614468812943\n",
      "Training | Average loss 0.12837945818901061\n",
      "Training iteration 0 | loss 0.12088313698768616\n",
      "Training iteration 1 | loss 0.13542617857456207\n",
      "Training iteration 2 | loss 0.12508462369441986\n",
      "Training iteration 3 | loss 0.12965027987957\n",
      "Training iteration 4 | loss 0.13154594600200653\n",
      "Training | Average loss 0.12851803302764891\n",
      "Training iteration 0 | loss 0.13605765998363495\n",
      "Training iteration 1 | loss 0.1350993663072586\n",
      "Training iteration 2 | loss 0.12804897129535675\n",
      "Training iteration 3 | loss 0.13109509646892548\n",
      "Training iteration 4 | loss 0.12568803131580353\n",
      "Training | Average loss 0.13119782507419586\n",
      "Training iteration 0 | loss 0.11413145065307617\n",
      "Training iteration 1 | loss 0.12092091888189316\n",
      "Training iteration 2 | loss 0.13317833840847015\n",
      "Training iteration 3 | loss 0.14557966589927673\n",
      "Training iteration 4 | loss 0.1293475478887558\n",
      "Training | Average loss 0.1286315843462944\n",
      "Training iteration 0 | loss 0.13205388188362122\n",
      "Training iteration 1 | loss 0.12256742268800735\n",
      "Training iteration 2 | loss 0.12706120312213898\n",
      "Training iteration 3 | loss 0.13863585889339447\n",
      "Training iteration 4 | loss 0.12673328816890717\n",
      "Training | Average loss 0.12941033095121385\n",
      "Training iteration 0 | loss 0.11196619272232056\n",
      "Training iteration 1 | loss 0.1368454247713089\n",
      "Training iteration 2 | loss 0.12350113689899445\n",
      "Training iteration 3 | loss 0.12949955463409424\n",
      "Training iteration 4 | loss 0.1396363377571106\n",
      "Training | Average loss 0.12828972935676575\n",
      "Training iteration 0 | loss 0.12126154452562332\n",
      "Training iteration 1 | loss 0.14378581941127777\n",
      "Training iteration 2 | loss 0.11262555420398712\n",
      "Training iteration 3 | loss 0.1248280256986618\n",
      "Training iteration 4 | loss 0.13570672273635864\n",
      "Training | Average loss 0.12764153331518174\n",
      "Training iteration 0 | loss 0.12671345472335815\n",
      "Training iteration 1 | loss 0.12560001015663147\n",
      "Training iteration 2 | loss 0.1453767716884613\n",
      "Training iteration 3 | loss 0.1291446089744568\n",
      "Training iteration 4 | loss 0.13265669345855713\n",
      "Training | Average loss 0.13189830780029296\n",
      "Training iteration 0 | loss 0.12831661105155945\n",
      "Training iteration 1 | loss 0.13215917348861694\n",
      "Training iteration 2 | loss 0.13260293006896973\n",
      "Training iteration 3 | loss 0.13124726712703705\n",
      "Training iteration 4 | loss 0.12421569973230362\n",
      "Training | Average loss 0.12970833629369735\n",
      "Training iteration 0 | loss 0.11988458037376404\n",
      "Training iteration 1 | loss 0.12928013503551483\n",
      "Training iteration 2 | loss 0.13243262469768524\n",
      "Training iteration 3 | loss 0.12851405143737793\n",
      "Training iteration 4 | loss 0.12837007641792297\n",
      "Training | Average loss 0.127696293592453\n",
      "Training iteration 0 | loss 0.1230899915099144\n",
      "Training iteration 1 | loss 0.1403467059135437\n",
      "Training iteration 2 | loss 0.13125468790531158\n",
      "Training iteration 3 | loss 0.12121433019638062\n",
      "Training iteration 4 | loss 0.13371971249580383\n",
      "Training | Average loss 0.12992508560419083\n",
      "Training iteration 0 | loss 0.13875006139278412\n",
      "Training iteration 1 | loss 0.12428783625364304\n",
      "Training iteration 2 | loss 0.1309928447008133\n",
      "Training iteration 3 | loss 0.12279254198074341\n",
      "Training iteration 4 | loss 0.12412653863430023\n",
      "Training | Average loss 0.12818996459245682\n",
      "Training iteration 0 | loss 0.14191800355911255\n",
      "Training iteration 1 | loss 0.12762144207954407\n",
      "Training iteration 2 | loss 0.13460275530815125\n",
      "Training iteration 3 | loss 0.11376312375068665\n",
      "Training iteration 4 | loss 0.1247892677783966\n",
      "Training | Average loss 0.12853891849517823\n",
      "Training iteration 0 | loss 0.1263923943042755\n",
      "Training iteration 1 | loss 0.1361459344625473\n",
      "Training iteration 2 | loss 0.13299229741096497\n",
      "Training iteration 3 | loss 0.12823638319969177\n",
      "Training iteration 4 | loss 0.11605788767337799\n",
      "Training | Average loss 0.12796497941017151\n",
      "Training iteration 0 | loss 0.11446009576320648\n",
      "Training iteration 1 | loss 0.129690021276474\n",
      "Training iteration 2 | loss 0.12796688079833984\n",
      "Training iteration 3 | loss 0.13418278098106384\n",
      "Training iteration 4 | loss 0.13349130749702454\n",
      "Training | Average loss 0.12795821726322174\n",
      "Training iteration 0 | loss 0.1375570148229599\n",
      "Training iteration 1 | loss 0.14398358762264252\n",
      "Training iteration 2 | loss 0.11843021959066391\n",
      "Training iteration 3 | loss 0.12683416903018951\n",
      "Training iteration 4 | loss 0.12318190187215805\n",
      "Training | Average loss 0.12999737858772278\n",
      "Training iteration 0 | loss 0.12609392404556274\n",
      "Training iteration 1 | loss 0.12220652401447296\n",
      "Training iteration 2 | loss 0.13011294603347778\n",
      "Training iteration 3 | loss 0.14475660026073456\n",
      "Training iteration 4 | loss 0.12511801719665527\n",
      "Training | Average loss 0.12965760231018067\n",
      "Training iteration 0 | loss 0.12294932454824448\n",
      "Training iteration 1 | loss 0.14126160740852356\n",
      "Training iteration 2 | loss 0.13042865693569183\n",
      "Training iteration 3 | loss 0.11927632242441177\n",
      "Training iteration 4 | loss 0.1307758241891861\n",
      "Training | Average loss 0.12893834710121155\n",
      "Training iteration 0 | loss 0.11470790952444077\n",
      "Training iteration 1 | loss 0.12431393563747406\n",
      "Training iteration 2 | loss 0.13934920728206635\n",
      "Training iteration 3 | loss 0.1408316045999527\n",
      "Training iteration 4 | loss 0.1266041398048401\n",
      "Training | Average loss 0.12916135936975479\n",
      "Training iteration 0 | loss 0.10969507694244385\n",
      "Training iteration 1 | loss 0.14963670074939728\n",
      "Training iteration 2 | loss 0.12744446098804474\n",
      "Training iteration 3 | loss 0.13605789840221405\n",
      "Training iteration 4 | loss 0.130800262093544\n",
      "Training | Average loss 0.1307268798351288\n",
      "Training iteration 0 | loss 0.1212809607386589\n",
      "Training iteration 1 | loss 0.13290154933929443\n",
      "Training iteration 2 | loss 0.12388797849416733\n",
      "Training iteration 3 | loss 0.12968595325946808\n",
      "Training iteration 4 | loss 0.13169510662555695\n",
      "Training | Average loss 0.12789030969142914\n",
      "Training iteration 0 | loss 0.1327998787164688\n",
      "Training iteration 1 | loss 0.1280597597360611\n",
      "Training iteration 2 | loss 0.14872954785823822\n",
      "Training iteration 3 | loss 0.12853604555130005\n",
      "Training iteration 4 | loss 0.11783263087272644\n",
      "Training | Average loss 0.13119157254695893\n",
      "Training iteration 0 | loss 0.1502426117658615\n",
      "Training iteration 1 | loss 0.12477055191993713\n",
      "Training iteration 2 | loss 0.11403293162584305\n",
      "Training iteration 3 | loss 0.12336400896310806\n",
      "Training iteration 4 | loss 0.12888936698436737\n",
      "Training | Average loss 0.12825989425182344\n",
      "Training iteration 0 | loss 0.12888532876968384\n",
      "Training iteration 1 | loss 0.12743303179740906\n",
      "Training iteration 2 | loss 0.13936910033226013\n",
      "Training iteration 3 | loss 0.12373237311840057\n",
      "Training iteration 4 | loss 0.12512421607971191\n",
      "Training | Average loss 0.12890881001949311\n",
      "Training iteration 0 | loss 0.12469132244586945\n",
      "Training iteration 1 | loss 0.13169348239898682\n",
      "Training iteration 2 | loss 0.11673298478126526\n",
      "Training iteration 3 | loss 0.12168023735284805\n",
      "Training iteration 4 | loss 0.14549779891967773\n",
      "Training | Average loss 0.12805916517972946\n",
      "Training iteration 0 | loss 0.13132883608341217\n",
      "Training iteration 1 | loss 0.12381467968225479\n",
      "Training iteration 2 | loss 0.13245628774166107\n",
      "Training iteration 3 | loss 0.12087936699390411\n",
      "Training iteration 4 | loss 0.1339319944381714\n",
      "Training | Average loss 0.1284822329878807\n",
      "Training iteration 0 | loss 0.11965547502040863\n",
      "Training iteration 1 | loss 0.1334264576435089\n",
      "Training iteration 2 | loss 0.12172316014766693\n",
      "Training iteration 3 | loss 0.14312107861042023\n",
      "Training iteration 4 | loss 0.12626811861991882\n",
      "Training | Average loss 0.1288388580083847\n",
      "Training iteration 0 | loss 0.12364304810762405\n",
      "Training iteration 1 | loss 0.13718579709529877\n",
      "Training iteration 2 | loss 0.1294054239988327\n",
      "Training iteration 3 | loss 0.12960395216941833\n",
      "Training iteration 4 | loss 0.12538579106330872\n",
      "Training | Average loss 0.1290448024868965\n",
      "Training iteration 0 | loss 0.12483464926481247\n",
      "Training iteration 1 | loss 0.1413576304912567\n",
      "Training iteration 2 | loss 0.12651695311069489\n",
      "Training iteration 3 | loss 0.11784328520298004\n",
      "Training iteration 4 | loss 0.13009269535541534\n",
      "Training | Average loss 0.1281290426850319\n",
      "Training iteration 0 | loss 0.13056913018226624\n",
      "Training iteration 1 | loss 0.12167505919933319\n",
      "Training iteration 2 | loss 0.14230559766292572\n",
      "Training iteration 3 | loss 0.125759094953537\n",
      "Training iteration 4 | loss 0.12327276915311813\n",
      "Training | Average loss 0.12871633023023604\n",
      "Training iteration 0 | loss 0.13446243107318878\n",
      "Training iteration 1 | loss 0.13456234335899353\n",
      "Training iteration 2 | loss 0.11612467467784882\n",
      "Training iteration 3 | loss 0.12142360210418701\n",
      "Training iteration 4 | loss 0.13366086781024933\n",
      "Training | Average loss 0.12804678380489348\n",
      "Training iteration 0 | loss 0.12786510586738586\n",
      "Training iteration 1 | loss 0.12290212512016296\n",
      "Training iteration 2 | loss 0.12938372790813446\n",
      "Training iteration 3 | loss 0.12529075145721436\n",
      "Training iteration 4 | loss 0.13505995273590088\n",
      "Training | Average loss 0.12810033261775972\n",
      "Training iteration 0 | loss 0.1229681447148323\n",
      "Training iteration 1 | loss 0.12276208400726318\n",
      "Training iteration 2 | loss 0.12912918627262115\n",
      "Training iteration 3 | loss 0.12653130292892456\n",
      "Training iteration 4 | loss 0.14170554280281067\n",
      "Training | Average loss 0.12861925214529038\n",
      "Training iteration 0 | loss 0.12408923357725143\n",
      "Training iteration 1 | loss 0.12818190455436707\n",
      "Training iteration 2 | loss 0.1296943873167038\n",
      "Training iteration 3 | loss 0.13004694879055023\n",
      "Training iteration 4 | loss 0.1342940330505371\n",
      "Training | Average loss 0.12926130145788192\n",
      "Training iteration 0 | loss 0.13389404118061066\n",
      "Training iteration 1 | loss 0.1207231730222702\n",
      "Training iteration 2 | loss 0.1269848793745041\n",
      "Training iteration 3 | loss 0.12075173109769821\n",
      "Training iteration 4 | loss 0.14019635319709778\n",
      "Training | Average loss 0.1285100355744362\n",
      "Training iteration 0 | loss 0.12425681948661804\n",
      "Training iteration 1 | loss 0.13929012417793274\n",
      "Training iteration 2 | loss 0.13181322813034058\n",
      "Training iteration 3 | loss 0.12499434500932693\n",
      "Training iteration 4 | loss 0.11985859274864197\n",
      "Training | Average loss 0.12804262191057206\n",
      "Training iteration 0 | loss 0.12254714965820312\n",
      "Training iteration 1 | loss 0.12336017936468124\n",
      "Training iteration 2 | loss 0.131665900349617\n",
      "Training iteration 3 | loss 0.12885838747024536\n",
      "Training iteration 4 | loss 0.13137570023536682\n",
      "Training | Average loss 0.12756146341562272\n",
      "Training iteration 0 | loss 0.12675315141677856\n",
      "Training iteration 1 | loss 0.12972457706928253\n",
      "Training iteration 2 | loss 0.1269005537033081\n",
      "Training iteration 3 | loss 0.12864993512630463\n",
      "Training iteration 4 | loss 0.1253553032875061\n",
      "Training | Average loss 0.127476704120636\n",
      "Training iteration 0 | loss 0.13349324464797974\n",
      "Training iteration 1 | loss 0.13579174876213074\n",
      "Training iteration 2 | loss 0.11825967580080032\n",
      "Training iteration 3 | loss 0.12643437087535858\n",
      "Training iteration 4 | loss 0.12298455089330673\n",
      "Training | Average loss 0.12739271819591522\n",
      "Training iteration 0 | loss 0.12746204435825348\n",
      "Training iteration 1 | loss 0.1315034031867981\n",
      "Training iteration 2 | loss 0.11575014889240265\n",
      "Training iteration 3 | loss 0.12831026315689087\n",
      "Training iteration 4 | loss 0.1344500035047531\n",
      "Training | Average loss 0.12749517261981963\n",
      "Training iteration 0 | loss 0.13995781540870667\n",
      "Training iteration 1 | loss 0.10918848216533661\n",
      "Training iteration 2 | loss 0.1228167861700058\n",
      "Training iteration 3 | loss 0.12600040435791016\n",
      "Training iteration 4 | loss 0.14541025459766388\n",
      "Training | Average loss 0.1286747485399246\n",
      "Training iteration 0 | loss 0.12952439486980438\n",
      "Training iteration 1 | loss 0.13433067500591278\n",
      "Training iteration 2 | loss 0.12487514317035675\n",
      "Training iteration 3 | loss 0.13481192290782928\n",
      "Training iteration 4 | loss 0.11507266014814377\n",
      "Training | Average loss 0.12772295922040938\n",
      "Training iteration 0 | loss 0.12314826995134354\n",
      "Training iteration 1 | loss 0.14530277252197266\n",
      "Training iteration 2 | loss 0.12759023904800415\n",
      "Training iteration 3 | loss 0.12234644591808319\n",
      "Training iteration 4 | loss 0.12552787363529205\n",
      "Training | Average loss 0.12878312021493912\n",
      "Training iteration 0 | loss 0.13566482067108154\n",
      "Training iteration 1 | loss 0.11066371202468872\n",
      "Training iteration 2 | loss 0.1271488219499588\n",
      "Training iteration 3 | loss 0.14144402742385864\n",
      "Training iteration 4 | loss 0.12759798765182495\n",
      "Training | Average loss 0.12850387394428253\n",
      "Training iteration 0 | loss 0.12890759110450745\n",
      "Training iteration 1 | loss 0.1339285969734192\n",
      "Training iteration 2 | loss 0.1268320232629776\n",
      "Training iteration 3 | loss 0.13474693894386292\n",
      "Training iteration 4 | loss 0.12345635890960693\n",
      "Training | Average loss 0.1295743018388748\n",
      "Training iteration 0 | loss 0.14238059520721436\n",
      "Training iteration 1 | loss 0.13369964063167572\n",
      "Training iteration 2 | loss 0.12135931849479675\n",
      "Training iteration 3 | loss 0.12315263599157333\n",
      "Training iteration 4 | loss 0.11777511984109879\n",
      "Training | Average loss 0.1276734620332718\n",
      "Training iteration 0 | loss 0.12704962491989136\n",
      "Training iteration 1 | loss 0.128509521484375\n",
      "Training iteration 2 | loss 0.12823788821697235\n",
      "Training iteration 3 | loss 0.1321641355752945\n",
      "Training iteration 4 | loss 0.12179204821586609\n",
      "Training | Average loss 0.12755064368247987\n",
      "Training iteration 0 | loss 0.11611580848693848\n",
      "Training iteration 1 | loss 0.12821917235851288\n",
      "Training iteration 2 | loss 0.13852861523628235\n",
      "Training iteration 3 | loss 0.13112376630306244\n",
      "Training iteration 4 | loss 0.12528352439403534\n",
      "Training | Average loss 0.1278541773557663\n",
      "Training iteration 0 | loss 0.12364806979894638\n",
      "Training iteration 1 | loss 0.11693918704986572\n",
      "Training iteration 2 | loss 0.1237419918179512\n",
      "Training iteration 3 | loss 0.14954036474227905\n",
      "Training iteration 4 | loss 0.12138696759939194\n",
      "Training | Average loss 0.12705131620168686\n",
      "Training iteration 0 | loss 0.14185790717601776\n",
      "Training iteration 1 | loss 0.1254376620054245\n",
      "Training iteration 2 | loss 0.11913767457008362\n",
      "Training iteration 3 | loss 0.12394089251756668\n",
      "Training iteration 4 | loss 0.12646286189556122\n",
      "Training | Average loss 0.12736739963293076\n",
      "Training iteration 0 | loss 0.13006584346294403\n",
      "Training iteration 1 | loss 0.11342675983905792\n",
      "Training iteration 2 | loss 0.13261814415454865\n",
      "Training iteration 3 | loss 0.1278204619884491\n",
      "Training iteration 4 | loss 0.1334286630153656\n",
      "Training | Average loss 0.12747197449207306\n",
      "Training iteration 0 | loss 0.1296498030424118\n",
      "Training iteration 1 | loss 0.12481041252613068\n",
      "Training iteration 2 | loss 0.12891271710395813\n",
      "Training iteration 3 | loss 0.12613093852996826\n",
      "Training iteration 4 | loss 0.12894494831562042\n",
      "Training | Average loss 0.12768976390361786\n",
      "Training iteration 0 | loss 0.13688047230243683\n",
      "Training iteration 1 | loss 0.12460672110319138\n",
      "Training iteration 2 | loss 0.11711885780096054\n",
      "Training iteration 3 | loss 0.12697812914848328\n",
      "Training iteration 4 | loss 0.13369479775428772\n",
      "Training | Average loss 0.12785579562187194\n",
      "Training iteration 0 | loss 0.13817861676216125\n",
      "Training iteration 1 | loss 0.12889856100082397\n",
      "Training iteration 2 | loss 0.1284627765417099\n",
      "Training iteration 3 | loss 0.1282423585653305\n",
      "Training iteration 4 | loss 0.12072130292654037\n",
      "Training | Average loss 0.1289007231593132\n",
      "Training iteration 0 | loss 0.13674801588058472\n",
      "Training iteration 1 | loss 0.13148710131645203\n",
      "Training iteration 2 | loss 0.13578353822231293\n",
      "Training iteration 3 | loss 0.12861719727516174\n",
      "Training iteration 4 | loss 0.11160733550786972\n",
      "Training | Average loss 0.12884863764047622\n",
      "Training iteration 0 | loss 0.13048189878463745\n",
      "Training iteration 1 | loss 0.13015128672122955\n",
      "Training iteration 2 | loss 0.1271262913942337\n",
      "Training iteration 3 | loss 0.13410939276218414\n",
      "Training iteration 4 | loss 0.121184803545475\n",
      "Training | Average loss 0.12861073464155198\n",
      "Training iteration 0 | loss 0.13429276645183563\n",
      "Training iteration 1 | loss 0.12537813186645508\n",
      "Training iteration 2 | loss 0.13429778814315796\n",
      "Training iteration 3 | loss 0.1251622438430786\n",
      "Training iteration 4 | loss 0.12590238451957703\n",
      "Training | Average loss 0.12900666296482086\n",
      "Training iteration 0 | loss 0.12553074955940247\n",
      "Training iteration 1 | loss 0.12943345308303833\n",
      "Training iteration 2 | loss 0.11840866506099701\n",
      "Training iteration 3 | loss 0.13692660629749298\n",
      "Training iteration 4 | loss 0.12964756786823273\n",
      "Training | Average loss 0.1279894083738327\n",
      "Training iteration 0 | loss 0.12386753410100937\n",
      "Training iteration 1 | loss 0.11475593596696854\n",
      "Training iteration 2 | loss 0.13371729850769043\n",
      "Training iteration 3 | loss 0.125586599111557\n",
      "Training iteration 4 | loss 0.14524297416210175\n",
      "Training | Average loss 0.1286340683698654\n",
      "Training iteration 0 | loss 0.1253013014793396\n",
      "Training iteration 1 | loss 0.11276829242706299\n",
      "Training iteration 2 | loss 0.1385461986064911\n",
      "Training iteration 3 | loss 0.13788340985774994\n",
      "Training iteration 4 | loss 0.1294497847557068\n",
      "Training | Average loss 0.12878979742527008\n",
      "Training iteration 0 | loss 0.13411571085453033\n",
      "Training iteration 1 | loss 0.12837187945842743\n",
      "Training iteration 2 | loss 0.10904187709093094\n",
      "Training iteration 3 | loss 0.13175906240940094\n",
      "Training iteration 4 | loss 0.13509908318519592\n",
      "Training | Average loss 0.1276775225996971\n",
      "Training iteration 0 | loss 0.12037228792905807\n",
      "Training iteration 1 | loss 0.1217469573020935\n",
      "Training iteration 2 | loss 0.13212816417217255\n",
      "Training iteration 3 | loss 0.11299731582403183\n",
      "Training iteration 4 | loss 0.15168720483779907\n",
      "Training | Average loss 0.127786386013031\n",
      "Training iteration 0 | loss 0.1259959489107132\n",
      "Training iteration 1 | loss 0.12447517365217209\n",
      "Training iteration 2 | loss 0.13297563791275024\n",
      "Training iteration 3 | loss 0.12718385457992554\n",
      "Training iteration 4 | loss 0.126389279961586\n",
      "Training | Average loss 0.12740397900342942\n",
      "Training iteration 0 | loss 0.1268928050994873\n",
      "Training iteration 1 | loss 0.10981416702270508\n",
      "Training iteration 2 | loss 0.1387840062379837\n",
      "Training iteration 3 | loss 0.13740849494934082\n",
      "Training iteration 4 | loss 0.1263100951910019\n",
      "Training | Average loss 0.12784191370010375\n",
      "Training iteration 0 | loss 0.13710002601146698\n",
      "Training iteration 1 | loss 0.13167688250541687\n",
      "Training iteration 2 | loss 0.12549655139446259\n",
      "Training iteration 3 | loss 0.13127391040325165\n",
      "Training iteration 4 | loss 0.11752087622880936\n",
      "Training | Average loss 0.1286136493086815\n",
      "Training iteration 0 | loss 0.1278330236673355\n",
      "Training iteration 1 | loss 0.1303400695323944\n",
      "Training iteration 2 | loss 0.12119527161121368\n",
      "Training iteration 3 | loss 0.13817434012889862\n",
      "Training iteration 4 | loss 0.12035439908504486\n",
      "Training | Average loss 0.12757942080497742\n",
      "Training iteration 0 | loss 0.11843626201152802\n",
      "Training iteration 1 | loss 0.12691326439380646\n",
      "Training iteration 2 | loss 0.12402209639549255\n",
      "Training iteration 3 | loss 0.12062206864356995\n",
      "Training iteration 4 | loss 0.14901146292686462\n",
      "Training | Average loss 0.12780103087425232\n",
      "Training iteration 0 | loss 0.1271568238735199\n",
      "Training iteration 1 | loss 0.1334722638130188\n",
      "Training iteration 2 | loss 0.11293813586235046\n",
      "Training iteration 3 | loss 0.12478840351104736\n",
      "Training iteration 4 | loss 0.13768815994262695\n",
      "Training | Average loss 0.12720875740051268\n",
      "Training iteration 0 | loss 0.14308258891105652\n",
      "Training iteration 1 | loss 0.12011672556400299\n",
      "Training iteration 2 | loss 0.1138966903090477\n",
      "Training iteration 3 | loss 0.12469621747732162\n",
      "Training iteration 4 | loss 0.1390744298696518\n",
      "Training | Average loss 0.12817333042621612\n",
      "Training iteration 0 | loss 0.130296990275383\n",
      "Training iteration 1 | loss 0.12336692959070206\n",
      "Training iteration 2 | loss 0.13402797281742096\n",
      "Training iteration 3 | loss 0.13749061524868011\n",
      "Training iteration 4 | loss 0.10988340526819229\n",
      "Training | Average loss 0.1270131826400757\n",
      "Training iteration 0 | loss 0.12427309900522232\n",
      "Training iteration 1 | loss 0.136091947555542\n",
      "Training iteration 2 | loss 0.12086868286132812\n",
      "Training iteration 3 | loss 0.12910598516464233\n",
      "Training iteration 4 | loss 0.12863487005233765\n",
      "Training | Average loss 0.12779491692781447\n",
      "Training iteration 0 | loss 0.11590457707643509\n",
      "Training iteration 1 | loss 0.13817228376865387\n",
      "Training iteration 2 | loss 0.13005276024341583\n",
      "Training iteration 3 | loss 0.11966592073440552\n",
      "Training iteration 4 | loss 0.132288858294487\n",
      "Training | Average loss 0.12721688002347947\n",
      "Training iteration 0 | loss 0.128565713763237\n",
      "Training iteration 1 | loss 0.1246144101023674\n",
      "Training iteration 2 | loss 0.12340958416461945\n",
      "Training iteration 3 | loss 0.13554692268371582\n",
      "Training iteration 4 | loss 0.13091666996479034\n",
      "Training | Average loss 0.128610660135746\n",
      "Training iteration 0 | loss 0.11473910510540009\n",
      "Training iteration 1 | loss 0.12762635946273804\n",
      "Training iteration 2 | loss 0.13341617584228516\n",
      "Training iteration 3 | loss 0.13200262188911438\n",
      "Training iteration 4 | loss 0.13398487865924835\n",
      "Training | Average loss 0.1283538281917572\n",
      "Training iteration 0 | loss 0.13639958202838898\n",
      "Training iteration 1 | loss 0.11551595479249954\n",
      "Training iteration 2 | loss 0.1280873566865921\n",
      "Training iteration 3 | loss 0.1255853772163391\n",
      "Training iteration 4 | loss 0.13987144827842712\n",
      "Training | Average loss 0.12909194380044936\n",
      "Training iteration 0 | loss 0.1317320168018341\n",
      "Training iteration 1 | loss 0.12358806282281876\n",
      "Training iteration 2 | loss 0.12069112807512283\n",
      "Training iteration 3 | loss 0.12900762259960175\n",
      "Training iteration 4 | loss 0.1355580985546112\n",
      "Training | Average loss 0.12811538577079773\n",
      "Training iteration 0 | loss 0.12212992459535599\n",
      "Training iteration 1 | loss 0.13210764527320862\n",
      "Training iteration 2 | loss 0.13389210402965546\n",
      "Training iteration 3 | loss 0.12683828175067902\n",
      "Training iteration 4 | loss 0.1317020207643509\n",
      "Training | Average loss 0.12933399528265\n",
      "Training iteration 0 | loss 0.13979031145572662\n",
      "Training iteration 1 | loss 0.12280841916799545\n",
      "Training iteration 2 | loss 0.11571571230888367\n",
      "Training iteration 3 | loss 0.14213457703590393\n",
      "Training iteration 4 | loss 0.1348174810409546\n",
      "Training | Average loss 0.13105330020189285\n",
      "Training iteration 0 | loss 0.12450048327445984\n",
      "Training iteration 1 | loss 0.1276787370443344\n",
      "Training iteration 2 | loss 0.1339825540781021\n",
      "Training iteration 3 | loss 0.1324644237756729\n",
      "Training iteration 4 | loss 0.12549646198749542\n",
      "Training | Average loss 0.12882453203201294\n",
      "Training iteration 0 | loss 0.13349632918834686\n",
      "Training iteration 1 | loss 0.12161143124103546\n",
      "Training iteration 2 | loss 0.12921033799648285\n",
      "Training iteration 3 | loss 0.12595267593860626\n",
      "Training iteration 4 | loss 0.12794587016105652\n",
      "Training | Average loss 0.1276433289051056\n",
      "Training iteration 0 | loss 0.1311216652393341\n",
      "Training iteration 1 | loss 0.13859012722969055\n",
      "Training iteration 2 | loss 0.11844777315855026\n",
      "Training iteration 3 | loss 0.12595167756080627\n",
      "Training iteration 4 | loss 0.1231766790151596\n",
      "Training | Average loss 0.12745758444070815\n",
      "Training iteration 0 | loss 0.1158519759774208\n",
      "Training iteration 1 | loss 0.1439184993505478\n",
      "Training iteration 2 | loss 0.12390410155057907\n",
      "Training iteration 3 | loss 0.12873424589633942\n",
      "Training iteration 4 | loss 0.12704260647296906\n",
      "Training | Average loss 0.12789028584957124\n",
      "Training iteration 0 | loss 0.121121346950531\n",
      "Training iteration 1 | loss 0.12822838127613068\n",
      "Training iteration 2 | loss 0.12456211447715759\n",
      "Training iteration 3 | loss 0.12524890899658203\n",
      "Training iteration 4 | loss 0.1350906491279602\n",
      "Training | Average loss 0.1268502801656723\n",
      "Training iteration 0 | loss 0.1367584764957428\n",
      "Training iteration 1 | loss 0.127206951379776\n",
      "Training iteration 2 | loss 0.13235700130462646\n",
      "Training iteration 3 | loss 0.1283433586359024\n",
      "Training iteration 4 | loss 0.11579424142837524\n",
      "Training | Average loss 0.12809200584888458\n",
      "Training iteration 0 | loss 0.12581060826778412\n",
      "Training iteration 1 | loss 0.124265156686306\n",
      "Training iteration 2 | loss 0.13564874231815338\n",
      "Training iteration 3 | loss 0.12400388717651367\n",
      "Training iteration 4 | loss 0.13819535076618195\n",
      "Training | Average loss 0.12958474904298783\n",
      "Training iteration 0 | loss 0.12783409655094147\n",
      "Training iteration 1 | loss 0.12470348924398422\n",
      "Training iteration 2 | loss 0.12639033794403076\n",
      "Training iteration 3 | loss 0.12551896274089813\n",
      "Training iteration 4 | loss 0.13182760775089264\n",
      "Training | Average loss 0.12725489884614943\n",
      "Training iteration 0 | loss 0.11860489845275879\n",
      "Training iteration 1 | loss 0.13365870714187622\n",
      "Training iteration 2 | loss 0.12052904069423676\n",
      "Training iteration 3 | loss 0.12665125727653503\n",
      "Training iteration 4 | loss 0.13753660023212433\n",
      "Training | Average loss 0.12739610075950622\n",
      "Training iteration 0 | loss 0.11848508566617966\n",
      "Training iteration 1 | loss 0.1357203722000122\n",
      "Training iteration 2 | loss 0.12923645973205566\n",
      "Training iteration 3 | loss 0.13366825878620148\n",
      "Training iteration 4 | loss 0.12070717662572861\n",
      "Training | Average loss 0.12756347060203552\n",
      "Training iteration 0 | loss 0.12452481687068939\n",
      "Training iteration 1 | loss 0.12203976511955261\n",
      "Training iteration 2 | loss 0.1323738545179367\n",
      "Training iteration 3 | loss 0.1230829581618309\n",
      "Training iteration 4 | loss 0.13671916723251343\n",
      "Training | Average loss 0.1277481123805046\n",
      "Training iteration 0 | loss 0.14285586774349213\n",
      "Training iteration 1 | loss 0.12489887326955795\n",
      "Training iteration 2 | loss 0.13697309792041779\n",
      "Training iteration 3 | loss 0.12483721971511841\n",
      "Training iteration 4 | loss 0.11681579053401947\n",
      "Training | Average loss 0.12927616983652115\n",
      "Training iteration 0 | loss 0.12568038702011108\n",
      "Training iteration 1 | loss 0.12262880802154541\n",
      "Training iteration 2 | loss 0.1250281184911728\n",
      "Training iteration 3 | loss 0.13304921984672546\n",
      "Training iteration 4 | loss 0.13668371737003326\n",
      "Training | Average loss 0.1286140501499176\n",
      "Training iteration 0 | loss 0.12633255124092102\n",
      "Training iteration 1 | loss 0.13267558813095093\n",
      "Training iteration 2 | loss 0.1334313601255417\n",
      "Training iteration 3 | loss 0.12756532430648804\n",
      "Training iteration 4 | loss 0.13046859204769135\n",
      "Training | Average loss 0.1300946831703186\n",
      "Training iteration 0 | loss 0.1277349442243576\n",
      "Training iteration 1 | loss 0.12153871357440948\n",
      "Training iteration 2 | loss 0.13476015627384186\n",
      "Training iteration 3 | loss 0.13555027544498444\n",
      "Training iteration 4 | loss 0.12432486563920975\n",
      "Training | Average loss 0.12878179103136062\n",
      "Training iteration 0 | loss 0.13707616925239563\n",
      "Training iteration 1 | loss 0.12727589905261993\n",
      "Training iteration 2 | loss 0.11983434855937958\n",
      "Training iteration 3 | loss 0.13336560130119324\n",
      "Training iteration 4 | loss 0.13096077740192413\n",
      "Training | Average loss 0.1297025591135025\n",
      "Training iteration 0 | loss 0.1313156932592392\n",
      "Training iteration 1 | loss 0.13470733165740967\n",
      "Training iteration 2 | loss 0.12288559228181839\n",
      "Training iteration 3 | loss 0.13311919569969177\n",
      "Training iteration 4 | loss 0.12673993408679962\n",
      "Training | Average loss 0.12975354939699174\n",
      "Training iteration 0 | loss 0.13244831562042236\n",
      "Training iteration 1 | loss 0.13337475061416626\n",
      "Training iteration 2 | loss 0.12206510454416275\n",
      "Training iteration 3 | loss 0.12359753251075745\n",
      "Training iteration 4 | loss 0.12532463669776917\n",
      "Training | Average loss 0.12736206799745559\n",
      "Training iteration 0 | loss 0.1287531554698944\n",
      "Training iteration 1 | loss 0.11657971143722534\n",
      "Training iteration 2 | loss 0.130635067820549\n",
      "Training iteration 3 | loss 0.13901308178901672\n",
      "Training iteration 4 | loss 0.12299862504005432\n",
      "Training | Average loss 0.12759592831134797\n",
      "Training iteration 0 | loss 0.1255558729171753\n",
      "Training iteration 1 | loss 0.13747340440750122\n",
      "Training iteration 2 | loss 0.12752602994441986\n",
      "Training iteration 3 | loss 0.12235357612371445\n",
      "Training iteration 4 | loss 0.12657125294208527\n",
      "Training | Average loss 0.1278960272669792\n",
      "Training iteration 0 | loss 0.1235773116350174\n",
      "Training iteration 1 | loss 0.13378342986106873\n",
      "Training iteration 2 | loss 0.11794020235538483\n",
      "Training iteration 3 | loss 0.12918853759765625\n",
      "Training iteration 4 | loss 0.13082300126552582\n",
      "Training | Average loss 0.12706249654293061\n",
      "Training iteration 0 | loss 0.1311901956796646\n",
      "Training iteration 1 | loss 0.1385047435760498\n",
      "Training iteration 2 | loss 0.12111745029687881\n",
      "Training iteration 3 | loss 0.12110919505357742\n",
      "Training iteration 4 | loss 0.12567177414894104\n",
      "Training | Average loss 0.12751867175102233\n",
      "Training iteration 0 | loss 0.12461834400892258\n",
      "Training iteration 1 | loss 0.12727148830890656\n",
      "Training iteration 2 | loss 0.1334606558084488\n",
      "Training iteration 3 | loss 0.13081477582454681\n",
      "Training iteration 4 | loss 0.12299933284521103\n",
      "Training | Average loss 0.12783291935920715\n",
      "Training iteration 0 | loss 0.12994275987148285\n",
      "Training iteration 1 | loss 0.1209179162979126\n",
      "Training iteration 2 | loss 0.1241336539387703\n",
      "Training iteration 3 | loss 0.13646787405014038\n",
      "Training iteration 4 | loss 0.12919588387012482\n",
      "Training | Average loss 0.1281316176056862\n",
      "Training iteration 0 | loss 0.13539688289165497\n",
      "Training iteration 1 | loss 0.11716971546411514\n",
      "Training iteration 2 | loss 0.13247406482696533\n",
      "Training iteration 3 | loss 0.11923249065876007\n",
      "Training iteration 4 | loss 0.1319473683834076\n",
      "Training | Average loss 0.12724410444498063\n",
      "Training iteration 0 | loss 0.12757508456707\n",
      "Training iteration 1 | loss 0.1261272430419922\n",
      "Training iteration 2 | loss 0.1343926191329956\n",
      "Training iteration 3 | loss 0.12582029402256012\n",
      "Training iteration 4 | loss 0.13135594129562378\n",
      "Training | Average loss 0.12905423641204833\n",
      "Training iteration 0 | loss 0.1301584243774414\n",
      "Training iteration 1 | loss 0.12279023975133896\n",
      "Training iteration 2 | loss 0.13149333000183105\n",
      "Training iteration 3 | loss 0.11807427555322647\n",
      "Training iteration 4 | loss 0.13067655265331268\n",
      "Training | Average loss 0.1266385644674301\n",
      "Training iteration 0 | loss 0.13261252641677856\n",
      "Training iteration 1 | loss 0.12062330543994904\n",
      "Training iteration 2 | loss 0.12882086634635925\n",
      "Training iteration 3 | loss 0.13251888751983643\n",
      "Training iteration 4 | loss 0.12258600443601608\n",
      "Training | Average loss 0.12743231803178787\n",
      "Training iteration 0 | loss 0.11883597820997238\n",
      "Training iteration 1 | loss 0.13392265141010284\n",
      "Training iteration 2 | loss 0.13501673936843872\n",
      "Training iteration 3 | loss 0.11998963356018066\n",
      "Training iteration 4 | loss 0.12883974611759186\n",
      "Training | Average loss 0.1273209497332573\n",
      "Training iteration 0 | loss 0.13011817634105682\n",
      "Training iteration 1 | loss 0.13511189818382263\n",
      "Training iteration 2 | loss 0.13485071063041687\n",
      "Training iteration 3 | loss 0.12820032238960266\n",
      "Training iteration 4 | loss 0.11352748423814774\n",
      "Training | Average loss 0.12836171835660934\n",
      "Training iteration 0 | loss 0.12633934617042542\n",
      "Training iteration 1 | loss 0.12867456674575806\n",
      "Training iteration 2 | loss 0.12682487070560455\n",
      "Training iteration 3 | loss 0.13035987317562103\n",
      "Training iteration 4 | loss 0.1252463310956955\n",
      "Training | Average loss 0.12748899757862092\n",
      "Training iteration 0 | loss 0.12605418264865875\n",
      "Training iteration 1 | loss 0.11654986441135406\n",
      "Training iteration 2 | loss 0.12675264477729797\n",
      "Training iteration 3 | loss 0.13013094663619995\n",
      "Training iteration 4 | loss 0.1376001089811325\n",
      "Training | Average loss 0.12741754949092865\n",
      "Training iteration 0 | loss 0.1307401955127716\n",
      "Training iteration 1 | loss 0.12093271315097809\n",
      "Training iteration 2 | loss 0.1214957907795906\n",
      "Training iteration 3 | loss 0.1300341933965683\n",
      "Training iteration 4 | loss 0.13894087076187134\n",
      "Training | Average loss 0.12842875272035598\n",
      "Training iteration 0 | loss 0.12293069809675217\n",
      "Training iteration 1 | loss 0.13570640981197357\n",
      "Training iteration 2 | loss 0.12435536831617355\n",
      "Training iteration 3 | loss 0.13567540049552917\n",
      "Training iteration 4 | loss 0.12195853143930435\n",
      "Training | Average loss 0.12812528163194656\n",
      "Training iteration 0 | loss 0.11686304211616516\n",
      "Training iteration 1 | loss 0.14725308120250702\n",
      "Training iteration 2 | loss 0.11691106855869293\n",
      "Training iteration 3 | loss 0.12228558957576752\n",
      "Training iteration 4 | loss 0.13456986844539642\n",
      "Training | Average loss 0.1275765299797058\n",
      "Training iteration 0 | loss 0.12868118286132812\n",
      "Training iteration 1 | loss 0.12590868771076202\n",
      "Training iteration 2 | loss 0.12407395988702774\n",
      "Training iteration 3 | loss 0.11563748121261597\n",
      "Training iteration 4 | loss 0.13999371230602264\n",
      "Training | Average loss 0.1268590047955513\n",
      "Training iteration 0 | loss 0.12991730868816376\n",
      "Training iteration 1 | loss 0.12595714628696442\n",
      "Training iteration 2 | loss 0.13949629664421082\n",
      "Training iteration 3 | loss 0.12563692033290863\n",
      "Training iteration 4 | loss 0.11908465623855591\n",
      "Training | Average loss 0.12801846563816072\n",
      "Training iteration 0 | loss 0.12983575463294983\n",
      "Training iteration 1 | loss 0.11578905582427979\n",
      "Training iteration 2 | loss 0.1302994042634964\n",
      "Training iteration 3 | loss 0.1281086504459381\n",
      "Training iteration 4 | loss 0.1301765888929367\n",
      "Training | Average loss 0.12684189081192015\n",
      "Training iteration 0 | loss 0.1156807467341423\n",
      "Training iteration 1 | loss 0.12135712057352066\n",
      "Training iteration 2 | loss 0.1337207853794098\n",
      "Training iteration 3 | loss 0.13258811831474304\n",
      "Training iteration 4 | loss 0.13250640034675598\n",
      "Training | Average loss 0.12717063426971437\n",
      "Training iteration 0 | loss 0.13059499859809875\n",
      "Training iteration 1 | loss 0.12189839035272598\n",
      "Training iteration 2 | loss 0.13366802036762238\n",
      "Training iteration 3 | loss 0.12407299131155014\n",
      "Training iteration 4 | loss 0.12491735070943832\n",
      "Training | Average loss 0.12703035026788712\n",
      "Training iteration 0 | loss 0.11407967656850815\n",
      "Training iteration 1 | loss 0.12367554008960724\n",
      "Training iteration 2 | loss 0.12222433090209961\n",
      "Training iteration 3 | loss 0.13989047706127167\n",
      "Training iteration 4 | loss 0.13352355360984802\n",
      "Training | Average loss 0.12667871564626693\n",
      "Training iteration 0 | loss 0.1242142766714096\n",
      "Training iteration 1 | loss 0.1304905265569687\n",
      "Training iteration 2 | loss 0.13599488139152527\n",
      "Training iteration 3 | loss 0.12502862513065338\n",
      "Training iteration 4 | loss 0.12138988822698593\n",
      "Training | Average loss 0.12742363959550856\n",
      "Training iteration 0 | loss 0.13368286192417145\n",
      "Training iteration 1 | loss 0.13252416253089905\n",
      "Training iteration 2 | loss 0.12361129373311996\n",
      "Training iteration 3 | loss 0.13481445610523224\n",
      "Training iteration 4 | loss 0.11188386380672455\n",
      "Training | Average loss 0.12730332762002944\n",
      "Training iteration 0 | loss 0.12914441525936127\n",
      "Training iteration 1 | loss 0.11446251720190048\n",
      "Training iteration 2 | loss 0.1325603574514389\n",
      "Training iteration 3 | loss 0.13175958395004272\n",
      "Training iteration 4 | loss 0.12622977793216705\n",
      "Training | Average loss 0.1268313303589821\n",
      "Training iteration 0 | loss 0.11969145387411118\n",
      "Training iteration 1 | loss 0.13577188551425934\n",
      "Training iteration 2 | loss 0.13886965811252594\n",
      "Training iteration 3 | loss 0.11687619239091873\n",
      "Training iteration 4 | loss 0.12841548025608063\n",
      "Training | Average loss 0.12792493402957916\n",
      "Training iteration 0 | loss 0.1241934671998024\n",
      "Training iteration 1 | loss 0.12734097242355347\n",
      "Training iteration 2 | loss 0.12660861015319824\n",
      "Training iteration 3 | loss 0.1375029981136322\n",
      "Training iteration 4 | loss 0.11948223412036896\n",
      "Training | Average loss 0.12702565640211105\n",
      "Training iteration 0 | loss 0.13637149333953857\n",
      "Training iteration 1 | loss 0.13154327869415283\n",
      "Training iteration 2 | loss 0.1325025111436844\n",
      "Training iteration 3 | loss 0.12792089581489563\n",
      "Training iteration 4 | loss 0.1152694821357727\n",
      "Training | Average loss 0.12872153222560884\n",
      "Training iteration 0 | loss 0.12515327334403992\n",
      "Training iteration 1 | loss 0.12632593512535095\n",
      "Training iteration 2 | loss 0.13447172939777374\n",
      "Training iteration 3 | loss 0.1303674429655075\n",
      "Training iteration 4 | loss 0.12226515263319016\n",
      "Training | Average loss 0.12771670669317245\n",
      "Training iteration 0 | loss 0.12869857251644135\n",
      "Training iteration 1 | loss 0.1252947449684143\n",
      "Training iteration 2 | loss 0.1263091117143631\n",
      "Training iteration 3 | loss 0.12672406435012817\n",
      "Training iteration 4 | loss 0.13807161152362823\n",
      "Training | Average loss 0.12901962101459502\n",
      "Training iteration 0 | loss 0.1304301768541336\n",
      "Training iteration 1 | loss 0.13736112415790558\n",
      "Training iteration 2 | loss 0.12332271784543991\n",
      "Training iteration 3 | loss 0.11729568243026733\n",
      "Training iteration 4 | loss 0.13236616551876068\n",
      "Training | Average loss 0.12815517336130142\n",
      "Training iteration 0 | loss 0.12784412503242493\n",
      "Training iteration 1 | loss 0.1294786036014557\n",
      "Training iteration 2 | loss 0.1327669620513916\n",
      "Training iteration 3 | loss 0.11560362577438354\n",
      "Training iteration 4 | loss 0.13059155642986298\n",
      "Training | Average loss 0.12725697457790375\n",
      "Training iteration 0 | loss 0.12630142271518707\n",
      "Training iteration 1 | loss 0.1150587648153305\n",
      "Training iteration 2 | loss 0.13781389594078064\n",
      "Training iteration 3 | loss 0.13536584377288818\n",
      "Training iteration 4 | loss 0.12494241446256638\n",
      "Training | Average loss 0.12789646834135054\n",
      "Training iteration 0 | loss 0.13719865679740906\n",
      "Training iteration 1 | loss 0.12019021809101105\n",
      "Training iteration 2 | loss 0.11951830238103867\n",
      "Training iteration 3 | loss 0.12570929527282715\n",
      "Training iteration 4 | loss 0.13935402035713196\n",
      "Training | Average loss 0.12839409857988357\n",
      "Training iteration 0 | loss 0.133718341588974\n",
      "Training iteration 1 | loss 0.1336195468902588\n",
      "Training iteration 2 | loss 0.12699973583221436\n",
      "Training iteration 3 | loss 0.11794973164796829\n",
      "Training iteration 4 | loss 0.13361193239688873\n",
      "Training | Average loss 0.12917985767126083\n",
      "Training iteration 0 | loss 0.12349077314138412\n",
      "Training iteration 1 | loss 0.13190540671348572\n",
      "Training iteration 2 | loss 0.11598902940750122\n",
      "Training iteration 3 | loss 0.13554809987545013\n",
      "Training iteration 4 | loss 0.13493233919143677\n",
      "Training | Average loss 0.1283731296658516\n",
      "Training iteration 0 | loss 0.12965711951255798\n",
      "Training iteration 1 | loss 0.12247525155544281\n",
      "Training iteration 2 | loss 0.12353391945362091\n",
      "Training iteration 3 | loss 0.124457947909832\n",
      "Training iteration 4 | loss 0.13744916021823883\n",
      "Training | Average loss 0.12751467972993852\n",
      "Training iteration 0 | loss 0.11505090445280075\n",
      "Training iteration 1 | loss 0.13710060715675354\n",
      "Training iteration 2 | loss 0.1294926106929779\n",
      "Training iteration 3 | loss 0.12117775529623032\n",
      "Training iteration 4 | loss 0.14070220291614532\n",
      "Training | Average loss 0.12870481610298157\n",
      "Training iteration 0 | loss 0.13204741477966309\n",
      "Training iteration 1 | loss 0.12858441472053528\n",
      "Training iteration 2 | loss 0.13065478205680847\n",
      "Training iteration 3 | loss 0.11330515146255493\n",
      "Training iteration 4 | loss 0.13260520994663239\n",
      "Training | Average loss 0.12743939459323883\n",
      "Training iteration 0 | loss 0.1319335252046585\n",
      "Training iteration 1 | loss 0.12627555429935455\n",
      "Training iteration 2 | loss 0.13136067986488342\n",
      "Training iteration 3 | loss 0.13077129423618317\n",
      "Training iteration 4 | loss 0.11703350394964218\n",
      "Training | Average loss 0.12747491151094437\n",
      "Training iteration 0 | loss 0.13742658495903015\n",
      "Training iteration 1 | loss 0.13207894563674927\n",
      "Training iteration 2 | loss 0.12212531268596649\n",
      "Training iteration 3 | loss 0.12439092248678207\n",
      "Training iteration 4 | loss 0.12152283638715744\n",
      "Training | Average loss 0.1275089204311371\n",
      "Training iteration 0 | loss 0.136252298951149\n",
      "Training iteration 1 | loss 0.1136292815208435\n",
      "Training iteration 2 | loss 0.13312558829784393\n",
      "Training iteration 3 | loss 0.12806572020053864\n",
      "Training iteration 4 | loss 0.12432324886322021\n",
      "Training | Average loss 0.12707922756671905\n",
      "Training iteration 0 | loss 0.11548309028148651\n",
      "Training iteration 1 | loss 0.12571541965007782\n",
      "Training iteration 2 | loss 0.118802510201931\n",
      "Training iteration 3 | loss 0.12755753099918365\n",
      "Training iteration 4 | loss 0.14961925148963928\n",
      "Training | Average loss 0.12743556052446364\n",
      "Training iteration 0 | loss 0.13294391334056854\n",
      "Training iteration 1 | loss 0.12529583275318146\n",
      "Training iteration 2 | loss 0.12430454045534134\n",
      "Training iteration 3 | loss 0.12141644954681396\n",
      "Training iteration 4 | loss 0.1301472932100296\n",
      "Training | Average loss 0.126821605861187\n",
      "Training iteration 0 | loss 0.11533243954181671\n",
      "Training iteration 1 | loss 0.12957842648029327\n",
      "Training iteration 2 | loss 0.12710915505886078\n",
      "Training iteration 3 | loss 0.1285027116537094\n",
      "Training iteration 4 | loss 0.1388513743877411\n",
      "Training | Average loss 0.12787482142448425\n",
      "Training iteration 0 | loss 0.13679496943950653\n",
      "Training iteration 1 | loss 0.12872038781642914\n",
      "Training iteration 2 | loss 0.12782563269138336\n",
      "Training iteration 3 | loss 0.13267402350902557\n",
      "Training iteration 4 | loss 0.11058560013771057\n",
      "Training | Average loss 0.12732012271881105\n",
      "Training iteration 0 | loss 0.1231805607676506\n",
      "Training iteration 1 | loss 0.1179969534277916\n",
      "Training iteration 2 | loss 0.13585129380226135\n",
      "Training iteration 3 | loss 0.12911173701286316\n",
      "Training iteration 4 | loss 0.13558067381381989\n",
      "Training | Average loss 0.12834424376487732\n",
      "Training iteration 0 | loss 0.12954537570476532\n",
      "Training iteration 1 | loss 0.11817258596420288\n",
      "Training iteration 2 | loss 0.13214482367038727\n",
      "Training iteration 3 | loss 0.12023307383060455\n",
      "Training iteration 4 | loss 0.13468113541603088\n",
      "Training | Average loss 0.12695539891719818\n",
      "Training iteration 0 | loss 0.132261261343956\n",
      "Training iteration 1 | loss 0.11774571985006332\n",
      "Training iteration 2 | loss 0.1250755339860916\n",
      "Training iteration 3 | loss 0.1256704181432724\n",
      "Training iteration 4 | loss 0.13278837502002716\n",
      "Training | Average loss 0.1267082616686821\n",
      "Training iteration 0 | loss 0.12883849442005157\n",
      "Training iteration 1 | loss 0.11977142840623856\n",
      "Training iteration 2 | loss 0.1258956342935562\n",
      "Training iteration 3 | loss 0.1253921538591385\n",
      "Training iteration 4 | loss 0.13685576617717743\n",
      "Training | Average loss 0.12735069543123245\n",
      "Training iteration 0 | loss 0.12979355454444885\n",
      "Training iteration 1 | loss 0.1322198361158371\n",
      "Training iteration 2 | loss 0.1264403611421585\n",
      "Training iteration 3 | loss 0.12223953008651733\n",
      "Training iteration 4 | loss 0.12719103693962097\n",
      "Training | Average loss 0.12757686376571656\n",
      "Training iteration 0 | loss 0.13142770528793335\n",
      "Training iteration 1 | loss 0.11347105354070663\n",
      "Training iteration 2 | loss 0.12566828727722168\n",
      "Training iteration 3 | loss 0.14535987377166748\n",
      "Training iteration 4 | loss 0.120231032371521\n",
      "Training | Average loss 0.12723159044981003\n",
      "Training iteration 0 | loss 0.12804093956947327\n",
      "Training iteration 1 | loss 0.13546980917453766\n",
      "Training iteration 2 | loss 0.1349242627620697\n",
      "Training iteration 3 | loss 0.11426015198230743\n",
      "Training iteration 4 | loss 0.1281050592660904\n",
      "Training | Average loss 0.1281600445508957\n",
      "Training iteration 0 | loss 0.12884831428527832\n",
      "Training iteration 1 | loss 0.13582684099674225\n",
      "Training iteration 2 | loss 0.12725260853767395\n",
      "Training iteration 3 | loss 0.1228569820523262\n",
      "Training iteration 4 | loss 0.12058965861797333\n",
      "Training | Average loss 0.12707488089799882\n",
      "Training iteration 0 | loss 0.11852369457483292\n",
      "Training iteration 1 | loss 0.12476761639118195\n",
      "Training iteration 2 | loss 0.12475962191820145\n",
      "Training iteration 3 | loss 0.13276302814483643\n",
      "Training iteration 4 | loss 0.1342771351337433\n",
      "Training | Average loss 0.1270182192325592\n",
      "Training iteration 0 | loss 0.11684871464967728\n",
      "Training iteration 1 | loss 0.1254645586013794\n",
      "Training iteration 2 | loss 0.1449282467365265\n",
      "Training iteration 3 | loss 0.13052336871623993\n",
      "Training iteration 4 | loss 0.11938938498497009\n",
      "Training | Average loss 0.12743085473775864\n",
      "Training iteration 0 | loss 0.12474999576807022\n",
      "Training iteration 1 | loss 0.13990965485572815\n",
      "Training iteration 2 | loss 0.11119150370359421\n",
      "Training iteration 3 | loss 0.12552955746650696\n",
      "Training iteration 4 | loss 0.1333702802658081\n",
      "Training | Average loss 0.12695019841194152\n",
      "Training iteration 0 | loss 0.12486494332551956\n",
      "Training iteration 1 | loss 0.13438069820404053\n",
      "Training iteration 2 | loss 0.12388868629932404\n",
      "Training iteration 3 | loss 0.12858253717422485\n",
      "Training iteration 4 | loss 0.1230262741446495\n",
      "Training | Average loss 0.12694862782955169\n",
      "Training iteration 0 | loss 0.1327335089445114\n",
      "Training iteration 1 | loss 0.11425165832042694\n",
      "Training iteration 2 | loss 0.14110390841960907\n",
      "Training iteration 3 | loss 0.11357463896274567\n",
      "Training iteration 4 | loss 0.13442154228687286\n",
      "Training | Average loss 0.1272170513868332\n",
      "Training iteration 0 | loss 0.13546784222126007\n",
      "Training iteration 1 | loss 0.1238204836845398\n",
      "Training iteration 2 | loss 0.11821061372756958\n",
      "Training iteration 3 | loss 0.12046603113412857\n",
      "Training iteration 4 | loss 0.1429481953382492\n",
      "Training | Average loss 0.12818263322114945\n",
      "Training iteration 0 | loss 0.13244841992855072\n",
      "Training iteration 1 | loss 0.12312132865190506\n",
      "Training iteration 2 | loss 0.12396315485239029\n",
      "Training iteration 3 | loss 0.1219559758901596\n",
      "Training iteration 4 | loss 0.13420578837394714\n",
      "Training | Average loss 0.12713893353939057\n",
      "Training iteration 0 | loss 0.1275668889284134\n",
      "Training iteration 1 | loss 0.12372807413339615\n",
      "Training iteration 2 | loss 0.12917254865169525\n",
      "Training iteration 3 | loss 0.13467425107955933\n",
      "Training iteration 4 | loss 0.12527847290039062\n",
      "Training | Average loss 0.12808404713869095\n",
      "Training iteration 0 | loss 0.12740914523601532\n",
      "Training iteration 1 | loss 0.11986853182315826\n",
      "Training iteration 2 | loss 0.13171029090881348\n",
      "Training iteration 3 | loss 0.1259646713733673\n",
      "Training iteration 4 | loss 0.13080470263957977\n",
      "Training | Average loss 0.12715146839618682\n",
      "Training iteration 0 | loss 0.12016080319881439\n",
      "Training iteration 1 | loss 0.142685204744339\n",
      "Training iteration 2 | loss 0.1426282525062561\n",
      "Training iteration 3 | loss 0.11669404804706573\n",
      "Training iteration 4 | loss 0.1235952153801918\n",
      "Training | Average loss 0.12915270477533342\n",
      "Training iteration 0 | loss 0.12825311720371246\n",
      "Training iteration 1 | loss 0.13484252989292145\n",
      "Training iteration 2 | loss 0.11153305321931839\n",
      "Training iteration 3 | loss 0.12919475138187408\n",
      "Training iteration 4 | loss 0.13086923956871033\n",
      "Training | Average loss 0.12693853825330734\n",
      "Training iteration 0 | loss 0.1291753500699997\n",
      "Training iteration 1 | loss 0.11343907564878464\n",
      "Training iteration 2 | loss 0.13137009739875793\n",
      "Training iteration 3 | loss 0.14063270390033722\n",
      "Training iteration 4 | loss 0.12217274308204651\n",
      "Training | Average loss 0.1273579940199852\n",
      "Training iteration 0 | loss 0.11701244115829468\n",
      "Training iteration 1 | loss 0.1282007247209549\n",
      "Training iteration 2 | loss 0.13140010833740234\n",
      "Training iteration 3 | loss 0.1254887878894806\n",
      "Training iteration 4 | loss 0.13239125907421112\n",
      "Training | Average loss 0.12689866423606871\n",
      "Training iteration 0 | loss 0.1353856325149536\n",
      "Training iteration 1 | loss 0.1198033019900322\n",
      "Training iteration 2 | loss 0.14410360157489777\n",
      "Training iteration 3 | loss 0.12371640652418137\n",
      "Training iteration 4 | loss 0.11602317541837692\n",
      "Training | Average loss 0.12780642360448838\n",
      "Training iteration 0 | loss 0.12187263369560242\n",
      "Training iteration 1 | loss 0.12608224153518677\n",
      "Training iteration 2 | loss 0.13522186875343323\n",
      "Training iteration 3 | loss 0.11357520520687103\n",
      "Training iteration 4 | loss 0.13760623335838318\n",
      "Training | Average loss 0.12687163650989533\n",
      "Training iteration 0 | loss 0.13577407598495483\n",
      "Training iteration 1 | loss 0.12145650386810303\n",
      "Training iteration 2 | loss 0.12082277238368988\n",
      "Training iteration 3 | loss 0.13820695877075195\n",
      "Training iteration 4 | loss 0.1253664195537567\n",
      "Training | Average loss 0.12832534611225127\n",
      "Training iteration 0 | loss 0.13125020265579224\n",
      "Training iteration 1 | loss 0.12833166122436523\n",
      "Training iteration 2 | loss 0.12205873429775238\n",
      "Training iteration 3 | loss 0.1192195788025856\n",
      "Training iteration 4 | loss 0.13651135563850403\n",
      "Training | Average loss 0.1274743065237999\n",
      "Training iteration 0 | loss 0.1375521719455719\n",
      "Training iteration 1 | loss 0.12398989498615265\n",
      "Training iteration 2 | loss 0.11918140202760696\n",
      "Training iteration 3 | loss 0.13302361965179443\n",
      "Training iteration 4 | loss 0.12346702069044113\n",
      "Training | Average loss 0.12744282186031342\n",
      "Training iteration 0 | loss 0.12082691490650177\n",
      "Training iteration 1 | loss 0.117157943546772\n",
      "Training iteration 2 | loss 0.13000932335853577\n",
      "Training iteration 3 | loss 0.1309196949005127\n",
      "Training iteration 4 | loss 0.13960416615009308\n",
      "Training | Average loss 0.12770360857248306\n",
      "Training iteration 0 | loss 0.1339961588382721\n",
      "Training iteration 1 | loss 0.13173259794712067\n",
      "Training iteration 2 | loss 0.13589182496070862\n",
      "Training iteration 3 | loss 0.12281472980976105\n",
      "Training iteration 4 | loss 0.11368125677108765\n",
      "Training | Average loss 0.12762331366539\n",
      "Training iteration 0 | loss 0.14574795961380005\n",
      "Training iteration 1 | loss 0.1278301626443863\n",
      "Training iteration 2 | loss 0.11259094625711441\n",
      "Training iteration 3 | loss 0.13624772429466248\n",
      "Training iteration 4 | loss 0.12480247765779495\n",
      "Training | Average loss 0.12944385409355164\n",
      "Training iteration 0 | loss 0.13049539923667908\n",
      "Training iteration 1 | loss 0.1259917914867401\n",
      "Training iteration 2 | loss 0.12056560814380646\n",
      "Training iteration 3 | loss 0.12984338402748108\n",
      "Training iteration 4 | loss 0.13232994079589844\n",
      "Training | Average loss 0.12784522473812104\n",
      "Training iteration 0 | loss 0.13605275750160217\n",
      "Training iteration 1 | loss 0.11455805599689484\n",
      "Training iteration 2 | loss 0.1260787397623062\n",
      "Training iteration 3 | loss 0.12379038333892822\n",
      "Training iteration 4 | loss 0.13724467158317566\n",
      "Training | Average loss 0.12754492163658143\n",
      "Training iteration 0 | loss 0.12281402200460434\n",
      "Training iteration 1 | loss 0.13094986975193024\n",
      "Training iteration 2 | loss 0.1267865151166916\n",
      "Training iteration 3 | loss 0.13710038363933563\n",
      "Training iteration 4 | loss 0.12502561509609222\n",
      "Training | Average loss 0.1285352811217308\n",
      "Training iteration 0 | loss 0.117067851126194\n",
      "Training iteration 1 | loss 0.13542380928993225\n",
      "Training iteration 2 | loss 0.12545377016067505\n",
      "Training iteration 3 | loss 0.12487655878067017\n",
      "Training iteration 4 | loss 0.13194964826107025\n",
      "Training | Average loss 0.12695432752370833\n",
      "Training iteration 0 | loss 0.13377737998962402\n",
      "Training iteration 1 | loss 0.12439067661762238\n",
      "Training iteration 2 | loss 0.12910118699073792\n",
      "Training iteration 3 | loss 0.12145046144723892\n",
      "Training iteration 4 | loss 0.12846453487873077\n",
      "Training | Average loss 0.1274368479847908\n",
      "Training iteration 0 | loss 0.13932278752326965\n",
      "Training iteration 1 | loss 0.12654371559619904\n",
      "Training iteration 2 | loss 0.12192109227180481\n",
      "Training iteration 3 | loss 0.12933075428009033\n",
      "Training iteration 4 | loss 0.12418962270021439\n",
      "Training | Average loss 0.12826159447431565\n",
      "Training iteration 0 | loss 0.11919420212507248\n",
      "Training iteration 1 | loss 0.12720169126987457\n",
      "Training iteration 2 | loss 0.1329544633626938\n",
      "Training iteration 3 | loss 0.13168495893478394\n",
      "Training iteration 4 | loss 0.12733645737171173\n",
      "Training | Average loss 0.1276743546128273\n",
      "Training iteration 0 | loss 0.1146639958024025\n",
      "Training iteration 1 | loss 0.12310333549976349\n",
      "Training iteration 2 | loss 0.13240879774093628\n",
      "Training iteration 3 | loss 0.13887371122837067\n",
      "Training iteration 4 | loss 0.12502288818359375\n",
      "Training | Average loss 0.12681454569101333\n",
      "Training iteration 0 | loss 0.1409987360239029\n",
      "Training iteration 1 | loss 0.13408440351486206\n",
      "Training iteration 2 | loss 0.12385077774524689\n",
      "Training iteration 3 | loss 0.12342610955238342\n",
      "Training iteration 4 | loss 0.11162114888429642\n",
      "Training | Average loss 0.12679623514413835\n",
      "Training iteration 0 | loss 0.13185808062553406\n",
      "Training iteration 1 | loss 0.12672850489616394\n",
      "Training iteration 2 | loss 0.13790827989578247\n",
      "Training iteration 3 | loss 0.12942972779273987\n",
      "Training iteration 4 | loss 0.11057384312152863\n",
      "Training | Average loss 0.1272996872663498\n",
      "Training iteration 0 | loss 0.12885405123233795\n",
      "Training iteration 1 | loss 0.12321798503398895\n",
      "Training iteration 2 | loss 0.1235223338007927\n",
      "Training iteration 3 | loss 0.1349378377199173\n",
      "Training iteration 4 | loss 0.12338229268789291\n",
      "Training | Average loss 0.12678290009498597\n",
      "Training iteration 0 | loss 0.12112484872341156\n",
      "Training iteration 1 | loss 0.13216757774353027\n",
      "Training iteration 2 | loss 0.12779469788074493\n",
      "Training iteration 3 | loss 0.12432508915662766\n",
      "Training iteration 4 | loss 0.1295008361339569\n",
      "Training | Average loss 0.12698260992765426\n",
      "Training iteration 0 | loss 0.12831929326057434\n",
      "Training iteration 1 | loss 0.11667938530445099\n",
      "Training iteration 2 | loss 0.13620513677597046\n",
      "Training iteration 3 | loss 0.11845783144235611\n",
      "Training iteration 4 | loss 0.13682857155799866\n",
      "Training | Average loss 0.1272980436682701\n",
      "Training iteration 0 | loss 0.12012878805398941\n",
      "Training iteration 1 | loss 0.12615881860256195\n",
      "Training iteration 2 | loss 0.13226963579654694\n",
      "Training iteration 3 | loss 0.13339613378047943\n",
      "Training iteration 4 | loss 0.1294342428445816\n",
      "Training | Average loss 0.12827752381563187\n",
      "Training iteration 0 | loss 0.12490683048963547\n",
      "Training iteration 1 | loss 0.12160864472389221\n",
      "Training iteration 2 | loss 0.12606821954250336\n",
      "Training iteration 3 | loss 0.1282736361026764\n",
      "Training iteration 4 | loss 0.13500289618968964\n",
      "Training | Average loss 0.1271720454096794\n",
      "Training iteration 0 | loss 0.1260451227426529\n",
      "Training iteration 1 | loss 0.1281619369983673\n",
      "Training iteration 2 | loss 0.13152825832366943\n",
      "Training iteration 3 | loss 0.1198330968618393\n",
      "Training iteration 4 | loss 0.12708449363708496\n",
      "Training | Average loss 0.12653058171272277\n",
      "Training iteration 0 | loss 0.1164175346493721\n",
      "Training iteration 1 | loss 0.11083931475877762\n",
      "Training iteration 2 | loss 0.12926633656024933\n",
      "Training iteration 3 | loss 0.12587563693523407\n",
      "Training iteration 4 | loss 0.1527998149394989\n",
      "Training | Average loss 0.12703972756862641\n",
      "Training iteration 0 | loss 0.1276639848947525\n",
      "Training iteration 1 | loss 0.12101556360721588\n",
      "Training iteration 2 | loss 0.13330300152301788\n",
      "Training iteration 3 | loss 0.11791306734085083\n",
      "Training iteration 4 | loss 0.13940173387527466\n",
      "Training | Average loss 0.12785947024822236\n",
      "Training iteration 0 | loss 0.1276175081729889\n",
      "Training iteration 1 | loss 0.1199902594089508\n",
      "Training iteration 2 | loss 0.13433119654655457\n",
      "Training iteration 3 | loss 0.1314534991979599\n",
      "Training iteration 4 | loss 0.12049778550863266\n",
      "Training | Average loss 0.12677804976701737\n",
      "Training iteration 0 | loss 0.1251814216375351\n",
      "Training iteration 1 | loss 0.11999080330133438\n",
      "Training iteration 2 | loss 0.12878714501857758\n",
      "Training iteration 3 | loss 0.12982892990112305\n",
      "Training iteration 4 | loss 0.13154932856559753\n",
      "Training | Average loss 0.12706752568483354\n",
      "Training iteration 0 | loss 0.12662068009376526\n",
      "Training iteration 1 | loss 0.13159829378128052\n",
      "Training iteration 2 | loss 0.12820056080818176\n",
      "Training iteration 3 | loss 0.13054946064949036\n",
      "Training iteration 4 | loss 0.12094492465257645\n",
      "Training | Average loss 0.12758278399705886\n",
      "Training iteration 0 | loss 0.14143803715705872\n",
      "Training iteration 1 | loss 0.11928611993789673\n",
      "Training iteration 2 | loss 0.12638913094997406\n",
      "Training iteration 3 | loss 0.11477276682853699\n",
      "Training iteration 4 | loss 0.13112671673297882\n",
      "Training | Average loss 0.12660255432128906\n",
      "Training iteration 0 | loss 0.12162467837333679\n",
      "Training iteration 1 | loss 0.12251805514097214\n",
      "Training iteration 2 | loss 0.14616331458091736\n",
      "Training iteration 3 | loss 0.1183297410607338\n",
      "Training iteration 4 | loss 0.12585899233818054\n",
      "Training | Average loss 0.12689895629882814\n",
      "Training iteration 0 | loss 0.12688350677490234\n",
      "Training iteration 1 | loss 0.12199551612138748\n",
      "Training iteration 2 | loss 0.1325736939907074\n",
      "Training iteration 3 | loss 0.1251559555530548\n",
      "Training iteration 4 | loss 0.12631477415561676\n",
      "Training | Average loss 0.12658468931913375\n",
      "Training iteration 0 | loss 0.12684552371501923\n",
      "Training iteration 1 | loss 0.1355466991662979\n",
      "Training iteration 2 | loss 0.12233950942754745\n",
      "Training iteration 3 | loss 0.1290288269519806\n",
      "Training iteration 4 | loss 0.12312322109937668\n",
      "Training | Average loss 0.12737675607204438\n",
      "Training iteration 0 | loss 0.13253897428512573\n",
      "Training iteration 1 | loss 0.12485985457897186\n",
      "Training iteration 2 | loss 0.1311209350824356\n",
      "Training iteration 3 | loss 0.13383913040161133\n",
      "Training iteration 4 | loss 0.11486503481864929\n",
      "Training | Average loss 0.12744478583335878\n",
      "Training iteration 0 | loss 0.14388856291770935\n",
      "Training iteration 1 | loss 0.12187526375055313\n",
      "Training iteration 2 | loss 0.1130993515253067\n",
      "Training iteration 3 | loss 0.12529775500297546\n",
      "Training iteration 4 | loss 0.1309376209974289\n",
      "Training | Average loss 0.1270197108387947\n",
      "Training iteration 0 | loss 0.11534339934587479\n",
      "Training iteration 1 | loss 0.1295582801103592\n",
      "Training iteration 2 | loss 0.13350729644298553\n",
      "Training iteration 3 | loss 0.118616983294487\n",
      "Training iteration 4 | loss 0.13620620965957642\n",
      "Training | Average loss 0.1266464337706566\n",
      "Training iteration 0 | loss 0.1206776574254036\n",
      "Training iteration 1 | loss 0.1258954256772995\n",
      "Training iteration 2 | loss 0.13165566325187683\n",
      "Training iteration 3 | loss 0.11811584234237671\n",
      "Training iteration 4 | loss 0.14252671599388123\n",
      "Training | Average loss 0.12777426093816757\n",
      "Training iteration 0 | loss 0.11728280037641525\n",
      "Training iteration 1 | loss 0.14241762459278107\n",
      "Training iteration 2 | loss 0.11821793764829636\n",
      "Training iteration 3 | loss 0.13151802122592926\n",
      "Training iteration 4 | loss 0.12659995257854462\n",
      "Training | Average loss 0.1272072672843933\n",
      "Training iteration 0 | loss 0.1261691451072693\n",
      "Training iteration 1 | loss 0.1250796914100647\n",
      "Training iteration 2 | loss 0.12628379464149475\n",
      "Training iteration 3 | loss 0.12619081139564514\n",
      "Training iteration 4 | loss 0.12927432358264923\n",
      "Training | Average loss 0.1265995532274246\n",
      "Training iteration 0 | loss 0.1361568421125412\n",
      "Training iteration 1 | loss 0.12116222828626633\n",
      "Training iteration 2 | loss 0.11315707117319107\n",
      "Training iteration 3 | loss 0.13860994577407837\n",
      "Training iteration 4 | loss 0.13326628506183624\n",
      "Training | Average loss 0.12847047448158264\n",
      "Training iteration 0 | loss 0.11540842801332474\n",
      "Training iteration 1 | loss 0.13426387310028076\n",
      "Training iteration 2 | loss 0.1333559900522232\n",
      "Training iteration 3 | loss 0.11990606039762497\n",
      "Training iteration 4 | loss 0.13970720767974854\n",
      "Training | Average loss 0.12852831184864044\n",
      "Training iteration 0 | loss 0.13601593673229218\n",
      "Training iteration 1 | loss 0.11978054791688919\n",
      "Training iteration 2 | loss 0.13406440615653992\n",
      "Training iteration 3 | loss 0.11436950415372849\n",
      "Training iteration 4 | loss 0.12954585254192352\n",
      "Training | Average loss 0.12675524950027467\n",
      "Training iteration 0 | loss 0.13170868158340454\n",
      "Training iteration 1 | loss 0.11796656250953674\n",
      "Training iteration 2 | loss 0.1297895312309265\n",
      "Training iteration 3 | loss 0.13023371994495392\n",
      "Training iteration 4 | loss 0.12568256258964539\n",
      "Training | Average loss 0.1270762115716934\n",
      "Training iteration 0 | loss 0.13673216104507446\n",
      "Training iteration 1 | loss 0.11990971863269806\n",
      "Training iteration 2 | loss 0.1135563850402832\n",
      "Training iteration 3 | loss 0.12831971049308777\n",
      "Training iteration 4 | loss 0.14464464783668518\n",
      "Training | Average loss 0.12863252460956573\n",
      "Training iteration 0 | loss 0.12858615815639496\n",
      "Training iteration 1 | loss 0.1182096004486084\n",
      "Training iteration 2 | loss 0.13767841458320618\n",
      "Training iteration 3 | loss 0.12453070282936096\n",
      "Training iteration 4 | loss 0.12577824294567108\n",
      "Training | Average loss 0.12695662379264833\n",
      "Training iteration 0 | loss 0.1355593502521515\n",
      "Training iteration 1 | loss 0.1272508054971695\n",
      "Training iteration 2 | loss 0.1166163757443428\n",
      "Training iteration 3 | loss 0.13868482410907745\n",
      "Training iteration 4 | loss 0.11825790256261826\n",
      "Training | Average loss 0.1272738516330719\n",
      "Training iteration 0 | loss 0.12637944519519806\n",
      "Training iteration 1 | loss 0.126206636428833\n",
      "Training iteration 2 | loss 0.1267462819814682\n",
      "Training iteration 3 | loss 0.13113847374916077\n",
      "Training iteration 4 | loss 0.1226382702589035\n",
      "Training | Average loss 0.1266218215227127\n",
      "Training iteration 0 | loss 0.12390165776014328\n",
      "Training iteration 1 | loss 0.129662424325943\n",
      "Training iteration 2 | loss 0.11976782977581024\n",
      "Training iteration 3 | loss 0.12044631689786911\n",
      "Training iteration 4 | loss 0.13912738859653473\n",
      "Training | Average loss 0.12658112347126008\n",
      "Training iteration 0 | loss 0.12509819865226746\n",
      "Training iteration 1 | loss 0.12731046974658966\n",
      "Training iteration 2 | loss 0.11728939414024353\n",
      "Training iteration 3 | loss 0.13501319289207458\n",
      "Training iteration 4 | loss 0.1294161081314087\n",
      "Training | Average loss 0.1268254727125168\n",
      "Training iteration 0 | loss 0.12712450325489044\n",
      "Training iteration 1 | loss 0.12054158002138138\n",
      "Training iteration 2 | loss 0.13348388671875\n",
      "Training iteration 3 | loss 0.12881343066692352\n",
      "Training iteration 4 | loss 0.12462184578180313\n",
      "Training | Average loss 0.1269170492887497\n",
      "Training iteration 0 | loss 0.13535672426223755\n",
      "Training iteration 1 | loss 0.11587879806756973\n",
      "Training iteration 2 | loss 0.12572361528873444\n",
      "Training iteration 3 | loss 0.13683566451072693\n",
      "Training iteration 4 | loss 0.11907945573329926\n",
      "Training | Average loss 0.12657485157251358\n",
      "Training iteration 0 | loss 0.11495929211378098\n",
      "Training iteration 1 | loss 0.12429388612508774\n",
      "Training iteration 2 | loss 0.13210217654705048\n",
      "Training iteration 3 | loss 0.11915493756532669\n",
      "Training iteration 4 | loss 0.141370490193367\n",
      "Training | Average loss 0.1263761565089226\n",
      "Training iteration 0 | loss 0.11070884019136429\n",
      "Training iteration 1 | loss 0.12037699669599533\n",
      "Training iteration 2 | loss 0.13001814484596252\n",
      "Training iteration 3 | loss 0.14781154692173004\n",
      "Training iteration 4 | loss 0.12368234992027283\n",
      "Training | Average loss 0.126519575715065\n",
      "Training iteration 0 | loss 0.1279224157333374\n",
      "Training iteration 1 | loss 0.12429548054933548\n",
      "Training iteration 2 | loss 0.11570379137992859\n",
      "Training iteration 3 | loss 0.13125060498714447\n",
      "Training iteration 4 | loss 0.13313017785549164\n",
      "Training | Average loss 0.1264604941010475\n",
      "Training iteration 0 | loss 0.12750214338302612\n",
      "Training iteration 1 | loss 0.13047169148921967\n",
      "Training iteration 2 | loss 0.12054017186164856\n",
      "Training iteration 3 | loss 0.12813405692577362\n",
      "Training iteration 4 | loss 0.13235612213611603\n",
      "Training | Average loss 0.1278008371591568\n",
      "Training iteration 0 | loss 0.1278068870306015\n",
      "Training iteration 1 | loss 0.11832711100578308\n",
      "Training iteration 2 | loss 0.13636617362499237\n",
      "Training iteration 3 | loss 0.13405905663967133\n",
      "Training iteration 4 | loss 0.11726934462785721\n",
      "Training | Average loss 0.1267657145857811\n",
      "Training iteration 0 | loss 0.12245351821184158\n",
      "Training iteration 1 | loss 0.1303270310163498\n",
      "Training iteration 2 | loss 0.12747722864151\n",
      "Training iteration 3 | loss 0.11962177604436874\n",
      "Training iteration 4 | loss 0.1331177055835724\n",
      "Training | Average loss 0.1265994518995285\n"
     ]
    }
   ],
   "source": [
    "from dig.ggraph.method import GraphDF, GraphAF\n",
    "import json\n",
    "from dig.ggraph.dataset import ZINC250k\n",
    "from torch_geometric.loader import DenseDataLoader\n",
    "import numpy as np\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "# torch.cuda.set_device(4)\n",
    "\n",
    "runner = GraphDF()\n",
    "\n",
    "config_dict =  {\n",
    "        \"max_size\": 20,\n",
    "        \"edge_unroll\": 20,\n",
    "        \"node_dim\": 10,\n",
    "        \"bond_dim\": 2,\n",
    "        \"num_flow_layer\": 12,\n",
    "        \"num_rgcn_layer\": 3,\n",
    "        \"nhid\": 128,\n",
    "        \"nout\": 128,\n",
    "        \"deq_coeff\": 0.9,\n",
    "        \"st_type\": \"exp\",\n",
    "        \"use_gpu\": True,\n",
    "        \"use_df\": False\n",
    "    }\n",
    "\n",
    "\n",
    "runner.train_rand_gen(loader=data_train_loader, lr = 0.001, wd = 0, max_epochs = 1000, \n",
    "                        model_conf_dict = config_dict, save_interval = 200, save_dir = 'community_test_DF')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Generating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected parameter logits (Tensor of shape (2,)) of distribution Categorical(logits: torch.Size([2])) to satisfy the constraint IndependentConstraint(Real(), 1), but found invalid values:\ntensor([nan, nan], device='cuda:0')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_31860/4226769650.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0mn_mols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m mols, _ = runner.run_rand_gen(model_conf_dict = config_dict, checkpoint_path=ckpt_path,\n\u001b[0;32m---> 31\u001b[0;31m                             n_mols=n_mols, atomic_num_list = atomic_list)\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/GraphAF_env/lib/python3.7/site-packages/dig/ggraph/method/GraphDF/graphdf.py\u001b[0m in \u001b[0;36mrun_rand_gen\u001b[0;34m(self, model_conf_dict, checkpoint_path, n_mols, num_min_node, num_max_node, temperature, atomic_num_list)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mcnt_mol\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mn_mols\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m             \u001b[0mmol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mno_resample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_atoms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0matom_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0matomic_num_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_atoms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_min_node\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_atoms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_max_node\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtemperature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnum_atoms\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mnum_min_node\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0mcnt_mol\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/GraphAF_env/lib/python3.7/site-packages/dig/ggraph/method/GraphDF/model/graphflow.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, atom_list, temperature, min_atoms, max_atoms)\u001b[0m\n\u001b[1;32m    121\u001b[0m                     \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mvalid\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minvalid_bond_type_set\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m3\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mresample_edge\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# haven't sampled all possible bond type or is not stuck in the loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m                             \u001b[0mprior_edge_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOneHotCategorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0medge_dis\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mtemperature\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m                             \u001b[0mlatent_edge\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprior_edge_dist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m                             \u001b[0mlatent_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlatent_edge\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/GraphAF_env/lib/python3.7/site-packages/torch/distributions/one_hot_categorical.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, probs, logits, validate_args)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_args\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_categorical\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCategorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0mbatch_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_categorical\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mevent_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_categorical\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/GraphAF_env/lib/python3.7/site-packages/torch/distributions/categorical.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, probs, logits, validate_args)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_events\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_param\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mbatch_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_param\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_param\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndimension\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCategorical\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_args\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mexpand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_instance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/GraphAF_env/lib/python3.7/site-packages/torch/distributions/distribution.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch_shape, event_shape, validate_args)\u001b[0m\n\u001b[1;32m     54\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mvalid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m                     raise ValueError(\n\u001b[0;32m---> 56\u001b[0;31m                         \u001b[0;34mf\"Expected parameter {param} \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m                         \u001b[0;34mf\"({type(value).__name__} of shape {tuple(value.shape)}) \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m                         \u001b[0;34mf\"of distribution {repr(self)} \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected parameter logits (Tensor of shape (2,)) of distribution Categorical(logits: torch.Size([2])) to satisfy the constraint IndependentConstraint(Real(), 1), but found invalid values:\ntensor([nan, nan], device='cuda:0')"
     ]
    }
   ],
   "source": [
    "from rdkit import RDLogger\n",
    "from dig.ggraph.method import GraphDF, GraphAF\n",
    "import json\n",
    "from dig.ggraph.dataset import ZINC250k\n",
    "from torch_geometric.loader import DenseDataLoader\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "runner = GraphDF()\n",
    "\n",
    "RDLogger.DisableLog('rdApp.*')\n",
    "ckpt_path = 'community_test_DF/rand_gen_ckpt_1000.pth'\n",
    "config_dict =  {\n",
    "        \"max_size\": 20,\n",
    "        \"edge_unroll\": 20,\n",
    "        \"node_dim\": 10,\n",
    "        \"bond_dim\": 2,\n",
    "        \"num_flow_layer\": 12,\n",
    "        \"num_rgcn_layer\": 3,\n",
    "        \"nhid\": 128,\n",
    "        \"nout\": 128,\n",
    "        \"deq_coeff\": 0.9,\n",
    "        \"st_type\": \"exp\",\n",
    "        \"use_gpu\": True,\n",
    "        \"use_df\": False\n",
    "    }\n",
    "atomic_list = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "# atomic_list = [1]\n",
    "n_mols = 10\n",
    "mols, _ = runner.run_rand_gen(model_conf_dict = config_dict, checkpoint_path=ckpt_path,\n",
    "                            n_mols=n_mols, atomic_num_list = atomic_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('GraphAF_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bbd2a620df9ef3ec3d33f10b9d42aecc9820701e9f4b46703644c7cadf9a0f79"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
