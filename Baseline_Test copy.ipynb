{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments on Graph-Generative-Models\n",
    "In this notebook, we aim to evluate the performance of \"GDSS\" proposed in \"Score-based Generative Modeling of Graphs via the System of Stochastic Differential Equations\" (https://arxiv.org/pdf/2202.02514.pdf). The baseline model is tested on 3 datasets (Grid, Protein, 3D Point Cloud) and measured under 4 metrics (degree, clustering, orbit, spectral).\n",
    "\n",
    "It should be noted that we adopt the same datasets presets as in \"Efficient Graph Generation with Graph Recurrent Attention Networks\" (https://arxiv.org/pdf/1910.00760.pdf), where:\n",
    "- Grid: 100 graphs are generated with $100\\leq |V| \\leq 400$;\n",
    "- Protein: 918 graphs are generated with $100\\leq |V| \\leq 500$;\n",
    "- 3D Point-Cloud (FirstMM-DB): 41 graphs are generated with $\\bar{|V|} > 1000$\n",
    "\n",
    "Following the experimental setting as in \"GraphRNN: Generating Realistic Graphs with Deep Auto-regressive Models\" (https://arxiv.org/abs/1802.08773), we conduct a 80\\%-20\\% split of the graph samples in each dataset. Then we generate the same size of graph samples as the test dataset and harness the maximum mean discrepancy (MMD) to evaluate the generative graph distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment on GDSS\n",
    "Here we immigrate the original terminal-executable GDSS codes into the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Change current directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "path = \"./GDSS_o/\"\n",
    "os.chdir(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt\n",
    "%conda install -c conda-forge rdkit=2020.09.1.0\n",
    "!yes | pip install git+https://github.com/fabriziocosta/EDeN.git --user!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Assign dataset and seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'grid'\n",
    "dataset = 'DD'\n",
    "dataset = 'community_small'\n",
    "dataset = 'caveman'\n",
    "# dataset = 'FIRSTMM_DB'\n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Clear cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: GPUtil in /nethome/hsun409/anaconda3/envs/GDSS_env/lib/python3.7/site-packages (1.4.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nethome/hsun409/anaconda3/envs/GDSS_env/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial GPU Usage\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 26% | 65% |\n",
      "|  1 |  1% | 82% |\n",
      "|  2 | 13% | 43% |\n",
      "|  3 |  7% | 60% |\n",
      "|  4 |  6% | 62% |\n",
      "|  5 |  6% | 89% |\n",
      "|  6 |  0% | 90% |\n",
      "|  7 |  0% | 95% |\n",
      "GPU Usage after emptying the cache\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  0% | 66% |\n",
      "|  1 |  0% | 82% |\n",
      "|  2 |  0% | 43% |\n",
      "|  3 |  6% | 60% |\n",
      "|  4 |  5% | 62% |\n",
      "|  5 |  1% | 89% |\n",
      "|  6 |  0% | 90% |\n",
      "|  7 |  0% | 95% |\n"
     ]
    }
   ],
   "source": [
    "%pip install GPUtil\n",
    "\n",
    "import torch\n",
    "from GPUtil import showUtilization as gpu_usage\n",
    "from numba import cuda\n",
    "\n",
    "def free_gpu_cache():\n",
    "    print(\"Initial GPU Usage\")\n",
    "    gpu_usage()                             \n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    cuda.select_device(0)\n",
    "    cuda.close()\n",
    "    cuda.select_device(0)\n",
    "\n",
    "    print(\"GPU Usage after emptying the cache\")\n",
    "    gpu_usage()\n",
    "\n",
    "free_gpu_cache()                           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Generate dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading graph dataset: FIRSTMM_DB\n",
      "56468\n",
      "126038\n",
      "Graphs loaded, total num: 24\n",
      "FIRSTMM_DB 24\n",
      "995\n"
     ]
    }
   ],
   "source": [
    "!python data/data_generators.py --dataset $dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train the GDSS model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import argparse\n",
    "import time\n",
    "from parsers.config import get_config\n",
    "from trainer import Trainer\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"6,7\"\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "ts = time.strftime('%b%d-%H:%M:%S', time.gmtime())\n",
    "config = get_config(dataset, seed)\n",
    "trainer = Trainer(config) \n",
    "ckpt = trainer.train(ts)\n",
    "if 'sample' in config.keys():\n",
    "    config.ckpt = ckpt\n",
    "    sampler = Sampler(config) \n",
    "    sampler.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Generate new graphs by the trained GDSS model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 caveman caveman_5000\n",
      "Test size: 40\n",
      "./checkpoints/caveman/caveman_5000.pth loaded\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Make Directory caveman/test in Logs\n",
      "caveman_5000-sample18:20:01\n",
      "caveman_5000-sample18:20:01\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[caveman]   init=deg (10)   seed=42   batch_size=40\n",
      "----------------------------------------------------------------------------------------------------\n",
      "lr=0.01 schedule=True ema=0.999 epochs=5000 reduce=False eps=1e-05\n",
      "(ScoreNetworkX)+(ScoreNetworkA=GCN,4)   : depth=3 adim=32 nhid=32 layers=5 linears=2 c=(2 8 4)\n",
      "(x:VP)=(0.10, 1.00) N=1000 (adj:VP)=(0.10, 1.00) N=1000\n",
      "----------------------------------------------------------------------------------------------------\n",
      "(Euler)+(Langevin): eps=0.0001 denoise=True ema=False || snr=0.05 seps=0.7 n_steps=1 \n",
      "----------------------------------------------------------------------------------------------------\n",
      "GEN SEED: 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Round 0 : 583.78s\n",
      "\u001b[91mdegree   \u001b[0m : \u001b[94m0.052101\u001b[0m\n",
      "\u001b[91mcluster  \u001b[0m : \u001b[94m0.035703\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "path = \"./GDSS_o/\"\n",
    "os.chdir(path)\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,2,3,4,5\"\n",
    "\n",
    "import torch\n",
    "import argparse\n",
    "import time\n",
    "from parsers.config import get_config\n",
    "from trainer import Trainer\n",
    "from sampler import Sampler, Sampler_mol\n",
    "from evaluation.stats import eval_graph_list\n",
    "from evaluation.mmd import gaussian, gaussian_emd\n",
    "from data.data_generators import load_dataset\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# dataset = 'caveman'\n",
    "# dataset = 'cora'\n",
    "# dataset = 'community_small'\n",
    "# dataset = 'breast'\n",
    "# dataset = 'grid'\n",
    "\n",
    "datasets = ['caveman', 'cora', 'breast']\n",
    "\n",
    "\n",
    "# ckpt = 'caveman_5000'\n",
    "# ckpt = 'cora_5000'\n",
    "# ckpt = 'gdss_community_small'\n",
    "# ckpt = 'gdss_grid'\n",
    "ckpts = ['caveman_5000', 'cora_5000', 'breast_5000']\n",
    "ckpts = ['caveman_5000']\n",
    "seed = 42\n",
    "test_split = 0.2\n",
    "\n",
    "GEN_seeds = [11,12,13,14,15]\n",
    "for gen_seed in GEN_seeds:\n",
    "    for dataset, ckpt in zip(datasets, ckpts):\n",
    "        # if dataset == 'caveman' and gen_seed == 13:\n",
    "        #     continue\n",
    "        # if dataset == 'cora' and gen_seed == 12:\n",
    "        #     continue\n",
    "        # if dataset == 'breast' and gen_seed == 13:\n",
    "        #     continue\n",
    "        print(gen_seed, dataset, ckpt)\n",
    "        graph_list = load_dataset(data_dir='data', file_name=dataset)\n",
    "        if dataset == 'cora':\n",
    "            print('for cora, only take first 200 samples')\n",
    "            graph_list = graph_list[:200].copy()\n",
    "        test_size = int(test_split * len(graph_list))\n",
    "        print('Test size:', test_size)\n",
    "\n",
    "        config = get_config(dataset, seed)\n",
    "        config.sample.seed = gen_seed\n",
    "        config.ckpt = ckpt\n",
    "        sampler = Sampler(config) \n",
    "        sampler.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import networkx as nx\n",
    "caveman_filenames = ['caveman_5000-sample18:20:01']\n",
    "max_degree = 7\n",
    "percentage_per_seed = []\n",
    "for caveman_file in caveman_filenames:\n",
    "    save_dir = './samples/pkl/caveman/test/' + caveman_file\n",
    "    with open(save_dir, 'rb') as f:\n",
    "        graph_list = pickle.load(f)\n",
    "    degrees = [list(g.degree().values()) for g in graph_list]\n",
    "    percentage_per_seed.append((max(degrees)>7)/len(degrees))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.018696  0.048331  0.006261  0.052933  546.804000]\n",
      "[ 0.160796  0.376346  0.187337  0.056069  410.404000]\n",
      "[ 0.112948  0.015744  0.001897  0.120352  820.944000]\n",
      "[ 0.016804  0.025670  0.003396  0.000942  19.820961]\n",
      "[ 0.011558  0.053710  0.004920  0.001155  13.397731]\n",
      "[ 0.008225  0.006769  0.000725  0.000290  22.578371]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "caveman_data = np.array(   [[0.052101, 0.035703, 0.008728, 0.052592, 543.10],\n",
    "                            [0.009252, 0.052657, 0.011149, 0.054503, 567.02],\n",
    "                            [0.013782, 0.096422, 0.003810, 0.053426, 514.05],\n",
    "                            [0.008394, 0.025967, 0.001613, 0.052327, 541.94],\n",
    "                            [0.009951, 0.030907, 0.006007, 0.051819, 567.91]])\n",
    "cora_data = np.array(      [[0.142844, 0.364501, 0.179599, 0.055982, 384.09],\n",
    "                            [0.171458, 0.359900, 0.192201, 0.056620, 421.69],\n",
    "                            [0.155826, 0.411962, 0.185434, 0.057684, 414.47],\n",
    "                            [0.158801, 0.292940, 0.186371, 0.054138, 415.15],\n",
    "                            [0.175053, 0.452426, 0.193078, 0.055921, 416.62]])\n",
    "breast_data = np.array(    [[0.111363, 0.026105, 0.000992, 0.120782, 788.48],\n",
    "                            [0.100157, 0.012885, 0.002211, 0.120369, 813.11],\n",
    "                            [0.111881, 0.019623, 0.003031, 0.120421, 858.41],\n",
    "                            [0.115581, 0.005944, 0.001996, 0.120318, 818.76],\n",
    "                            [0.125758, 0.014162, 0.001254, 0.119872, 825.96]])\n",
    "with np.printoptions(precision=6, suppress=True, formatter={'float': '{: .6f}'.format}):\n",
    "    print(np.mean(caveman_data, axis=0))\n",
    "    print(np.mean(cora_data, axis=0))\n",
    "    print(np.mean(breast_data, axis=0))\n",
    "    print(np.std(caveman_data, axis=0))\n",
    "    print(np.std(cora_data, axis=0))\n",
    "    print(np.std(breast_data, axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Decide which metric to be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nethome/hsun409/anaconda3/envs/GDSS_env/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "path = \"./GDSS/\"\n",
    "os.chdir(path)\n",
    "\n",
    "seed = 42\n",
    "dataset = 'grid'\n",
    "# dataset = 'community_small'\n",
    "# dataset = 'caveman'\n",
    "# dataset = 'cora'\n",
    "# dataset = 'breast'\n",
    "\n",
    "metric_selection = 'EMD'\n",
    "\n",
    "if metric_selection == 'EMD':\n",
    "    from sampler import Sampler, Sampler_mol\n",
    "    from evaluation.stats import eval_graph_list\n",
    "    from evaluation.mmd import gaussian, gaussian_emd\n",
    "else:\n",
    "    from sampler_new import Sampler, Sampler_mol\n",
    "    from evaluation.stats_new import eval_graph_list\n",
    "    import evaluation.mmd_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load and calculate the metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target dataset:grid\n",
      "Length of testing dataset:20\n",
      "Length of gen dataset:20\n",
      "\u001b[91mdegree   \u001b[0m : \u001b[94m0.056618\u001b[0m\n",
      "\u001b[91mcluster  \u001b[0m : \u001b[94m0.011350\u001b[0m\n",
      "\u001b[91morbit    \u001b[0m : \u001b[94m0.254204\u001b[0m\n",
      "\u001b[91mspectral \u001b[0m : \u001b[94m1.046698\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import math\n",
    "\n",
    "from utils.logger import Logger, set_log, start_log, train_log, sample_log, check_log\n",
    "from data.data_generators import load_dataset\n",
    "\n",
    "# save_dir = '../GDSS_o/samples/pkl/caveman/test/caveman_5000-sample.pkl'\n",
    "# save_dir = '../GDSS_o/samples/pkl/cora/test/cora_5000-sample.pkl'\n",
    "# save_dir = '../GDSS_o/samples/pkl/community_small/test/gdss_community_small-sample.pkl'\n",
    "# save_dir = '../GDSS_o/samples/pkl/breast/test/breast_5000-sample.pkl'\n",
    "\n",
    "with open(save_dir, 'rb') as f:\n",
    "    gen_graph_list = pickle.load(f)\n",
    "\n",
    "test_split = 0.2\n",
    "\n",
    "graph_list = load_dataset(data_dir='../GDSS_o/data', file_name=dataset)\n",
    "print('Target dataset:' + dataset)\n",
    "if dataset == 'cora':\n",
    "    print('for cora, only take first 200 samples')\n",
    "    graph_list = graph_list[:200].copy()\n",
    "test_size = int(test_split * len(graph_list))\n",
    "train_size = len(graph_list) - test_size\n",
    "train_graph_list, test_graph_list = graph_list[:train_size], graph_list[train_size:]\n",
    "# train_graph_list, test_graph_list = graph_list[test_size:], graph_list[:test_size]\n",
    "print('Length of testing dataset:' + str(len(test_graph_list)))\n",
    "print('Length of gen dataset:' + str(len(gen_graph_list)))\n",
    "methods = ['degree', 'cluster', 'orbit', 'spectral'] \n",
    "kernels = {}\n",
    "if metric_selection == 'EMD':\n",
    "    kernels = {'degree':gaussian_emd, \n",
    "                'cluster':gaussian_emd, \n",
    "                'orbit':gaussian,\n",
    "                'spectral':gaussian_emd}\n",
    "    result_dict = eval_graph_list(test_graph_list, gen_graph_list, methods, kernels)\n",
    "else:\n",
    "    result_dict = eval_graph_list(test_graph_list, gen_graph_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Plot graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.plot import plot_graphs_list\n",
    "\n",
    "for idx, g in enumerate(gen_graph_list):\n",
    "    print(idx, g)\n",
    "\n",
    "# plot_graphs_list(graphs=gen_graph_list, title=dataset, max_num=16, save_dir=dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment on GraphAF/GraphDF\n",
    "Here we investigate the likelihood on GraphAF/GraphDF. Since they are autoreggressive models, their likelihoods can be derived directly by extracting the loss functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch; print(torch.__version__)\n",
    "import torch; print(torch.version.cuda)\n",
    "CUDA = 'cu116'\n",
    "TORCH = '1.12.1'\n",
    "%pip install torch-scatter -f https://data.pyg.org/whl/torch-1.12.1+cu116.html\n",
    "%pip install torch-sparse -f https://data.pyg.org/whl/torch-1.12.1+cu116.html\n",
    "%pip install torch-geometric\n",
    "%pip install torch-cluster -f https://data.pyg.org/whl/torch-1.12.1+cu116.html\n",
    "%pip install torch-spline-conv -f https://data.pyg.org/whl/torch-1.12.1+cu116.html\n",
    "%pip install dive-into-graphs\n",
    "\n",
    "import torch_geometric; print(torch_geometric.__version__)\n",
    "from dig.version import __version__; print(__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load dataset 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nethome/hsun409/anaconda3/envs/GraphAF_env/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "83\n",
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "node tensor size: torch.Size([16, 20, 10])\n",
      "adj tensor size: torch.Size([16, 2, 20, 20])\n",
      "80\n"
     ]
    }
   ],
   "source": [
    "from lib2to3.pytree import Node\n",
    "import pickle\n",
    "import networkx as nx\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from scipy.sparse import csr_matrix\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3,4\"\n",
    "import torch\n",
    "\n",
    "\n",
    "def calc_max_prev_node_helper(idx, graphs_path):\n",
    "    with open(graphs_path + 'graph' + str(idx) + '.dat', 'rb') as f:\n",
    "        G = pickle.load(f)\n",
    "\n",
    "    max_prev_node = []\n",
    "    for _ in range(100):\n",
    "        bfs_seq, _ = get_random_bfs_seq(G)\n",
    "        bfs_order_map = {bfs_seq[i]: i for i in range(len(G.nodes()))}\n",
    "        G = nx.relabel_nodes(G, bfs_order_map)\n",
    "\n",
    "        max_prev_node_iter = 0\n",
    "        for u, v in G.edges():\n",
    "            max_prev_node_iter = max(max_prev_node_iter, max(u, v) - min(u, v))\n",
    "\n",
    "        max_prev_node.append(max_prev_node_iter)\n",
    "\n",
    "    return max_prev_node\n",
    "\n",
    "\n",
    "def calc_max_prev_node(graphs_path):\n",
    "    \"\"\"\n",
    "    Approximate max_prev_node from simulating bfs sequences \n",
    "    \"\"\"\n",
    "    max_prev_node = []\n",
    "    count = len([name for name in os.listdir(\n",
    "        graphs_path) if name.endswith(\".dat\")])\n",
    "\n",
    "    max_prev_node = []\n",
    "    with Pool(processes=8) as pool:\n",
    "        for max_prev_node_g in tqdm(pool.imap_unordered(\n",
    "                partial(calc_max_prev_node_helper, graphs_path=graphs_path), list(range(count)))):\n",
    "            max_prev_node.extend(max_prev_node_g)\n",
    "\n",
    "    max_prev_node = sorted(max_prev_node)[-1 * int(0.001 * len(max_prev_node))]\n",
    "    return max_prev_node\n",
    "\n",
    "def get_bfs_seq(G, start_id):\n",
    "    \"\"\"\n",
    "    Get a bfs node sequence\n",
    "    :param G: graph\n",
    "    :param start_id: starting node\n",
    "    :return: List of bfs node sequence\n",
    "    \"\"\"\n",
    "    successors_dict = dict(nx.bfs_successors(G, start_id))\n",
    "    start = [start_id]\n",
    "    output = [start_id]\n",
    "    while len(start) > 0:\n",
    "        succ = []\n",
    "        for current in start:\n",
    "            if current in successors_dict:\n",
    "                succ = succ + successors_dict[current]\n",
    "\n",
    "        output = output + succ\n",
    "        start = succ\n",
    "    return output\n",
    "\n",
    "def get_random_bfs_seq(graph):\n",
    "    n = len(graph.nodes())\n",
    "    # Create a random permutaion of graph nodes\n",
    "    perm = torch.randperm(n)\n",
    "    adj = nx.to_numpy_matrix(graph, nodelist=perm.numpy().tolist(), dtype=int)\n",
    "    G = nx.from_numpy_matrix(adj)\n",
    "    # Construct bfs ordering starting from a random node\n",
    "    start_id = 0\n",
    "    bfs_seq = get_bfs_seq(G, start_id)\n",
    "    return [perm[bfs_seq[i]] for i in range(n)], perm\n",
    "\n",
    "# load a list of graphs\n",
    "def load_graph_list(fname,is_real=True):\n",
    "    with open(fname, \"rb\") as f:\n",
    "        graph_list = pickle.load(f)\n",
    "    for i in range(len(graph_list)):\n",
    "        edges_with_selfloops = graph_list[i].selfloop_edges()\n",
    "        if len(edges_with_selfloops)>0:\n",
    "            graph_list[i].remove_edges_from(edges_with_selfloops)\n",
    "        if is_real:\n",
    "            graph_list[i] = max(nx.connected_component_subgraphs(graph_list[i]), key=len)\n",
    "            graph_list[i] = nx.convert_node_labels_to_integers(graph_list[i])\n",
    "        else:\n",
    "            graph_list[i] = pick_connected_component_new(graph_list[i])\n",
    "    return graph_list\n",
    "\n",
    "class CommunityDataset(Dataset):\n",
    "     def __init__(self, adj, node):\n",
    "         super(Dataset, self).__init__()\n",
    "         self.x = node\n",
    "         self.adj = adj\n",
    "        \n",
    "     def __len__(self):\n",
    "         return len(self.adj)\n",
    "        \n",
    "     def __getitem__(self, index):\n",
    "        return self.adj[index], self.x[index]\n",
    "\n",
    "dataset_path = 'GraphRNN/graphs/GraphRNN_RNN_caveman_small_4_64_train_0.dat'\n",
    "graph_list = load_graph_list(dataset_path)\n",
    "adj_features_list = []\n",
    "node_features_list = []\n",
    "max_node_num = max([g.number_of_nodes() for g in graph_list])\n",
    "print(max_node_num)\n",
    "max_edge_num = max([g.number_of_edges() for g in graph_list])\n",
    "print(max_edge_num)\n",
    "degrees = [list(g.degree().values()) for g in graph_list]\n",
    "degree_list = list(set([item for sublist in degrees for item in sublist]))\n",
    "print(degree_list)\n",
    "\n",
    "for g in graph_list:\n",
    "    adj_tensor = torch.zeros((2, max_node_num, max_node_num))\n",
    "    adj_tensor[0, :g.number_of_nodes(), :g.number_of_nodes()] = torch.tensor(nx.adjacency_matrix(g).toarray())\n",
    "    adj_tensor[1, :g.number_of_nodes(), :g.number_of_nodes()] = 1 - torch.tensor(nx.adjacency_matrix(g).toarray())\n",
    "    # one-hot encoding\n",
    "    node_degree = np.array(list(g.degree().values()), dtype=int) - 1\n",
    "    #(B, N, node_dim)\n",
    "    node_tensor = torch.zeros(max_node_num, len(degree_list))\n",
    "    for idx, deg in enumerate(node_degree):\n",
    "        node_tensor[idx, degree_list == deg] = 1\n",
    "\n",
    "    #(B, 1, N, N)\n",
    "    # adj_tensor = adj_tensor.unsqueeze(0)\n",
    "    adj_features_list.append(adj_tensor)\n",
    "    node_features_list.append(node_tensor)\n",
    "\n",
    "split = 0.8\n",
    "num_samples = len(adj_features_list)\n",
    "training_num = math.ceil(split*num_samples)\n",
    "training_adj_list = adj_features_list[:training_num].copy()\n",
    "training_node_list = node_features_list[:training_num].copy()\n",
    "data_train = CommunityDataset(training_adj_list, training_node_list)\n",
    "\n",
    "data_train_loader = DataLoader(data_train, batch_size=16, shuffle=True)\n",
    "for batch_idx, data_batch in enumerate(data_train_loader):\n",
    "    if batch_idx == 0:\n",
    "        adj_features, node_features = data_batch \n",
    "        #(B, N, node_dim)\n",
    "        print('node tensor size:', node_features.shape)\n",
    "        #(B, 2, N, N)\n",
    "        print('adj tensor size:', adj_features.shape)\n",
    "print(len(data_train_loader.dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dig.ggraph.method import GraphDF, GraphAF\n",
    "import json\n",
    "from dig.ggraph.dataset import ZINC250k\n",
    "from torch_geometric.loader import DenseDataLoader\n",
    "import numpy as np\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "# torch.cuda.set_device(4)\n",
    "\n",
    "runner = GraphAF()\n",
    "\n",
    "config_dict =  {\n",
    "        \"max_size\": 20,\n",
    "        \"edge_unroll\": 20,\n",
    "        \"node_dim\": 10,\n",
    "        \"bond_dim\": 2,\n",
    "        \"num_flow_layer\": 12,\n",
    "        \"num_rgcn_layer\": 3,\n",
    "        \"nhid\": 128,\n",
    "        \"nout\": 128,\n",
    "        \"deq_coeff\": 0.9,\n",
    "        \"st_type\": \"exp\",\n",
    "        \"use_gpu\": True,\n",
    "        \"use_df\": False\n",
    "    }\n",
    "\n",
    "\n",
    "runner.train_rand_gen(loader=data_train_loader, lr = 0.001, wd = 0, max_epochs = 1000, \n",
    "                        model_conf_dict = config_dict, save_interval = 200, save_dir = 'community_test_AF')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Generating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import RDLogger\n",
    "from dig.ggraph.method import GraphDF, GraphAF\n",
    "import json\n",
    "from dig.ggraph.dataset import ZINC250k\n",
    "from torch_geometric.loader import DenseDataLoader\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "runner = GraphAF()\n",
    "\n",
    "RDLogger.DisableLog('rdApp.*')\n",
    "ckpt_path = 'community_test_AF/rand_gen_ckpt_1000.pth'\n",
    "config_dict =  {\n",
    "        \"max_size\": 20,\n",
    "        \"edge_unroll\": 20,\n",
    "        \"node_dim\": 10,\n",
    "        \"bond_dim\": 2,\n",
    "        \"num_flow_layer\": 12,\n",
    "        \"num_rgcn_layer\": 3,\n",
    "        \"nhid\": 128,\n",
    "        \"nout\": 128,\n",
    "        \"deq_coeff\": 0.9,\n",
    "        \"st_type\": \"exp\",\n",
    "        \"use_gpu\": True,\n",
    "        \"use_df\": False\n",
    "    }\n",
    "atomic_list = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "# atomic_list = [0]\n",
    "n_mols = 10\n",
    "mols, _ = runner.run_rand_gen(model_conf_dict = config_dict, checkpoint_path=ckpt_path,\n",
    "                            n_mols=n_mols, atomic_num_list = atomic_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('GDSS_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c154a6e913370b3048d4224c64056db13e3907ff8e1125622ab77ec4e5033cc1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
